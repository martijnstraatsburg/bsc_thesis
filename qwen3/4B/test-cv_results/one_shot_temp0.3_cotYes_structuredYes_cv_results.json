{
  "average_metrics": {
    "story_classification": {
      "accuracy": {
        "mean": 0.8445234708392604,
        "std": 0.055821229146794146
      },
      "micro_precision": {
        "mean": 0.8445234708392604,
        "std": 0.055821229146794146
      },
      "micro_recall": {
        "mean": 0.8445234708392604,
        "std": 0.055821229146794146
      },
      "micro_f1": {
        "mean": 0.8445234708392604,
        "std": 0.055821229146794146
      },
      "macro_precision": {
        "mean": 0.8289041084661288,
        "std": 0.057188112244138195
      },
      "macro_recall": {
        "mean": 0.8522563693616325,
        "std": 0.0626967546159248
      },
      "macro_f1": {
        "mean": 0.8350360772232694,
        "std": 0.05914755746480247
      },
      "weighted_precision": {
        "mean": 0.859270061735737,
        "std": 0.0554069016055579
      },
      "weighted_recall": {
        "mean": 0.8445234708392604,
        "std": 0.055821229146794146
      },
      "weighted_f1": {
        "mean": 0.8471042144456715,
        "std": 0.05487509860444714
      },
      "mcc": {
        "mean": 0.6806490502260345,
        "std": 0.11925584573526946
      }
    },
    "rating_metrics": {
      "suspense": {
        "perfect_accuracy": {
          "mean": 0.322475106685633,
          "std": 0.07192380746820161
        },
        "off_by_one_accuracy": {
          "mean": 0.8817923186344239,
          "std": 0.06050186467152871
        },
        "level_accuracy": {
          "mean": 0.5807965860597439,
          "std": 0.10120348075933565
        },
        "rmse": {
          "mean": 1.0691793315981106,
          "std": 0.15071879220243872
        },
        "mae": {
          "mean": 0.822475106685633,
          "std": 0.13643130716681004
        },
        "quadratic_weighted_kappa": {
          "mean": 0.4272203272291485,
          "std": 0.1205017430737469
        }
      },
      "curiosity": {
        "perfect_accuracy": {
          "mean": 0.33314366998577527,
          "std": 0.07265158444960598
        },
        "off_by_one_accuracy": {
          "mean": 0.8337126600284496,
          "std": 0.041021577740138676
        },
        "level_accuracy": {
          "mean": 0.5753911806543386,
          "std": 0.05390035796875237
        },
        "rmse": {
          "mean": 1.1803551759306372,
          "std": 0.11958585501299134
        },
        "mae": {
          "mean": 0.8815078236130868,
          "std": 0.10635472626340095
        },
        "quadratic_weighted_kappa": {
          "mean": 0.49172770547962424,
          "std": 0.08515255103662042
        }
      },
      "surprise": {
        "perfect_accuracy": {
          "mean": 0.30156472261735423,
          "std": 0.09683284452413946
        },
        "off_by_one_accuracy": {
          "mean": 0.8173541963015648,
          "std": 0.0984393884965265
        },
        "level_accuracy": {
          "mean": 0.5648648648648649,
          "std": 0.07851804889910242
        },
        "rmse": {
          "mean": 1.2297578228331847,
          "std": 0.17304105827562716
        },
        "mae": {
          "mean": 0.9402560455192035,
          "std": 0.16937133395592036
        },
        "quadratic_weighted_kappa": {
          "mean": 0.3408531404812719,
          "std": 0.1588847576140158
        }
      }
    }
  },
  "fold_metrics": [
    {
      "story_classification": {
        "accuracy": 0.7631578947368421,
        "micro_precision": 0.7631578947368421,
        "micro_recall": 0.7631578947368421,
        "micro_f1": 0.7631578947368421,
        "macro_precision": 0.7347826086956522,
        "macro_recall": 0.7596153846153846,
        "macro_f1": 0.7414965986394557,
        "weighted_precision": 0.7844393592677346,
        "weighted_recall": 0.7631578947368421,
        "weighted_f1": 0.7690655209452202,
        "mcc": 0.49377394527263213
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.34210526315789475,
          "off_by_one_accuracy": 0.868421052631579,
          "level_accuracy": 0.5526315789473685,
          "rmse": 1.1470786693528088,
          "mae": 0.8421052631578947,
          "quadratic_weighted_kappa": 0.375
        },
        "curiosity": {
          "perfect_accuracy": 0.3684210526315789,
          "off_by_one_accuracy": 0.7631578947368421,
          "level_accuracy": 0.5526315789473685,
          "rmse": 1.266989801811655,
          "mae": 0.9210526315789473,
          "quadratic_weighted_kappa": 0.4167086059386008
        },
        "surprise": {
          "perfect_accuracy": 0.21052631578947367,
          "off_by_one_accuracy": 0.7894736842105263,
          "level_accuracy": 0.5,
          "rmse": 1.2977713690461004,
          "mae": 1.0526315789473684,
          "quadratic_weighted_kappa": 0.3583113456464381
        }
      }
    },
    {
      "story_classification": {
        "accuracy": 0.8918918918918919,
        "micro_precision": 0.8918918918918919,
        "micro_recall": 0.8918918918918919,
        "micro_f1": 0.8918918918918919,
        "macro_precision": 0.8637820512820513,
        "macro_recall": 0.8968531468531469,
        "macro_f1": 0.8766666666666666,
        "weighted_precision": 0.9021136521136522,
        "weighted_recall": 0.8918918918918919,
        "weighted_f1": 0.8942342342342342,
        "mcc": 0.7599159211912194
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.1891891891891892,
          "off_by_one_accuracy": 0.7837837837837838,
          "level_accuracy": 0.40540540540540543,
          "rmse": 1.2627725822944504,
          "mae": 1.054054054054054,
          "quadratic_weighted_kappa": 0.33180287725742263
        },
        "curiosity": {
          "perfect_accuracy": 0.3783783783783784,
          "off_by_one_accuracy": 0.8918918918918919,
          "level_accuracy": 0.5945945945945946,
          "rmse": 0.9725975251592747,
          "mae": 0.7297297297297297,
          "quadratic_weighted_kappa": 0.6236559139784946
        },
        "surprise": {
          "perfect_accuracy": 0.40540540540540543,
          "off_by_one_accuracy": 0.8648648648648649,
          "level_accuracy": 0.5675675675675675,
          "rmse": 1.1270626736212455,
          "mae": 0.7837837837837838,
          "quadratic_weighted_kappa": 0.40178878568971443
        }
      }
    },
    {
      "story_classification": {
        "accuracy": 0.8108108108108109,
        "micro_precision": 0.8108108108108109,
        "micro_recall": 0.8108108108108109,
        "micro_f1": 0.8108108108108109,
        "macro_precision": 0.8117647058823529,
        "macro_recall": 0.8099415204678362,
        "macro_f1": 0.8102564102564103,
        "weighted_precision": 0.8114467408585055,
        "weighted_recall": 0.8108108108108109,
        "weighted_f1": 0.8105336105336105,
        "mcc": 0.621703553052045
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.40540540540540543,
          "off_by_one_accuracy": 0.9459459459459459,
          "level_accuracy": 0.7027027027027027,
          "rmse": 0.86991767240168,
          "mae": 0.6486486486486487,
          "quadratic_weighted_kappa": 0.6385205861828331
        },
        "curiosity": {
          "perfect_accuracy": 0.3783783783783784,
          "off_by_one_accuracy": 0.8378378378378378,
          "level_accuracy": 0.6486486486486487,
          "rmse": 1.2302493704584911,
          "mae": 0.8648648648648649,
          "quadratic_weighted_kappa": 0.5436123348017621
        },
        "surprise": {
          "perfect_accuracy": 0.43243243243243246,
          "off_by_one_accuracy": 0.8378378378378378,
          "level_accuracy": 0.6756756756756757,
          "rmse": 1.150792911137501,
          "mae": 0.7837837837837838,
          "quadratic_weighted_kappa": 0.4224275246893915
        }
      }
    },
    {
      "story_classification": {
        "accuracy": 0.918918918918919,
        "micro_precision": 0.918918918918919,
        "micro_recall": 0.918918918918919,
        "micro_f1": 0.918918918918919,
        "macro_precision": 0.90625,
        "macro_recall": 0.9375,
        "macro_f1": 0.9149425287356322,
        "weighted_precision": 0.9341216216216216,
        "weighted_recall": 0.918918918918919,
        "weighted_f1": 0.9204100652376515,
        "mcc": 0.8431710977020026
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.35135135135135137,
          "off_by_one_accuracy": 0.8648648648648649,
          "level_accuracy": 0.5945945945945946,
          "rmse": 1.150792911137501,
          "mae": 0.8378378378378378,
          "quadratic_weighted_kappa": 0.31143182681352066
        },
        "curiosity": {
          "perfect_accuracy": 0.35135135135135137,
          "off_by_one_accuracy": 0.8378378378378378,
          "level_accuracy": 0.5945945945945946,
          "rmse": 1.1270626736212455,
          "mae": 0.8378378378378378,
          "quadratic_weighted_kappa": 0.48535069547203313
        },
        "surprise": {
          "perfect_accuracy": 0.24324324324324326,
          "off_by_one_accuracy": 0.6486486486486487,
          "level_accuracy": 0.4594594594594595,
          "rmse": 1.5334116705410037,
          "mae": 1.2162162162162162,
          "quadratic_weighted_kappa": 0.03420342034203416
        }
      }
    },
    {
      "story_classification": {
        "accuracy": 0.8378378378378378,
        "micro_precision": 0.8378378378378378,
        "micro_recall": 0.8378378378378378,
        "micro_f1": 0.8378378378378378,
        "macro_precision": 0.8279411764705882,
        "macro_recall": 0.8573717948717949,
        "macro_f1": 0.8318181818181818,
        "weighted_precision": 0.8642289348171701,
        "weighted_recall": 0.8378378378378378,
        "weighted_f1": 0.8412776412776413,
        "mcc": 0.6846807339122728
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.32432432432432434,
          "off_by_one_accuracy": 0.9459459459459459,
          "level_accuracy": 0.6486486486486487,
          "rmse": 0.9153348228041135,
          "mae": 0.7297297297297297,
          "quadratic_weighted_kappa": 0.47934634589196556
        },
        "curiosity": {
          "perfect_accuracy": 0.1891891891891892,
          "off_by_one_accuracy": 0.8378378378378378,
          "level_accuracy": 0.4864864864864865,
          "rmse": 1.30487650860252,
          "mae": 1.054054054054054,
          "quadratic_weighted_kappa": 0.38931097720723085
        },
        "surprise": {
          "perfect_accuracy": 0.21621621621621623,
          "off_by_one_accuracy": 0.9459459459459459,
          "level_accuracy": 0.6216216216216216,
          "rmse": 1.0397504898200727,
          "mae": 0.8648648648648649,
          "quadratic_weighted_kappa": 0.48753462603878117
        }
      }
    }
  ]
}