{
  "average_metrics": {
    "story_classification": {
      "accuracy": {
        "mean": 0.8066856330014225,
        "std": 0.03417020895504037
      },
      "micro_precision": {
        "mean": 0.8066856330014225,
        "std": 0.03417020895504037
      },
      "micro_recall": {
        "mean": 0.8066856330014225,
        "std": 0.03417020895504037
      },
      "micro_f1": {
        "mean": 0.8066856330014225,
        "std": 0.03417020895504037
      },
      "macro_precision": {
        "mean": 0.7887339900792586,
        "std": 0.04509405587348491
      },
      "macro_recall": {
        "mean": 0.8072680243732876,
        "std": 0.05198995628745072
      },
      "macro_f1": {
        "mean": 0.792398836787975,
        "std": 0.04298637264960047
      },
      "weighted_precision": {
        "mean": 0.8220133149162627,
        "std": 0.045282576036461415
      },
      "weighted_recall": {
        "mean": 0.8066856330014225,
        "std": 0.03417020895504037
      },
      "weighted_f1": {
        "mean": 0.8094377254064394,
        "std": 0.034363297254242774
      },
      "mcc": {
        "mean": 0.5955725179860493,
        "std": 0.09604858207075496
      }
    },
    "rating_metrics": {
      "suspense": {
        "perfect_accuracy": {
          "mean": 0.41934566145092456,
          "std": 0.08461228219334975
        },
        "off_by_one_accuracy": {
          "mean": 0.9139402560455192,
          "std": 0.060236404370898416
        },
        "level_accuracy": {
          "mean": 0.5432432432432432,
          "std": 0.09532536264080213
        },
        "rmse": {
          "mean": 0.9193236407113801,
          "std": 0.14389107362417622
        },
        "mae": {
          "mean": 0.6721194879089616,
          "std": 0.13799474926112004
        },
        "quadratic_weighted_kappa": {
          "mean": 0.4635706778002978,
          "std": 0.1538697843027898
        }
      },
      "curiosity": {
        "perfect_accuracy": {
          "mean": 0.35974395448079666,
          "std": 0.05938454515547698
        },
        "off_by_one_accuracy": {
          "mean": 0.8493598862019915,
          "std": 0.03281696623403261
        },
        "level_accuracy": {
          "mean": 0.5001422475106686,
          "std": 0.04826866986492698
        },
        "rmse": {
          "mean": 1.0815523382175098,
          "std": 0.05917776348043941
        },
        "mae": {
          "mean": 0.8071123755334282,
          "std": 0.07435940149763148
        },
        "quadratic_weighted_kappa": {
          "mean": 0.42537434145843234,
          "std": 0.08119196635136135
        }
      },
      "surprise": {
        "perfect_accuracy": {
          "mean": 0.3657183499288762,
          "std": 0.11447734639563957
        },
        "off_by_one_accuracy": {
          "mean": 0.828022759601707,
          "std": 0.06730336907681998
        },
        "level_accuracy": {
          "mean": 0.5216216216216216,
          "std": 0.0934681971123815
        },
        "rmse": {
          "mean": 1.0866257742662948,
          "std": 0.15333574932230207
        },
        "mae": {
          "mean": 0.8170697012802277,
          "std": 0.16881513619493665
        },
        "quadratic_weighted_kappa": {
          "mean": 0.35238968107847296,
          "std": 0.15422203214376884
        }
      }
    }
  },
  "fold_metrics": [
    {
      "story_classification": {
        "accuracy": 0.7631578947368421,
        "micro_precision": 0.7631578947368421,
        "micro_recall": 0.7631578947368421,
        "micro_f1": 0.7631578947368421,
        "macro_precision": 0.7276923076923076,
        "macro_recall": 0.7371794871794872,
        "macro_f1": 0.7317647058823529,
        "weighted_precision": 0.7690688259109312,
        "weighted_recall": 0.7631578947368421,
        "weighted_f1": 0.7655727554179567,
        "mcc": 0.4647749768357825
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.42105263157894735,
          "off_by_one_accuracy": 0.9210526315789473,
          "level_accuracy": 0.5,
          "rmse": 0.9032106474595007,
          "mae": 0.6578947368421053,
          "quadratic_weighted_kappa": 0.4828797190517998
        },
        "curiosity": {
          "perfect_accuracy": 0.4473684210526316,
          "off_by_one_accuracy": 0.868421052631579,
          "level_accuracy": 0.47368421052631576,
          "rmse": 0.9733285267845752,
          "mae": 0.6842105263157895,
          "quadratic_weighted_kappa": 0.487256371814093
        },
        "surprise": {
          "perfect_accuracy": 0.34210526315789475,
          "off_by_one_accuracy": 0.8157894736842105,
          "level_accuracy": 0.5,
          "rmse": 1.1002392084403616,
          "mae": 0.8421052631578947,
          "quadratic_weighted_kappa": 0.3492181682799702
        }
      }
    },
    {
      "story_classification": {
        "accuracy": 0.8108108108108109,
        "micro_precision": 0.8108108108108109,
        "micro_recall": 0.8108108108108109,
        "micro_f1": 0.8108108108108109,
        "macro_precision": 0.7779503105590062,
        "macro_recall": 0.8129370629370629,
        "macro_f1": 0.7885714285714287,
        "weighted_precision": 0.8327178109786805,
        "weighted_recall": 0.8108108108108109,
        "weighted_f1": 0.8163706563706565,
        "mcc": 0.5898506720476968
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.32432432432432434,
          "off_by_one_accuracy": 0.8108108108108109,
          "level_accuracy": 0.40540540540540543,
          "rmse": 1.115008180796555,
          "mae": 0.8648648648648649,
          "quadratic_weighted_kappa": 0.35870384325546356
        },
        "curiosity": {
          "perfect_accuracy": 0.35135135135135137,
          "off_by_one_accuracy": 0.7837837837837838,
          "level_accuracy": 0.4864864864864865,
          "rmse": 1.138989594902999,
          "mae": 0.8648648648648649,
          "quadratic_weighted_kappa": 0.4
        },
        "surprise": {
          "perfect_accuracy": 0.4594594594594595,
          "off_by_one_accuracy": 0.7567567567567568,
          "level_accuracy": 0.5675675675675675,
          "rmse": 1.1854979567276382,
          "mae": 0.8108108108108109,
          "quadratic_weighted_kappa": 0.29832239241429614
        }
      }
    },
    {
      "story_classification": {
        "accuracy": 0.8108108108108109,
        "micro_precision": 0.8108108108108109,
        "micro_recall": 0.8108108108108109,
        "micro_f1": 0.8108108108108109,
        "macro_precision": 0.8117647058823529,
        "macro_recall": 0.8099415204678362,
        "macro_f1": 0.8102564102564103,
        "weighted_precision": 0.8114467408585055,
        "weighted_recall": 0.8108108108108109,
        "weighted_f1": 0.8105336105336105,
        "mcc": 0.621703553052045
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.5135135135135135,
          "off_by_one_accuracy": 1.0,
          "level_accuracy": 0.6756756756756757,
          "rmse": 0.6974858324629157,
          "mae": 0.4864864864864865,
          "quadratic_weighted_kappa": 0.7547864506627393
        },
        "curiosity": {
          "perfect_accuracy": 0.2972972972972973,
          "off_by_one_accuracy": 0.8648648648648649,
          "level_accuracy": 0.5675675675675675,
          "rmse": 1.115008180796555,
          "mae": 0.8648648648648649,
          "quadratic_weighted_kappa": 0.5456486919380673
        },
        "surprise": {
          "perfect_accuracy": 0.3783783783783784,
          "off_by_one_accuracy": 0.8918918918918919,
          "level_accuracy": 0.5675675675675675,
          "rmse": 0.9725975251592747,
          "mae": 0.7297297297297297,
          "quadratic_weighted_kappa": 0.5336694274396832
        }
      }
    },
    {
      "story_classification": {
        "accuracy": 0.8648648648648649,
        "micro_precision": 0.8648648648648649,
        "micro_recall": 0.8648648648648649,
        "micro_f1": 0.8648648648648649,
        "macro_precision": 0.8611111111111112,
        "macro_recall": 0.8958333333333333,
        "macro_f1": 0.8612153038259565,
        "weighted_precision": 0.9024024024024023,
        "weighted_recall": 0.8648648648648649,
        "weighted_f1": 0.8679061657306217,
        "mcc": 0.7561476438231222
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.32432432432432434,
          "off_by_one_accuracy": 0.918918918918919,
          "level_accuracy": 0.5135135135135135,
          "rmse": 1.0266713466606798,
          "mae": 0.7837837837837838,
          "quadratic_weighted_kappa": 0.3377696190913263
        },
        "curiosity": {
          "perfect_accuracy": 0.40540540540540543,
          "off_by_one_accuracy": 0.8648648648648649,
          "level_accuracy": 0.5405405405405406,
          "rmse": 1.065427207806866,
          "mae": 0.7567567567567568,
          "quadratic_weighted_kappa": 0.3764044943820225
        },
        "surprise": {
          "perfect_accuracy": 0.16216216216216217,
          "off_by_one_accuracy": 0.7567567567567568,
          "level_accuracy": 0.35135135135135137,
          "rmse": 1.30487650860252,
          "mae": 1.1081081081081081,
          "quadratic_weighted_kappa": 0.09616130283055457
        }
      }
    },
    {
      "story_classification": {
        "accuracy": 0.7837837837837838,
        "micro_precision": 0.7837837837837838,
        "micro_recall": 0.7837837837837838,
        "micro_f1": 0.7837837837837838,
        "macro_precision": 0.7651515151515151,
        "macro_recall": 0.780448717948718,
        "macro_f1": 0.7701863354037266,
        "weighted_precision": 0.7944307944307943,
        "weighted_recall": 0.7837837837837838,
        "weighted_f1": 0.786805438979352,
        "mcc": 0.5453857441716
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.5135135135135135,
          "off_by_one_accuracy": 0.918918918918919,
          "level_accuracy": 0.6216216216216216,
          "rmse": 0.8542421961772491,
          "mae": 0.5675675675675675,
          "quadratic_weighted_kappa": 0.3837137569401603
        },
        "curiosity": {
          "perfect_accuracy": 0.2972972972972973,
          "off_by_one_accuracy": 0.8648648648648649,
          "level_accuracy": 0.43243243243243246,
          "rmse": 1.115008180796555,
          "mae": 0.8648648648648649,
          "quadratic_weighted_kappa": 0.3175621491579791
        },
        "surprise": {
          "perfect_accuracy": 0.4864864864864865,
          "off_by_one_accuracy": 0.918918918918919,
          "level_accuracy": 0.6216216216216216,
          "rmse": 0.86991767240168,
          "mae": 0.5945945945945946,
          "quadratic_weighted_kappa": 0.48457711442786067
        }
      }
    }
  ]
}