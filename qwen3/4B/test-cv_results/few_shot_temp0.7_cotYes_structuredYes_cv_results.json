{
  "average_metrics": {
    "story_classification": {
      "accuracy": {
        "mean": 0.8496443812233286,
        "std": 0.026235315808588355
      },
      "micro_precision": {
        "mean": 0.8496443812233286,
        "std": 0.026235315808588355
      },
      "micro_recall": {
        "mean": 0.8496443812233286,
        "std": 0.026235315808588355
      },
      "micro_f1": {
        "mean": 0.8496443812233286,
        "std": 0.026235315808588382
      },
      "macro_precision": {
        "mean": 0.8340295148159598,
        "std": 0.03323117605064454
      },
      "macro_recall": {
        "mean": 0.8567619514987935,
        "std": 0.035742678284927554
      },
      "macro_f1": {
        "mean": 0.839364568119132,
        "std": 0.03144586019071015
      },
      "weighted_precision": {
        "mean": 0.8650371882031594,
        "std": 0.0305708955092671
      },
      "weighted_recall": {
        "mean": 0.8496443812233286,
        "std": 0.026235315808588355
      },
      "weighted_f1": {
        "mean": 0.8520786529737101,
        "std": 0.02588229567611978
      },
      "mcc": {
        "mean": 0.6902641427158157,
        "std": 0.0674460477857589
      }
    },
    "rating_metrics": {
      "suspense": {
        "perfect_accuracy": {
          "mean": 0.39800853485064014,
          "std": 0.10776842644653557
        },
        "off_by_one_accuracy": {
          "mean": 0.8709815078236132,
          "std": 0.06482117767161892
        },
        "level_accuracy": {
          "mean": 0.5704125177809388,
          "std": 0.12905855678145217
        },
        "rmse": {
          "mean": 1.0321720942250634,
          "std": 0.17625146846574807
        },
        "mae": {
          "mean": 0.7524893314366998,
          "std": 0.18304330965614884
        },
        "quadratic_weighted_kappa": {
          "mean": 0.4575209463284299,
          "std": 0.15068724217205426
        }
      },
      "curiosity": {
        "perfect_accuracy": {
          "mean": 0.3493598862019915,
          "std": 0.060881934566145095
        },
        "off_by_one_accuracy": {
          "mean": 0.833570412517781,
          "std": 0.024406953729191723
        },
        "level_accuracy": {
          "mean": 0.5433854907539117,
          "std": 0.09289100087751187
        },
        "rmse": {
          "mean": 1.1441488483076503,
          "std": 0.047991809402879004
        },
        "mae": {
          "mean": 0.8493598862019915,
          "std": 0.05843308891842457
        },
        "quadratic_weighted_kappa": {
          "mean": 0.456827198424973,
          "std": 0.0668395628911643
        }
      },
      "surprise": {
        "perfect_accuracy": {
          "mean": 0.34423897581792323,
          "std": 0.09156144204857253
        },
        "off_by_one_accuracy": {
          "mean": 0.8169274537695591,
          "std": 0.07576012144183918
        },
        "level_accuracy": {
          "mean": 0.5914651493598861,
          "std": 0.10555647112653409
        },
        "rmse": {
          "mean": 1.1647855848890973,
          "std": 0.19165373918556794
        },
        "mae": {
          "mean": 0.876529160739687,
          "std": 0.19368324103439244
        },
        "quadratic_weighted_kappa": {
          "mean": 0.39426769947012025,
          "std": 0.18967096335902772
        }
      }
    }
  },
  "fold_metrics": [
    {
      "story_classification": {
        "accuracy": 0.8157894736842105,
        "micro_precision": 0.8157894736842105,
        "micro_recall": 0.8157894736842105,
        "micro_f1": 0.8157894736842104,
        "macro_precision": 0.7898550724637681,
        "macro_recall": 0.8205128205128205,
        "macro_f1": 0.7989417989417991,
        "weighted_precision": 0.8352402745995423,
        "weighted_recall": 0.8157894736842105,
        "weighted_f1": 0.8203842940685047,
        "mcc": 0.6095974632995459
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.3684210526315789,
          "off_by_one_accuracy": 0.868421052631579,
          "level_accuracy": 0.47368421052631576,
          "rmse": 1.0760551736979407,
          "mae": 0.7894736842105263,
          "quadratic_weighted_kappa": 0.3834808259587019
        },
        "curiosity": {
          "perfect_accuracy": 0.3684210526315789,
          "off_by_one_accuracy": 0.7894736842105263,
          "level_accuracy": 0.47368421052631576,
          "rmse": 1.180989772227204,
          "mae": 0.868421052631579,
          "quadratic_weighted_kappa": 0.39446782922429346
        },
        "surprise": {
          "perfect_accuracy": 0.3157894736842105,
          "off_by_one_accuracy": 0.868421052631579,
          "level_accuracy": 0.5789473684210527,
          "rmse": 1.1002392084403616,
          "mae": 0.8421052631578947,
          "quadratic_weighted_kappa": 0.4843657817109145
        }
      }
    },
    {
      "story_classification": {
        "accuracy": 0.8378378378378378,
        "micro_precision": 0.8378378378378378,
        "micro_recall": 0.8378378378378378,
        "micro_f1": 0.8378378378378378,
        "macro_precision": 0.8044871794871795,
        "macro_recall": 0.8321678321678322,
        "macro_f1": 0.815,
        "weighted_precision": 0.84996534996535,
        "weighted_recall": 0.8378378378378378,
        "weighted_f1": 0.8413513513513512,
        "mcc": 0.6360529736842806
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.21621621621621623,
          "off_by_one_accuracy": 0.7567567567567568,
          "level_accuracy": 0.3783783783783784,
          "rmse": 1.283997137321049,
          "mae": 1.054054054054054,
          "quadratic_weighted_kappa": 0.30830524057615694
        },
        "curiosity": {
          "perfect_accuracy": 0.24324324324324326,
          "off_by_one_accuracy": 0.8378378378378378,
          "level_accuracy": 0.43243243243243246,
          "rmse": 1.115008180796555,
          "mae": 0.918918918918919,
          "quadratic_weighted_kappa": 0.4772727272727273
        },
        "surprise": {
          "perfect_accuracy": 0.4594594594594595,
          "off_by_one_accuracy": 0.8378378378378378,
          "level_accuracy": 0.6216216216216216,
          "rmse": 1.0780362527123855,
          "mae": 0.7297297297297297,
          "quadratic_weighted_kappa": 0.4743970928311859
        }
      }
    },
    {
      "story_classification": {
        "accuracy": 0.8378378378378378,
        "micro_precision": 0.8378378378378378,
        "micro_recall": 0.8378378378378378,
        "micro_f1": 0.8378378378378378,
        "macro_precision": 0.8422619047619048,
        "macro_recall": 0.8362573099415205,
        "macro_f1": 0.8367647058823531,
        "weighted_precision": 0.8413770913770914,
        "weighted_recall": 0.8378378378378378,
        "weighted_f1": 0.8371224165341813,
        "mcc": 0.6784926451795892
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.5405405405405406,
          "off_by_one_accuracy": 0.9459459459459459,
          "level_accuracy": 0.7297297297297297,
          "rmse": 0.7884298457197201,
          "mae": 0.5135135135135135,
          "quadratic_weighted_kappa": 0.709457152611813
        },
        "curiosity": {
          "perfect_accuracy": 0.40540540540540543,
          "off_by_one_accuracy": 0.8378378378378378,
          "level_accuracy": 0.7027027027027027,
          "rmse": 1.1624763874381927,
          "mae": 0.8108108108108109,
          "quadratic_weighted_kappa": 0.5695672405770127
        },
        "surprise": {
          "perfect_accuracy": 0.35135135135135137,
          "off_by_one_accuracy": 0.8108108108108109,
          "level_accuracy": 0.6216216216216216,
          "rmse": 1.1624763874381927,
          "mae": 0.8648648648648649,
          "quadratic_weighted_kappa": 0.43563148261134843
        }
      }
    },
    {
      "story_classification": {
        "accuracy": 0.8918918918918919,
        "micro_precision": 0.8918918918918919,
        "micro_recall": 0.8918918918918919,
        "micro_f1": 0.8918918918918919,
        "macro_precision": 0.8823529411764706,
        "macro_recall": 0.9166666666666667,
        "macro_f1": 0.8878787878787878,
        "weighted_precision": 0.917329093799682,
        "weighted_recall": 0.8918918918918919,
        "weighted_f1": 0.8941850941850941,
        "mcc": 0.7982824700322464
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.40540540540540543,
          "off_by_one_accuracy": 0.8648648648648649,
          "level_accuracy": 0.5945945945945946,
          "rmse": 1.1270626736212455,
          "mae": 0.7837837837837838,
          "quadratic_weighted_kappa": 0.33853176112590344
        },
        "curiosity": {
          "perfect_accuracy": 0.40540540540540543,
          "off_by_one_accuracy": 0.8648648648648649,
          "level_accuracy": 0.5675675675675675,
          "rmse": 1.065427207806866,
          "mae": 0.7567567567567568,
          "quadratic_weighted_kappa": 0.4589136490250697
        },
        "surprise": {
          "perfect_accuracy": 0.1891891891891892,
          "off_by_one_accuracy": 0.6756756756756757,
          "level_accuracy": 0.40540540540540543,
          "rmse": 1.5245734893157248,
          "mae": 1.2432432432432432,
          "quadratic_weighted_kappa": 0.02272727272727293
        }
      }
    },
    {
      "story_classification": {
        "accuracy": 0.8648648648648649,
        "micro_precision": 0.8648648648648649,
        "micro_recall": 0.8648648648648649,
        "micro_f1": 0.8648648648648649,
        "macro_precision": 0.8511904761904762,
        "macro_recall": 0.8782051282051282,
        "macro_f1": 0.8582375478927203,
        "weighted_precision": 0.8812741312741312,
        "weighted_recall": 0.8648648648648649,
        "weighted_f1": 0.8673501087294192,
        "mcc": 0.7288951613834163
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.4594594594594595,
          "off_by_one_accuracy": 0.918918918918919,
          "level_accuracy": 0.6756756756756757,
          "rmse": 0.8853156407653622,
          "mae": 0.6216216216216216,
          "quadratic_weighted_kappa": 0.5478297513695743
        },
        "curiosity": {
          "perfect_accuracy": 0.32432432432432434,
          "off_by_one_accuracy": 0.8378378378378378,
          "level_accuracy": 0.5405405405405406,
          "rmse": 1.196842693269434,
          "mae": 0.8918918918918919,
          "quadratic_weighted_kappa": 0.3839145460257618
        },
        "surprise": {
          "perfect_accuracy": 0.40540540540540543,
          "off_by_one_accuracy": 0.8918918918918919,
          "level_accuracy": 0.7297297297297297,
          "rmse": 0.9586025865388216,
          "mae": 0.7027027027027027,
          "quadratic_weighted_kappa": 0.5542168674698795
        }
      }
    }
  ]
}