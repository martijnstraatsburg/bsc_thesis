{
  "average_metrics": {
    "story_classification": {
      "accuracy": {
        "mean": 0.7488372093023256,
        "std": 0.038106413852603604
      },
      "micro_precision": {
        "mean": 0.7488372093023256,
        "std": 0.038106413852603604
      },
      "micro_recall": {
        "mean": 0.7488372093023256,
        "std": 0.038106413852603604
      },
      "micro_f1": {
        "mean": 0.7488372093023256,
        "std": 0.03810641385260361
      },
      "macro_precision": {
        "mean": 0.7469799607726376,
        "std": 0.038852743051373026
      },
      "macro_recall": {
        "mean": 0.7504922924891636,
        "std": 0.03730605146909967
      },
      "macro_f1": {
        "mean": 0.74686628040016,
        "std": 0.03905379248095306
      },
      "mcc": {
        "mean": 0.4974530215188156,
        "std": 0.07614947819180658
      }
    },
    "rating_metrics": {
      "suspense": {
        "perfect_accuracy": {
          "mean": 0.46300454423950815,
          "std": 0.050023100996300515
        },
        "off_by_one_accuracy": {
          "mean": 0.8594760759155304,
          "std": 0.047810043663554076
        },
        "rmse": {
          "mean": 1.020693646845741,
          "std": 0.09401800037401546
        },
        "mae": {
          "mean": 0.6959369152632985,
          "std": 0.08835464268844918
        },
        "quadratic_weighted_kappa": {
          "mean": 0.4472498193174671,
          "std": 0.08598237968804795
        }
      },
      "curiosity": {
        "perfect_accuracy": {
          "mean": 0.36166800320769854,
          "std": 0.03294295750441892
        },
        "off_by_one_accuracy": {
          "mean": 0.8501202886928627,
          "std": 0.028901889378665092
        },
        "rmse": {
          "mean": 1.0736698150514523,
          "std": 0.0656005358640716
        },
        "mae": {
          "mean": 0.8020315423683506,
          "std": 0.06277512267845911
        },
        "quadratic_weighted_kappa": {
          "mean": 0.44356893396045216,
          "std": 0.050491594130028475
        }
      },
      "surprise": {
        "perfect_accuracy": {
          "mean": 0.4353916065223203,
          "std": 0.07009051569977398
        },
        "off_by_one_accuracy": {
          "mean": 0.8502539427960439,
          "std": 0.04167724993443609
        },
        "rmse": {
          "mean": 1.0591973559115437,
          "std": 0.10842077481258812
        },
        "mae": {
          "mean": 0.7373964180700348,
          "std": 0.09774048444810897
        },
        "quadratic_weighted_kappa": {
          "mean": 0.4370353190437495,
          "std": 0.0931317852881539
        }
      }
    }
  },
  "fold_metrics": [
    {
      "story_classification": {
        "accuracy": 0.6896551724137931,
        "micro_precision": 0.6896551724137931,
        "micro_recall": 0.6896551724137931,
        "micro_f1": 0.6896551724137931,
        "macro_precision": 0.6873015873015873,
        "macro_recall": 0.6913513513513514,
        "macro_f1": 0.6870086608927382,
        "mcc": 0.37863128154135306
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.4942528735632184,
          "off_by_one_accuracy": 0.896551724137931,
          "rmse": 0.9649012813540153,
          "mae": 0.632183908045977,
          "quadratic_weighted_kappa": 0.4728850325379609
        },
        "curiosity": {
          "perfect_accuracy": 0.3218390804597701,
          "off_by_one_accuracy": 0.8505747126436781,
          "rmse": 1.1141720290623112,
          "mae": 0.8505747126436781,
          "quadratic_weighted_kappa": 0.4156716417910449
        },
        "surprise": {
          "perfect_accuracy": 0.5057471264367817,
          "off_by_one_accuracy": 0.7816091954022989,
          "rmse": 1.1497126077675979,
          "mae": 0.7471264367816092,
          "quadratic_weighted_kappa": 0.40179372197309415
        }
      }
    },
    {
      "story_classification": {
        "accuracy": 0.735632183908046,
        "micro_precision": 0.735632183908046,
        "micro_recall": 0.735632183908046,
        "micro_f1": 0.735632183908046,
        "macro_precision": 0.736522198731501,
        "macro_recall": 0.7418918918918919,
        "macro_f1": 0.7343687773795301,
        "mcc": 0.4783839551054898
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.4827586206896552,
          "off_by_one_accuracy": 0.8160919540229885,
          "rmse": 1.1141720290623112,
          "mae": 0.735632183908046,
          "quadratic_weighted_kappa": 0.3561737700424832
        },
        "curiosity": {
          "perfect_accuracy": 0.367816091954023,
          "off_by_one_accuracy": 0.8390804597701149,
          "rmse": 1.109001829336302,
          "mae": 0.8160919540229885,
          "quadratic_weighted_kappa": 0.41596085074345945
        },
        "surprise": {
          "perfect_accuracy": 0.3218390804597701,
          "off_by_one_accuracy": 0.8275862068965517,
          "rmse": 1.203443335628631,
          "mae": 0.896551724137931,
          "quadratic_weighted_kappa": 0.27729430379746844
        }
      }
    },
    {
      "story_classification": {
        "accuracy": 0.7701149425287356,
        "micro_precision": 0.7701149425287356,
        "micro_recall": 0.7701149425287356,
        "micro_f1": 0.7701149425287356,
        "macro_precision": 0.7690476190476191,
        "macro_recall": 0.7704787234042554,
        "macro_f1": 0.7693531283138919,
        "mcc": 0.5395244444321476
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.40229885057471265,
          "off_by_one_accuracy": 0.7931034482758621,
          "rmse": 1.1295406451144276,
          "mae": 0.8160919540229885,
          "quadratic_weighted_kappa": 0.3364254792826221
        },
        "curiosity": {
          "perfect_accuracy": 0.39080459770114945,
          "off_by_one_accuracy": 0.8850574712643678,
          "rmse": 0.9767410038007758,
          "mae": 0.7241379310344828,
          "quadratic_weighted_kappa": 0.5235864617008643
        },
        "surprise": {
          "perfect_accuracy": 0.4482758620689655,
          "off_by_one_accuracy": 0.8850574712643678,
          "rmse": 0.9468641529479986,
          "mae": 0.6666666666666666,
          "quadratic_weighted_kappa": 0.5526107594936709
        }
      }
    },
    {
      "story_classification": {
        "accuracy": 0.8045977011494253,
        "micro_precision": 0.8045977011494253,
        "micro_recall": 0.8045977011494253,
        "micro_f1": 0.8045977011494253,
        "macro_precision": 0.8044397463002114,
        "macro_recall": 0.8047619047619048,
        "macro_f1": 0.8044943820224719,
        "mcc": 0.6092015658800738
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.5287356321839081,
          "off_by_one_accuracy": 0.9195402298850575,
          "rmse": 0.8775619308793742,
          "mae": 0.5632183908045977,
          "quadratic_weighted_kappa": 0.5452843435525392
        },
        "curiosity": {
          "perfect_accuracy": 0.40229885057471265,
          "off_by_one_accuracy": 0.8735632183908046,
          "rmse": 1.0170952554312156,
          "mae": 0.735632183908046,
          "quadratic_weighted_kappa": 0.4787644787644788
        },
        "surprise": {
          "perfect_accuracy": 0.5057471264367817,
          "off_by_one_accuracy": 0.896551724137931,
          "rmse": 0.9284766908852593,
          "mae": 0.6091954022988506,
          "quadratic_weighted_kappa": 0.48374080227866134
        }
      }
    },
    {
      "story_classification": {
        "accuracy": 0.7441860465116279,
        "micro_precision": 0.7441860465116279,
        "micro_recall": 0.7441860465116279,
        "micro_f1": 0.7441860465116278,
        "macro_precision": 0.7375886524822695,
        "macro_recall": 0.7439775910364146,
        "macro_f1": 0.7391064533921676,
        "mcc": 0.48152386063501346
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.4069767441860465,
          "off_by_one_accuracy": 0.872093023255814,
          "rmse": 1.017292347818577,
          "mae": 0.7325581395348837,
          "quadratic_weighted_kappa": 0.5254804711717298
        },
        "curiosity": {
          "perfect_accuracy": 0.32558139534883723,
          "off_by_one_accuracy": 0.8023255813953488,
          "rmse": 1.151338957626657,
          "mae": 0.8837209302325582,
          "quadratic_weighted_kappa": 0.38386123680241324
        },
        "surprise": {
          "perfect_accuracy": 0.3953488372093023,
          "off_by_one_accuracy": 0.8604651162790697,
          "rmse": 1.0674899923282326,
          "mae": 0.7674418604651163,
          "quadratic_weighted_kappa": 0.46973700767585247
        }
      }
    }
  ]
}