{
  "average_metrics": {
    "story_classification": {
      "accuracy": {
        "mean": 0.7579524191392675,
        "std": 0.04994201565962095
      },
      "micro_precision": {
        "mean": 0.7579524191392675,
        "std": 0.04994201565962095
      },
      "micro_recall": {
        "mean": 0.7579524191392675,
        "std": 0.04994201565962095
      },
      "micro_f1": {
        "mean": 0.7579524191392676,
        "std": 0.04994201565962094
      },
      "macro_precision": {
        "mean": 0.754599125881346,
        "std": 0.052252772488070316
      },
      "macro_recall": {
        "mean": 0.7571952357327826,
        "std": 0.050240611478817004
      },
      "macro_f1": {
        "mean": 0.7550912794741125,
        "std": 0.0518365886184442
      },
      "mcc": {
        "mean": 0.5117813387648431,
        "std": 0.10250143108957487
      }
    },
    "rating_metrics": {
      "suspense": {
        "perfect_accuracy": {
          "mean": 0.4422881582464582,
          "std": 0.03629198933320563
        },
        "off_by_one_accuracy": {
          "mean": 0.875568029938519,
          "std": 0.04953589626789327
        },
        "rmse": {
          "mean": 1.0021365562494258,
          "std": 0.08557744320000481
        },
        "mae": {
          "mean": 0.6982624966586474,
          "std": 0.0746159436833498
        },
        "quadratic_weighted_kappa": {
          "mean": 0.47345635167059685,
          "std": 0.06829340752931647
        }
      },
      "curiosity": {
        "perfect_accuracy": {
          "mean": 0.38930767174552255,
          "std": 0.03643739334784252
        },
        "off_by_one_accuracy": {
          "mean": 0.8385993049986634,
          "std": 0.03319808653435876
        },
        "rmse": {
          "mean": 1.0769604323074697,
          "std": 0.06441903326778078
        },
        "mae": {
          "mean": 0.785912857524726,
          "std": 0.056138312788587844
        },
        "quadratic_weighted_kappa": {
          "mean": 0.4378113529008649,
          "std": 0.057463802393142416
        }
      },
      "surprise": {
        "perfect_accuracy": {
          "mean": 0.39165998396150764,
          "std": 0.0600837022364985
        },
        "off_by_one_accuracy": {
          "mean": 0.8479283614006949,
          "std": 0.029420714010240337
        },
        "rmse": {
          "mean": 1.0788484726395489,
          "std": 0.09462658261596486
        },
        "mae": {
          "mean": 0.7811547714514836,
          "std": 0.09551171712675051
        },
        "quadratic_weighted_kappa": {
          "mean": 0.4119629452912504,
          "std": 0.08213566437223033
        }
      }
    }
  },
  "fold_metrics": [
    {
      "story_classification": {
        "accuracy": 0.7241379310344828,
        "micro_precision": 0.7241379310344828,
        "micro_recall": 0.7241379310344828,
        "micro_f1": 0.7241379310344829,
        "macro_precision": 0.7205726405090138,
        "macro_recall": 0.7248648648648648,
        "macro_f1": 0.721153846153846,
        "mcc": 0.44541682501201413
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.4482758620689655,
          "off_by_one_accuracy": 0.9425287356321839,
          "rmse": 0.9160133513055991,
          "mae": 0.632183908045977,
          "quadratic_weighted_kappa": 0.5186813186813187
        },
        "curiosity": {
          "perfect_accuracy": 0.4482758620689655,
          "off_by_one_accuracy": 0.8160919540229885,
          "rmse": 1.1038074128205977,
          "mae": 0.7586206896551724,
          "quadratic_weighted_kappa": 0.41802347595607725
        },
        "surprise": {
          "perfect_accuracy": 0.40229885057471265,
          "off_by_one_accuracy": 0.8275862068965517,
          "rmse": 1.1596670152276025,
          "mae": 0.8160919540229885,
          "quadratic_weighted_kappa": 0.3529336977941644
        }
      }
    },
    {
      "story_classification": {
        "accuracy": 0.7241379310344828,
        "micro_precision": 0.7241379310344828,
        "micro_recall": 0.7241379310344828,
        "micro_f1": 0.7241379310344829,
        "macro_precision": 0.7205726405090138,
        "macro_recall": 0.7248648648648648,
        "macro_f1": 0.721153846153846,
        "mcc": 0.44541682501201413
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.4367816091954023,
          "off_by_one_accuracy": 0.8160919540229885,
          "rmse": 1.109001829336302,
          "mae": 0.7701149425287356,
          "quadratic_weighted_kappa": 0.3711409849354861
        },
        "curiosity": {
          "perfect_accuracy": 0.367816091954023,
          "off_by_one_accuracy": 0.8620689655172413,
          "rmse": 1.1038074128205977,
          "mae": 0.8045977011494253,
          "quadratic_weighted_kappa": 0.39866979655712054
        },
        "surprise": {
          "perfect_accuracy": 0.28735632183908044,
          "off_by_one_accuracy": 0.8045977011494253,
          "rmse": 1.1986582537134602,
          "mae": 0.9310344827586207,
          "quadratic_weighted_kappa": 0.2976167409416779
        }
      }
    },
    {
      "story_classification": {
        "accuracy": 0.7931034482758621,
        "micro_precision": 0.7931034482758621,
        "micro_recall": 0.7931034482758621,
        "micro_f1": 0.7931034482758621,
        "macro_precision": 0.7917553191489362,
        "macro_recall": 0.7917553191489362,
        "macro_f1": 0.7917553191489362,
        "mcc": 0.5835106382978723
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.42528735632183906,
          "off_by_one_accuracy": 0.8275862068965517,
          "rmse": 1.072112534837795,
          "mae": 0.7586206896551724,
          "quadratic_weighted_kappa": 0.41247974068071314
        },
        "curiosity": {
          "perfect_accuracy": 0.41379310344827586,
          "off_by_one_accuracy": 0.8850574712643678,
          "rmse": 0.9649012813540153,
          "mae": 0.7011494252873564,
          "quadratic_weighted_kappa": 0.5512322486149144
        },
        "surprise": {
          "perfect_accuracy": 0.45977011494252873,
          "off_by_one_accuracy": 0.8735632183908046,
          "rmse": 0.9589266029707683,
          "mae": 0.6666666666666666,
          "quadratic_weighted_kappa": 0.5343858710195345
        }
      }
    },
    {
      "story_classification": {
        "accuracy": 0.8390804597701149,
        "micro_precision": 0.8390804597701149,
        "micro_recall": 0.8390804597701149,
        "micro_f1": 0.8390804597701149,
        "macro_precision": 0.8388888888888889,
        "macro_recall": 0.8388888888888889,
        "macro_f1": 0.8388888888888889,
        "mcc": 0.6777777777777778
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.5057471264367817,
          "off_by_one_accuracy": 0.9195402298850575,
          "rmse": 0.8905635565617213,
          "mae": 0.5862068965517241,
          "quadratic_weighted_kappa": 0.5401761777096898
        },
        "curiosity": {
          "perfect_accuracy": 0.367816091954023,
          "off_by_one_accuracy": 0.8390804597701149,
          "rmse": 1.0559083903140614,
          "mae": 0.7931034482758621,
          "quadratic_weighted_kappa": 0.4214711729622266
        },
        "surprise": {
          "perfect_accuracy": 0.4367816091954023,
          "off_by_one_accuracy": 0.8850574712643678,
          "rmse": 0.982607368881035,
          "mae": 0.6896551724137931,
          "quadratic_weighted_kappa": 0.4163871586008625
        }
      }
    },
    {
      "story_classification": {
        "accuracy": 0.7093023255813954,
        "micro_precision": 0.7093023255813954,
        "micro_recall": 0.7093023255813954,
        "micro_f1": 0.7093023255813953,
        "macro_precision": 0.7012061403508771,
        "macro_recall": 0.7056022408963585,
        "macro_f1": 0.7025044970250449,
        "mcc": 0.40678462772453716
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.3953488372093023,
          "off_by_one_accuracy": 0.872093023255814,
          "rmse": 1.0229915092057102,
          "mae": 0.7441860465116279,
          "quadratic_weighted_kappa": 0.5248035363457761
        },
        "curiosity": {
          "perfect_accuracy": 0.3488372093023256,
          "off_by_one_accuracy": 0.7906976744186046,
          "rmse": 1.156377664228076,
          "mae": 0.872093023255814,
          "quadratic_weighted_kappa": 0.39966007041398566
        },
        "surprise": {
          "perfect_accuracy": 0.37209302325581395,
          "off_by_one_accuracy": 0.8488372093023255,
          "rmse": 1.0943831224048783,
          "mae": 0.8023255813953488,
          "quadratic_weighted_kappa": 0.45849125810001223
        }
      }
    }
  ]
}