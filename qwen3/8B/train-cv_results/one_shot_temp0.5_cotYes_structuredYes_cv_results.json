{
  "average_metrics": {
    "story_classification": {
      "accuracy": {
        "mean": 0.7325581395348837,
        "std": 0.040082685050362145
      },
      "micro_precision": {
        "mean": 0.7325581395348837,
        "std": 0.040082685050362145
      },
      "micro_recall": {
        "mean": 0.7325581395348837,
        "std": 0.040082685050362145
      },
      "micro_f1": {
        "mean": 0.7325581395348837,
        "std": 0.040082685050362166
      },
      "macro_precision": {
        "mean": 0.7454012372371387,
        "std": 0.05235481045153402
      },
      "macro_recall": {
        "mean": 0.7115637428059205,
        "std": 0.052282069096518106
      },
      "macro_f1": {
        "mean": 0.7111788147336756,
        "std": 0.05384680270640697
      },
      "mcc": {
        "mean": 0.4553786494296098,
        "std": 0.1043324829584403
      }
    },
    "rating_metrics": {
      "suspense": {
        "perfect_accuracy": {
          "mean": 0.359395883453622,
          "std": 0.03143935443489671
        },
        "off_by_one_accuracy": {
          "mean": 0.8687249398556537,
          "std": 0.04260832438244865
        },
        "rmse": {
          "mean": 1.085829908204849,
          "std": 0.07043897278332086
        },
        "mae": {
          "mean": 0.8017909649826251,
          "std": 0.06030582143205759
        },
        "quadratic_weighted_kappa": {
          "mean": 0.3936906956199602,
          "std": 0.07691753327932632
        }
      },
      "curiosity": {
        "perfect_accuracy": {
          "mean": 0.3434910451750869,
          "std": 0.05773065178772548
        },
        "off_by_one_accuracy": {
          "mean": 0.801817695803261,
          "std": 0.02474332748506225
        },
        "rmse": {
          "mean": 1.1718359359460748,
          "std": 0.06753144790446572
        },
        "mae": {
          "mean": 0.8800320769847634,
          "std": 0.07952019739329579
        },
        "quadratic_weighted_kappa": {
          "mean": 0.42894289286900095,
          "std": 0.04331828760519323
        }
      },
      "surprise": {
        "perfect_accuracy": {
          "mean": 0.36388666132050257,
          "std": 0.07953996325913792
        },
        "off_by_one_accuracy": {
          "mean": 0.7971398021919274,
          "std": 0.04846928045147661
        },
        "rmse": {
          "mean": 1.2207998340018271,
          "std": 0.10991028149815987
        },
        "mae": {
          "mean": 0.8896017107725207,
          "std": 0.1272540923670307
        },
        "quadratic_weighted_kappa": {
          "mean": 0.29777845455143886,
          "std": 0.12027826278702922
        }
      }
    }
  },
  "fold_metrics": [
    {
      "story_classification": {
        "accuracy": 0.7241379310344828,
        "micro_precision": 0.7241379310344828,
        "micro_recall": 0.7241379310344828,
        "micro_f1": 0.7241379310344829,
        "macro_precision": 0.7188552188552189,
        "macro_recall": 0.7108108108108109,
        "macro_f1": 0.7131868131868131,
        "mcc": 0.4295907174837284
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.39080459770114945,
          "off_by_one_accuracy": 0.896551724137931,
          "rmse": 1.0449660391555822,
          "mae": 0.7471264367816092,
          "quadratic_weighted_kappa": 0.4400027102107188
        },
        "curiosity": {
          "perfect_accuracy": 0.25287356321839083,
          "off_by_one_accuracy": 0.7586206896551724,
          "rmse": 1.3042811910348766,
          "mae": 1.0344827586206897,
          "quadratic_weighted_kappa": 0.3431282522191613
        },
        "surprise": {
          "perfect_accuracy": 0.3333333333333333,
          "off_by_one_accuracy": 0.735632183908046,
          "rmse": 1.364576478442026,
          "mae": 1.0114942528735633,
          "quadratic_weighted_kappa": 0.20029505220154353
        }
      }
    },
    {
      "story_classification": {
        "accuracy": 0.7586206896551724,
        "micro_precision": 0.7586206896551724,
        "micro_recall": 0.7586206896551724,
        "micro_f1": 0.7586206896551724,
        "macro_precision": 0.7726986128625473,
        "macro_recall": 0.7337837837837837,
        "macro_f1": 0.7387387387387387,
        "mcc": 0.504985201951881
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.367816091954023,
          "off_by_one_accuracy": 0.7931034482758621,
          "rmse": 1.1938539928826468,
          "mae": 0.8735632183908046,
          "quadratic_weighted_kappa": 0.26331603387052716
        },
        "curiosity": {
          "perfect_accuracy": 0.39080459770114945,
          "off_by_one_accuracy": 0.8160919540229885,
          "rmse": 1.1547005383792515,
          "mae": 0.8275862068965517,
          "quadratic_weighted_kappa": 0.44671052631578945
        },
        "surprise": {
          "perfect_accuracy": 0.3218390804597701,
          "off_by_one_accuracy": 0.8160919540229885,
          "rmse": 1.2865350418053538,
          "mae": 0.9425287356321839,
          "quadratic_weighted_kappa": 0.17470355731225296
        }
      }
    },
    {
      "story_classification": {
        "accuracy": 0.735632183908046,
        "micro_precision": 0.735632183908046,
        "micro_recall": 0.735632183908046,
        "micro_f1": 0.735632183908046,
        "macro_precision": 0.7667741935483872,
        "macro_recall": 0.7199468085106383,
        "macro_f1": 0.7175723359209598,
        "mcc": 0.4844631357037068
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.3103448275862069,
          "off_by_one_accuracy": 0.8505747126436781,
          "rmse": 1.144702942944678,
          "mae": 0.8735632183908046,
          "quadratic_weighted_kappa": 0.34972462627852086
        },
        "curiosity": {
          "perfect_accuracy": 0.3218390804597701,
          "off_by_one_accuracy": 0.8275862068965517,
          "rmse": 1.1193183475751618,
          "mae": 0.8620689655172413,
          "quadratic_weighted_kappa": 0.4547806588857587
        },
        "surprise": {
          "perfect_accuracy": 0.3563218390804598,
          "off_by_one_accuracy": 0.8045977011494253,
          "rmse": 1.1596670152276025,
          "mae": 0.8620689655172413,
          "quadratic_weighted_kappa": 0.34987545506802065
        }
      }
    },
    {
      "story_classification": {
        "accuracy": 0.7816091954022989,
        "micro_precision": 0.7816091954022989,
        "micro_recall": 0.7816091954022989,
        "micro_f1": 0.781609195402299,
        "macro_precision": 0.8103448275862069,
        "macro_recall": 0.7761904761904762,
        "macro_f1": 0.7739641733898537,
        "mcc": 0.5855400437691198
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.39080459770114945,
          "off_by_one_accuracy": 0.9080459770114943,
          "rmse": 1.0283342182227606,
          "mae": 0.735632183908046,
          "quadratic_weighted_kappa": 0.4452453562517328
        },
        "curiosity": {
          "perfect_accuracy": 0.3333333333333333,
          "off_by_one_accuracy": 0.8160919540229885,
          "rmse": 1.1295406451144276,
          "mae": 0.8620689655172413,
          "quadratic_weighted_kappa": 0.4585365853658536
        },
        "surprise": {
          "perfect_accuracy": 0.5172413793103449,
          "off_by_one_accuracy": 0.8735632183908046,
          "rmse": 1.0449660391555822,
          "mae": 0.6551724137931034,
          "quadratic_weighted_kappa": 0.5060658578856152
        }
      }
    },
    {
      "story_classification": {
        "accuracy": 0.6627906976744186,
        "micro_precision": 0.6627906976744186,
        "micro_recall": 0.6627906976744186,
        "micro_f1": 0.6627906976744186,
        "macro_precision": 0.6583333333333333,
        "macro_recall": 0.6170868347338936,
        "macro_f1": 0.6124320124320124,
        "mcc": 0.272314148239613
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.3372093023255814,
          "off_by_one_accuracy": 0.8953488372093024,
          "rmse": 1.017292347818577,
          "mae": 0.7790697674418605,
          "quadratic_weighted_kappa": 0.4701647514883013
        },
        "curiosity": {
          "perfect_accuracy": 0.4186046511627907,
          "off_by_one_accuracy": 0.7906976744186046,
          "rmse": 1.151338957626657,
          "mae": 0.813953488372093,
          "quadratic_weighted_kappa": 0.4415584415584416
        },
        "surprise": {
          "perfect_accuracy": 0.29069767441860467,
          "off_by_one_accuracy": 0.7558139534883721,
          "rmse": 1.2482545953785713,
          "mae": 0.9767441860465116,
          "quadratic_weighted_kappa": 0.2579523502897618
        }
      }
    }
  ]
}