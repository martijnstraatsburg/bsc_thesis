# FIRST TIME SETUP!
# Login into Habrok with account via MobaXterm; see https://wiki.hpc.rug.nl/habrok/connecting_to_the_system/windows
# Start interactive job on single GPU A100 node; adjust time to whatever, but shorter time usually gets prio over longer time
srun --nodes=1 --ntasks=1 --partition=gpu --mem=120G --time=04:00:00 --gres=gpu:a100:1 --pty bash

# Load Python and CUDA modules
module load Python/3.11.5-GCCcore-13.2.0 CUDA/12.1.1

# Create a virtual environment (only needed for first time, after that only activation needed)
# See/follow documentation https://wiki.hpc.rug.nl/habrok/examples/python#python_environments
python3 -m venv $HOME/venvs/llm_env
source $HOME/venvs/llm_env/bin/activate
pip install --upgrade pip
pip install --upgrade wheel
pip install vllm

# Add HuggingFace parameters and tokens to venv
# Go to HuggingFace settings and create your own token
export HF_HOME=/tmp
export HF_TOKEN=hf_...

# Run and serve model via vllm; takes less than a minute and downloads every time into the /tmp/model folder
# Switch between Qwen/Qwen3-4B or Qwen/Qwen3-8B for either model
# Up the number after '--max-model-len' if you need more tokens to be taken as input
vllm serve Qwen/Qwen3-4B --download-dir /tmp/models --max-model-len 4096 --gpu-memory-utilization 0.95 --port 8000

# Open 2nd terminal and forward Habrok port to your local port
# Check what node it is running on, more specifically the number after 'a100gpu'
ssh -NL 8000:a100gpu1:8000 s3726134@login2.hb.hpc.rug.nl

# Check http://localhost:8000/v1/models to see if it is running
# Python scripts can be run locally now

# AFTER SETUP IN ORDER!
srun --nodes=1 --ntasks=1 --partition=gpu --mem=120G --time=04:00:00 --gres=gpu:a100:1 --pty bash

source $HOME/venvs/llm_env/bin/activate

vllm serve Qwen/Qwen3-1.7B --download-dir /tmp/models --max-model-len 4096 --gpu-memory-utilization 0.95 --port 8000
vllm serve Qwen/Qwen3-4B --download-dir /tmp/models --max-model-len 4096 --gpu-memory-utilization 0.95 --port 8000
vllm serve Qwen/Qwen3-8B --download-dir /tmp/models --max-model-len 4096 --gpu-memory-utilization 0.95 --port 8000
vllm serve Qwen/Qwen3-14B --download-dir /tmp/models --max-model-len 4096 --gpu-memory-utilization 0.95 --port 8000

# 2ND TERMINAL
ssh -NL 8000:a100gpu6:8000 s3726134@login2.hb.hpc.rug.nl