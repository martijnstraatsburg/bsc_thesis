{
  "average_metrics": {
    "story_classification": {
      "accuracy": {
        "mean": 0.7743651430098903,
        "std": 0.04458316249402523
      },
      "micro_precision": {
        "mean": 0.7743651430098903,
        "std": 0.04458316249402523
      },
      "micro_recall": {
        "mean": 0.7743651430098903,
        "std": 0.04458316249402523
      },
      "micro_f1": {
        "mean": 0.7743651430098903,
        "std": 0.04458316249402521
      },
      "macro_precision": {
        "mean": 0.7724808433269591,
        "std": 0.043273013026327425
      },
      "macro_recall": {
        "mean": 0.7726071828507438,
        "std": 0.042924239095483874
      },
      "macro_f1": {
        "mean": 0.7713866606435379,
        "std": 0.04377599409053887
      },
      "weighted_precision": {
        "mean": 0.7768132311538045,
        "std": 0.04270217260422056
      },
      "weighted_recall": {
        "mean": 0.7743651430098903,
        "std": 0.04458316249402523
      },
      "weighted_f1": {
        "mean": 0.7744448943447791,
        "std": 0.04435321901612668
      },
      "mcc": {
        "mean": 0.5450794989100539,
        "std": 0.0861421300101182
      }
    },
    "rating_metrics": {
      "suspense": {
        "perfect_accuracy": {
          "mean": 0.4194065757818765,
          "std": 0.019755505177120422
        },
        "off_by_one_accuracy": {
          "mean": 0.8940390269981288,
          "std": 0.016722985975213423
        },
        "level_accuracy": {
          "mean": 0.5115744453354718,
          "std": 0.030471916200275707
        },
        "rmse": {
          "mean": 0.9588500679808705,
          "std": 0.04551259280620679
        },
        "mae": {
          "mean": 0.6911520983694199,
          "std": 0.03746841116646321
        },
        "quadratic_weighted_kappa": {
          "mean": 0.4384811752414506,
          "std": 0.0611293652373042
        }
      },
      "curiosity": {
        "perfect_accuracy": {
          "mean": 0.44720662924351784,
          "std": 0.04896368407807203
        },
        "off_by_one_accuracy": {
          "mean": 0.8664795509222133,
          "std": 0.03862358464551511
        },
        "level_accuracy": {
          "mean": 0.49326383319967926,
          "std": 0.049870972170408354
        },
        "rmse": {
          "mean": 0.9787208214489993,
          "std": 0.08339379188885457
        },
        "mae": {
          "mean": 0.6886126704089816,
          "std": 0.08556052579856852
        },
        "quadratic_weighted_kappa": {
          "mean": 0.4062581262551747,
          "std": 0.11562651385131238
        }
      },
      "surprise": {
        "perfect_accuracy": {
          "mean": 0.3895215183106121,
          "std": 0.03278092422568019
        },
        "off_by_one_accuracy": {
          "mean": 0.9170542635658915,
          "std": 0.01837121228130483
        },
        "level_accuracy": {
          "mean": 0.47022186581128034,
          "std": 0.040415232851839386
        },
        "rmse": {
          "mean": 0.9441433539275069,
          "std": 0.048905795637342826
        },
        "mae": {
          "mean": 0.7003207698476344,
          "std": 0.03550537287949205
        },
        "quadratic_weighted_kappa": {
          "mean": 0.4155256485507624,
          "std": 0.10522825959484461
        }
      }
    }
  },
  "fold_metrics": [
    {
      "story_classification": {
        "accuracy": 0.7931034482758621,
        "micro_precision": 0.7931034482758621,
        "micro_recall": 0.7931034482758621,
        "micro_f1": 0.7931034482758621,
        "macro_precision": 0.7861952861952861,
        "macro_recall": 0.7802197802197802,
        "macro_f1": 0.7827413984461709,
        "weighted_precision": 0.7917876078795618,
        "weighted_recall": 0.7931034482758621,
        "weighted_f1": 0.7920127061885261,
        "mcc": 0.5663835456564048
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.39080459770114945,
          "off_by_one_accuracy": 0.896551724137931,
          "level_accuracy": 0.45977011494252873,
          "rmse": 0.988438917815802,
          "mae": 0.7241379310344828,
          "quadratic_weighted_kappa": 0.4468546637744035
        },
        "curiosity": {
          "perfect_accuracy": 0.40229885057471265,
          "off_by_one_accuracy": 0.8275862068965517,
          "level_accuracy": 0.4482758620689655,
          "rmse": 1.0559083903140614,
          "mae": 0.7701149425287356,
          "quadratic_weighted_kappa": 0.35722446492497517
        },
        "surprise": {
          "perfect_accuracy": 0.3563218390804598,
          "off_by_one_accuracy": 0.9310344827586207,
          "level_accuracy": 0.45977011494252873,
          "rmse": 0.922266074754828,
          "mae": 0.7126436781609196,
          "quadratic_weighted_kappa": 0.5055299539170507
        }
      }
    },
    {
      "story_classification": {
        "accuracy": 0.7701149425287356,
        "micro_precision": 0.7701149425287356,
        "micro_recall": 0.7701149425287356,
        "micro_f1": 0.7701149425287356,
        "macro_precision": 0.7706131078224101,
        "macro_recall": 0.7723404255319148,
        "macro_f1": 0.7698412698412699,
        "weighted_precision": 0.7741002648781318,
        "weighted_recall": 0.7701149425287356,
        "weighted_f1": 0.7704798394453568,
        "mcc": 0.5429507857582274
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.40229885057471265,
          "off_by_one_accuracy": 0.8620689655172413,
          "level_accuracy": 0.4942528735632184,
          "rmse": 1.0339078862458586,
          "mae": 0.7471264367816092,
          "quadratic_weighted_kappa": 0.3466042154566744
        },
        "curiosity": {
          "perfect_accuracy": 0.40229885057471265,
          "off_by_one_accuracy": 0.8160919540229885,
          "level_accuracy": 0.4367816091954023,
          "rmse": 1.072112534837795,
          "mae": 0.7816091954022989,
          "quadratic_weighted_kappa": 0.23509759099701077
        },
        "surprise": {
          "perfect_accuracy": 0.367816091954023,
          "off_by_one_accuracy": 0.9425287356321839,
          "level_accuracy": 0.42528735632183906,
          "rmse": 0.8969937018449045,
          "mae": 0.6896551724137931,
          "quadratic_weighted_kappa": 0.43953616786305916
        }
      }
    },
    {
      "story_classification": {
        "accuracy": 0.735632183908046,
        "micro_precision": 0.735632183908046,
        "micro_recall": 0.735632183908046,
        "micro_f1": 0.735632183908046,
        "macro_precision": 0.7329787234042553,
        "macro_recall": 0.733974358974359,
        "macro_f1": 0.7333777481678881,
        "weighted_precision": 0.7363903154805576,
        "weighted_recall": 0.735632183908046,
        "weighted_f1": 0.7359139883755658,
        "mcc": 0.4669520209322372
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.42528735632183906,
          "off_by_one_accuracy": 0.9080459770114943,
          "level_accuracy": 0.5402298850574713,
          "rmse": 0.922266074754828,
          "mae": 0.6666666666666666,
          "quadratic_weighted_kappa": 0.46125523012552305
        },
        "curiosity": {
          "perfect_accuracy": 0.45977011494252873,
          "off_by_one_accuracy": 0.8850574712643678,
          "level_accuracy": 0.5287356321839081,
          "rmse": 0.9407749312478345,
          "mae": 0.6551724137931034,
          "quadratic_weighted_kappa": 0.4486873508353223
        },
        "surprise": {
          "perfect_accuracy": 0.367816091954023,
          "off_by_one_accuracy": 0.896551724137931,
          "level_accuracy": 0.45977011494252873,
          "rmse": 1.0,
          "mae": 0.7471264367816092,
          "quadratic_weighted_kappa": 0.24226649314245674
        }
      }
    },
    {
      "story_classification": {
        "accuracy": 0.7241379310344828,
        "micro_precision": 0.7241379310344828,
        "micro_recall": 0.7241379310344828,
        "micro_f1": 0.7241379310344829,
        "macro_precision": 0.7261904761904762,
        "macro_recall": 0.7283653846153846,
        "macro_f1": 0.7238095238095238,
        "weighted_precision": 0.7323481116584565,
        "weighted_recall": 0.7241379310344828,
        "weighted_f1": 0.7247947454844007,
        "mcc": 0.4545506576458782
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.4367816091954023,
          "off_by_one_accuracy": 0.896551724137931,
          "level_accuracy": 0.5287356321839081,
          "rmse": 0.9346460390922355,
          "mae": 0.6666666666666666,
          "quadratic_weighted_kappa": 0.40635661698689174
        },
        "curiosity": {
          "perfect_accuracy": 0.4367816091954023,
          "off_by_one_accuracy": 0.8850574712643678,
          "level_accuracy": 0.4827586206896552,
          "rmse": 0.982607368881035,
          "mae": 0.6896551724137931,
          "quadratic_weighted_kappa": 0.40147420147420143
        },
        "surprise": {
          "perfect_accuracy": 0.41379310344827586,
          "off_by_one_accuracy": 0.896551724137931,
          "level_accuracy": 0.45977011494252873,
          "rmse": 1.0057307059414877,
          "mae": 0.7126436781609196,
          "quadratic_weighted_kappa": 0.3587939698492463
        }
      }
    },
    {
      "story_classification": {
        "accuracy": 0.8488372093023255,
        "micro_precision": 0.8488372093023255,
        "micro_recall": 0.8488372093023255,
        "micro_f1": 0.8488372093023255,
        "macro_precision": 0.8464266230223677,
        "macro_recall": 0.8481359649122806,
        "macro_f1": 0.8471633629528367,
        "weighted_precision": 0.8494398558723151,
        "weighted_recall": 0.8488372093023255,
        "weighted_f1": 0.8490231922300465,
        "mcc": 0.6945604845575218
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.4418604651162791,
          "off_by_one_accuracy": 0.9069767441860465,
          "level_accuracy": 0.5348837209302325,
          "rmse": 0.914991421995628,
          "mae": 0.6511627906976745,
          "quadratic_weighted_kappa": 0.5313351498637602
        },
        "curiosity": {
          "perfect_accuracy": 0.5348837209302325,
          "off_by_one_accuracy": 0.9186046511627907,
          "level_accuracy": 0.5697674418604651,
          "rmse": 0.8422008819642707,
          "mae": 0.5465116279069767,
          "quadratic_weighted_kappa": 0.5888070230443643
        },
        "surprise": {
          "perfect_accuracy": 0.4418604651162791,
          "off_by_one_accuracy": 0.9186046511627907,
          "level_accuracy": 0.5465116279069767,
          "rmse": 0.8957262870963143,
          "mae": 0.6395348837209303,
          "quadratic_weighted_kappa": 0.5315016579819991
        }
      }
    }
  ]
}