{
  "average_metrics": {
    "story_classification": {
      "accuracy": {
        "mean": 0.7466452820101577,
        "std": 0.02639207983129055
      },
      "micro_precision": {
        "mean": 0.7466452820101577,
        "std": 0.02639207983129055
      },
      "micro_recall": {
        "mean": 0.7466452820101577,
        "std": 0.02639207983129055
      },
      "micro_f1": {
        "mean": 0.7466452820101577,
        "std": 0.02639207983129055
      },
      "macro_precision": {
        "mean": 0.7584907051748535,
        "std": 0.021130207133961087
      },
      "macro_recall": {
        "mean": 0.7285992343789095,
        "std": 0.027242260042794307
      },
      "macro_f1": {
        "mean": 0.7306412182128316,
        "std": 0.028997979635976272
      },
      "weighted_precision": {
        "mean": 0.7552042234146585,
        "std": 0.021882640098144276
      },
      "weighted_recall": {
        "mean": 0.7466452820101577,
        "std": 0.02639207983129055
      },
      "weighted_f1": {
        "mean": 0.7386023700078121,
        "std": 0.028985873866019512
      },
      "mcc": {
        "mean": 0.4860722278546229,
        "std": 0.04847635909335748
      }
    },
    "rating_metrics": {
      "suspense": {
        "perfect_accuracy": {
          "mean": 0.3456829724672547,
          "std": 0.024971451353022804
        },
        "off_by_one_accuracy": {
          "mean": 0.8688051323175621,
          "std": 0.0406221941009846
        },
        "level_accuracy": {
          "mean": 0.4931034482758621,
          "std": 0.03605606239392674
        },
        "rmse": {
          "mean": 1.0855968956869773,
          "std": 0.0862884652824284
        },
        "mae": {
          "mean": 0.813124832932371,
          "std": 0.06038233824418399
        },
        "quadratic_weighted_kappa": {
          "mean": 0.4034913427810826,
          "std": 0.10728042974053575
        }
      },
      "curiosity": {
        "perfect_accuracy": {
          "mean": 0.32731889869018976,
          "std": 0.06229164524646664
        },
        "off_by_one_accuracy": {
          "mean": 0.7766907244052392,
          "std": 0.05750584240435202
        },
        "level_accuracy": {
          "mean": 0.5256081261694734,
          "std": 0.08777131609697388
        },
        "rmse": {
          "mean": 1.2155696517644186,
          "std": 0.12121033522809148
        },
        "mae": {
          "mean": 0.9259288960171077,
          "std": 0.13012453662636408
        },
        "quadratic_weighted_kappa": {
          "mean": 0.37915259662257284,
          "std": 0.16064304357766013
        }
      },
      "surprise": {
        "perfect_accuracy": {
          "mean": 0.3688051323175621,
          "std": 0.035034062516207505
        },
        "off_by_one_accuracy": {
          "mean": 0.8180433039294306,
          "std": 0.04421083793151983
        },
        "level_accuracy": {
          "mean": 0.5139802191927292,
          "std": 0.04977143854489129
        },
        "rmse": {
          "mean": 1.1946420460801004,
          "std": 0.0873722507772064
        },
        "mae": {
          "mean": 0.8637797380379577,
          "std": 0.07694643971704525
        },
        "quadratic_weighted_kappa": {
          "mean": 0.32801182221909475,
          "std": 0.1157854902414869
        }
      }
    }
  },
  "fold_metrics": [
    {
      "story_classification": {
        "accuracy": 0.7586206896551724,
        "micro_precision": 0.7586206896551724,
        "micro_recall": 0.7586206896551724,
        "micro_f1": 0.7586206896551724,
        "macro_precision": 0.7616645649432534,
        "macro_recall": 0.728021978021978,
        "macro_f1": 0.7349484984767154,
        "weighted_precision": 0.7601861112318997,
        "weighted_recall": 0.7586206896551724,
        "weighted_f1": 0.7504264696318604,
        "mcc": 0.4885295146728434
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.3563218390804598,
          "off_by_one_accuracy": 0.8620689655172413,
          "level_accuracy": 0.4942528735632184,
          "rmse": 1.1346172578623508,
          "mae": 0.8275862068965517,
          "quadratic_weighted_kappa": 0.38909090909090915
        },
        "curiosity": {
          "perfect_accuracy": 0.26436781609195403,
          "off_by_one_accuracy": 0.7241379310344828,
          "level_accuracy": 0.4482758620689655,
          "rmse": 1.2954385047312087,
          "mae": 1.0344827586206897,
          "quadratic_weighted_kappa": 0.31819645732689206
        },
        "surprise": {
          "perfect_accuracy": 0.3218390804597701,
          "off_by_one_accuracy": 0.8275862068965517,
          "level_accuracy": 0.4827586206896552,
          "rmse": 1.1938539928826468,
          "mae": 0.896551724137931,
          "quadratic_weighted_kappa": 0.38396528095020566
        }
      }
    },
    {
      "story_classification": {
        "accuracy": 0.7126436781609196,
        "micro_precision": 0.7126436781609196,
        "micro_recall": 0.7126436781609196,
        "micro_f1": 0.7126436781609196,
        "macro_precision": 0.7305555555555556,
        "macro_recall": 0.6986702127659574,
        "macro_f1": 0.6966104059143535,
        "weighted_precision": 0.7267560664112388,
        "weighted_recall": 0.7126436781609196,
        "weighted_f1": 0.7022220512006516,
        "mcc": 0.42803981719739925
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.3218390804597701,
          "off_by_one_accuracy": 0.8160919540229885,
          "level_accuracy": 0.42528735632183906,
          "rmse": 1.18418699983352,
          "mae": 0.896551724137931,
          "quadratic_weighted_kappa": 0.246378869639307
        },
        "curiosity": {
          "perfect_accuracy": 0.2413793103448276,
          "off_by_one_accuracy": 0.7011494252873564,
          "level_accuracy": 0.40229885057471265,
          "rmse": 1.3937462952891333,
          "mae": 1.1149425287356323,
          "quadratic_weighted_kappa": 0.11036485750589942
        },
        "surprise": {
          "perfect_accuracy": 0.3563218390804598,
          "off_by_one_accuracy": 0.735632183908046,
          "level_accuracy": 0.4367816091954023,
          "rmse": 1.3347693416475743,
          "mae": 0.9770114942528736,
          "quadratic_weighted_kappa": 0.14700487064330447
        }
      }
    },
    {
      "story_classification": {
        "accuracy": 0.735632183908046,
        "micro_precision": 0.735632183908046,
        "micro_recall": 0.735632183908046,
        "micro_f1": 0.735632183908046,
        "macro_precision": 0.7563051702395964,
        "macro_recall": 0.7171474358974359,
        "macro_f1": 0.7175723359209597,
        "weighted_precision": 0.7509892594686264,
        "weighted_recall": 0.735632183908046,
        "weighted_f1": 0.7249604555520404,
        "mcc": 0.4718305226446639
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.367816091954023,
          "off_by_one_accuracy": 0.8390804597701149,
          "level_accuracy": 0.5172413793103449,
          "rmse": 1.1346172578623508,
          "mae": 0.8275862068965517,
          "quadratic_weighted_kappa": 0.35547030030427296
        },
        "curiosity": {
          "perfect_accuracy": 0.39080459770114945,
          "off_by_one_accuracy": 0.8045977011494253,
          "level_accuracy": 0.5862068965517241,
          "rmse": 1.093344547181068,
          "mae": 0.8045977011494253,
          "quadratic_weighted_kappa": 0.5030755711775043
        },
        "surprise": {
          "perfect_accuracy": 0.367816091954023,
          "off_by_one_accuracy": 0.8620689655172413,
          "level_accuracy": 0.5287356321839081,
          "rmse": 1.1547005383792515,
          "mae": 0.8275862068965517,
          "quadratic_weighted_kappa": 0.2747916067835585
        }
      }
    },
    {
      "story_classification": {
        "accuracy": 0.735632183908046,
        "micro_precision": 0.735632183908046,
        "micro_recall": 0.735632183908046,
        "micro_f1": 0.735632183908046,
        "macro_precision": 0.7487893462469734,
        "macro_recall": 0.719551282051282,
        "macro_f1": 0.7208815734412052,
        "weighted_precision": 0.7449695249227687,
        "weighted_recall": 0.735632183908046,
        "weighted_f1": 0.7275193481512835,
        "mcc": 0.4674270849200903
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.3103448275862069,
          "off_by_one_accuracy": 0.896551724137931,
          "level_accuracy": 0.5287356321839081,
          "rmse": 1.0283342182227606,
          "mae": 0.8045977011494253,
          "quadratic_weighted_kappa": 0.4573559322033899
        },
        "curiosity": {
          "perfect_accuracy": 0.3563218390804598,
          "off_by_one_accuracy": 0.7931034482758621,
          "level_accuracy": 0.5517241379310345,
          "rmse": 1.2223963651627971,
          "mae": 0.896551724137931,
          "quadratic_weighted_kappa": 0.3911498708010336
        },
        "surprise": {
          "perfect_accuracy": 0.367816091954023,
          "off_by_one_accuracy": 0.8160919540229885,
          "level_accuracy": 0.5402298850574713,
          "rmse": 1.2223963651627971,
          "mae": 0.8735632183908046,
          "quadratic_weighted_kappa": 0.3383643383643383
        }
      }
    },
    {
      "story_classification": {
        "accuracy": 0.7906976744186046,
        "micro_precision": 0.7906976744186046,
        "micro_recall": 0.7906976744186046,
        "micro_f1": 0.7906976744186046,
        "macro_precision": 0.7951388888888888,
        "macro_recall": 0.7796052631578947,
        "macro_f1": 0.7831932773109245,
        "weighted_precision": 0.7931201550387598,
        "weighted_recall": 0.7906976744186046,
        "weighted_f1": 0.7878835255032246,
        "mcc": 0.5745341998381173
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.37209302325581395,
          "off_by_one_accuracy": 0.9302325581395349,
          "level_accuracy": 0.5,
          "rmse": 0.9462287446539036,
          "mae": 0.7093023255813954,
          "quadratic_weighted_kappa": 0.5691607026675342
        },
        "curiosity": {
          "perfect_accuracy": 0.38372093023255816,
          "off_by_one_accuracy": 0.8604651162790697,
          "level_accuracy": 0.6395348837209303,
          "rmse": 1.072922546457886,
          "mae": 0.7790697674418605,
          "quadratic_weighted_kappa": 0.5729762263015348
        },
        "surprise": {
          "perfect_accuracy": 0.43023255813953487,
          "off_by_one_accuracy": 0.8488372093023255,
          "level_accuracy": 0.5813953488372093,
          "rmse": 1.0674899923282326,
          "mae": 0.7441860465116279,
          "quadratic_weighted_kappa": 0.495933014354067
        }
      }
    }
  ]
}