{
  "average_metrics": {
    "story_classification": {
      "accuracy": {
        "mean": 0.8061166429587482,
        "std": 0.10494897559083251
      },
      "micro_precision": {
        "mean": 0.8061166429587482,
        "std": 0.10494897559083251
      },
      "micro_recall": {
        "mean": 0.8061166429587482,
        "std": 0.10494897559083251
      },
      "micro_f1": {
        "mean": 0.8061166429587482,
        "std": 0.10494897559083251
      },
      "macro_precision": {
        "mean": 0.8070688580425422,
        "std": 0.11030499898477664
      },
      "macro_recall": {
        "mean": 0.7859806497849976,
        "std": 0.10122758302966976
      },
      "macro_f1": {
        "mean": 0.7891443228732908,
        "std": 0.10733164031905947
      },
      "weighted_precision": {
        "mean": 0.8155032088189982,
        "std": 0.09578953457179638
      },
      "weighted_recall": {
        "mean": 0.8061166429587482,
        "std": 0.10494897559083251
      },
      "weighted_f1": {
        "mean": 0.8043878388430222,
        "std": 0.10248300700518262
      },
      "mcc": {
        "mean": 0.5920220422459138,
        "std": 0.20959478809362028
      }
    },
    "rating_metrics": {
      "suspense": {
        "perfect_accuracy": {
          "mean": 0.27425320056899005,
          "std": 0.08796096552749706
        },
        "off_by_one_accuracy": {
          "mean": 0.8709815078236132,
          "std": 0.020084623151889375
        },
        "level_accuracy": {
          "mean": 0.44096728307254623,
          "std": 0.09179539477071755
        },
        "rmse": {
          "mean": 1.1124624030684171,
          "std": 0.10180772492570533
        },
        "mae": {
          "mean": 0.8817923186344239,
          "std": 0.11406299163051742
        },
        "quadratic_weighted_kappa": {
          "mean": 0.32601975124493665,
          "std": 0.15155451800715203
        }
      },
      "curiosity": {
        "perfect_accuracy": {
          "mean": 0.3064011379800854,
          "std": 0.08277582973049596
        },
        "off_by_one_accuracy": {
          "mean": 0.8223328591749646,
          "std": 0.07009844173404627
        },
        "level_accuracy": {
          "mean": 0.5108108108108109,
          "std": 0.05156428115767274
        },
        "rmse": {
          "mean": 1.1987414278663355,
          "std": 0.18128021639231304
        },
        "mae": {
          "mean": 0.919914651493599,
          "std": 0.15877243782528624
        },
        "quadratic_weighted_kappa": {
          "mean": 0.44402032992751606,
          "std": 0.14754081258227167
        }
      },
      "surprise": {
        "perfect_accuracy": {
          "mean": 0.30654338549075394,
          "std": 0.10076306017752841
        },
        "off_by_one_accuracy": {
          "mean": 0.8332859174964439,
          "std": 0.05523454537371141
        },
        "level_accuracy": {
          "mean": 0.5056899004267426,
          "std": 0.08595328637624668
        },
        "rmse": {
          "mean": 1.1785759707480423,
          "std": 0.14164223190170933
        },
        "mae": {
          "mean": 0.903271692745377,
          "std": 0.17141385753092475
        },
        "quadratic_weighted_kappa": {
          "mean": 0.30506893398223633,
          "std": 0.147070540207327
        }
      }
    }
  },
  "fold_metrics": [
    {
      "story_classification": {
        "accuracy": 0.6486486486486487,
        "micro_precision": 0.6486486486486487,
        "micro_recall": 0.6486486486486487,
        "micro_f1": 0.6486486486486487,
        "macro_precision": 0.6447368421052632,
        "macro_recall": 0.6586538461538461,
        "macro_f1": 0.6391597899474869,
        "weighted_precision": 0.6877667140825036,
        "weighted_recall": 0.6486486486486487,
        "weighted_f1": 0.6565560308996168,
        "mcc": 0.30307132282789107
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.24324324324324326,
          "off_by_one_accuracy": 0.8378378378378378,
          "level_accuracy": 0.5405405405405406,
          "rmse": 1.2302493704584911,
          "mae": 0.972972972972973,
          "quadratic_weighted_kappa": 0.2904109589041096
        },
        "curiosity": {
          "perfect_accuracy": 0.2702702702702703,
          "off_by_one_accuracy": 0.7297297297297297,
          "level_accuracy": 0.5135135135135135,
          "rmse": 1.4886961463697743,
          "mae": 1.135135135135135,
          "quadratic_weighted_kappa": 0.26997112608277174
        },
        "surprise": {
          "perfect_accuracy": 0.13513513513513514,
          "off_by_one_accuracy": 0.7297297297297297,
          "level_accuracy": 0.40540540540540543,
          "rmse": 1.4425952589278397,
          "mae": 1.2162162162162162,
          "quadratic_weighted_kappa": 0.0506497834055315
        }
      }
    },
    {
      "story_classification": {
        "accuracy": 0.7297297297297297,
        "micro_precision": 0.7297297297297297,
        "micro_recall": 0.7297297297297297,
        "micro_f1": 0.7297297297297297,
        "macro_precision": 0.7133333333333334,
        "macro_recall": 0.6987577639751552,
        "macro_f1": 0.703525641025641,
        "weighted_precision": 0.7246846846846846,
        "weighted_recall": 0.7297297297297297,
        "weighted_f1": 0.72496534996535,
        "mcc": 0.4118332493361836
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.2702702702702703,
          "off_by_one_accuracy": 0.8648648648648649,
          "level_accuracy": 0.40540540540540543,
          "rmse": 1.1270626736212455,
          "mae": 0.8918918918918919,
          "quadratic_weighted_kappa": 0.2198295199641096
        },
        "curiosity": {
          "perfect_accuracy": 0.21621621621621623,
          "off_by_one_accuracy": 0.7567567567567568,
          "level_accuracy": 0.43243243243243246,
          "rmse": 1.3355836865519823,
          "mae": 1.0810810810810811,
          "quadratic_weighted_kappa": 0.26622596153846145
        },
        "surprise": {
          "perfect_accuracy": 0.3783783783783784,
          "off_by_one_accuracy": 0.8648648648648649,
          "level_accuracy": 0.6216216216216216,
          "rmse": 1.138989594902999,
          "mae": 0.8108108108108109,
          "quadratic_weighted_kappa": 0.2974683544303798
        }
      }
    },
    {
      "story_classification": {
        "accuracy": 0.868421052631579,
        "micro_precision": 0.868421052631579,
        "micro_recall": 0.868421052631579,
        "micro_f1": 0.868421052631579,
        "macro_precision": 0.8814102564102564,
        "macro_recall": 0.844927536231884,
        "macro_f1": 0.8563869992441422,
        "weighted_precision": 0.8739878542510121,
        "weighted_recall": 0.868421052631579,
        "weighted_f1": 0.8651390380713689,
        "mcc": 0.7254209813264596
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.2631578947368421,
          "off_by_one_accuracy": 0.868421052631579,
          "level_accuracy": 0.42105263157894735,
          "rmse": 1.063756996389881,
          "mae": 0.868421052631579,
          "quadratic_weighted_kappa": 0.37681159420289856
        },
        "curiosity": {
          "perfect_accuracy": 0.3157894736842105,
          "off_by_one_accuracy": 0.868421052631579,
          "level_accuracy": 0.5,
          "rmse": 1.038723913473187,
          "mae": 0.8157894736842105,
          "quadratic_weighted_kappa": 0.5655326268823202
        },
        "surprise": {
          "perfect_accuracy": 0.2894736842105263,
          "off_by_one_accuracy": 0.8421052631578947,
          "level_accuracy": 0.4473684210526316,
          "rmse": 1.1470786693528088,
          "mae": 0.8947368421052632,
          "quadratic_weighted_kappa": 0.3286219081272086
        }
      }
    },
    {
      "story_classification": {
        "accuracy": 0.8378378378378378,
        "micro_precision": 0.8378378378378378,
        "micro_recall": 0.8378378378378378,
        "micro_f1": 0.8378378378378378,
        "macro_precision": 0.8551587301587301,
        "macro_recall": 0.7868589743589745,
        "macro_f1": 0.805944055944056,
        "weighted_precision": 0.8451308451308452,
        "weighted_recall": 0.8378378378378378,
        "weighted_f1": 0.8293328293328295,
        "mcc": 0.6383744013287784
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.16216216216216217,
          "off_by_one_accuracy": 0.8918918918918919,
          "level_accuracy": 0.2972972972972973,
          "rmse": 1.196842693269434,
          "mae": 1.0,
          "quadratic_weighted_kappa": 0.15291576673866092
        },
        "curiosity": {
          "perfect_accuracy": 0.4594594594594595,
          "off_by_one_accuracy": 0.8378378378378378,
          "level_accuracy": 0.5945945945945946,
          "rmse": 1.0780362527123855,
          "mae": 0.7297297297297297,
          "quadratic_weighted_kappa": 0.5060540204905308
        },
        "surprise": {
          "perfect_accuracy": 0.2972972972972973,
          "off_by_one_accuracy": 0.8378378378378378,
          "level_accuracy": 0.4594594594594595,
          "rmse": 1.150792911137501,
          "mae": 0.8918918918918919,
          "quadratic_weighted_kappa": 0.33952641165755926
        }
      }
    },
    {
      "story_classification": {
        "accuracy": 0.9459459459459459,
        "micro_precision": 0.9459459459459459,
        "micro_recall": 0.9459459459459459,
        "micro_f1": 0.9459459459459459,
        "macro_precision": 0.9407051282051282,
        "macro_recall": 0.9407051282051282,
        "macro_f1": 0.9407051282051282,
        "weighted_precision": 0.9459459459459459,
        "weighted_recall": 0.9459459459459459,
        "weighted_f1": 0.9459459459459459,
        "mcc": 0.8814102564102564
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.43243243243243246,
          "off_by_one_accuracy": 0.8918918918918919,
          "level_accuracy": 0.5405405405405406,
          "rmse": 0.944400281603035,
          "mae": 0.6756756756756757,
          "quadratic_weighted_kappa": 0.5901309164149043
        },
        "curiosity": {
          "perfect_accuracy": 0.2702702702702703,
          "off_by_one_accuracy": 0.918918918918919,
          "level_accuracy": 0.5135135135135135,
          "rmse": 1.0526671402243484,
          "mae": 0.8378378378378378,
          "quadratic_weighted_kappa": 0.612317914643496
        },
        "surprise": {
          "perfect_accuracy": 0.43243243243243246,
          "off_by_one_accuracy": 0.8918918918918919,
          "level_accuracy": 0.5945945945945946,
          "rmse": 1.0134234194190634,
          "mae": 0.7027027027027027,
          "quadratic_weighted_kappa": 0.5090782122905028
        }
      }
    }
  ]
}