{
  "average_metrics": {
    "story_classification": {
      "accuracy": {
        "mean": 0.7281742849505479,
        "std": 0.06819065351694466
      },
      "micro_precision": {
        "mean": 0.7281742849505479,
        "std": 0.06819065351694466
      },
      "micro_recall": {
        "mean": 0.7281742849505479,
        "std": 0.06819065351694466
      },
      "micro_f1": {
        "mean": 0.7281742849505479,
        "std": 0.06819065351694464
      },
      "macro_precision": {
        "mean": 0.7270611137733365,
        "std": 0.06714584109739898
      },
      "macro_recall": {
        "mean": 0.7241340000984458,
        "std": 0.06363738354408048
      },
      "macro_f1": {
        "mean": 0.7240852609552426,
        "std": 0.06560263362603005
      },
      "weighted_precision": {
        "mean": 0.7304016504869392,
        "std": 0.06600436312252525
      },
      "weighted_recall": {
        "mean": 0.7281742849505479,
        "std": 0.06819065351694466
      },
      "weighted_f1": {
        "mean": 0.7278020537398433,
        "std": 0.06736165684749246
      },
      "mcc": {
        "mean": 0.4511630774745218,
        "std": 0.13070532266279938
      }
    },
    "rating_metrics": {
      "suspense": {
        "perfect_accuracy": {
          "mean": 0.4009890403635391,
          "std": 0.0394308508855835
        },
        "off_by_one_accuracy": {
          "mean": 0.8065223202352312,
          "std": 0.027156408895779107
        },
        "level_accuracy": {
          "mean": 0.610638866613205,
          "std": 0.021547495487459798
        },
        "rmse": {
          "mean": 1.1802948252706034,
          "std": 0.09799968532445065
        },
        "mae": {
          "mean": 0.8362202619620422,
          "std": 0.08756306927210589
        },
        "quadratic_weighted_kappa": {
          "mean": 0.3631454276927537,
          "std": 0.07279592348251361
        }
      },
      "curiosity": {
        "perfect_accuracy": {
          "mean": 0.2948944132584871,
          "std": 0.0690362244226018
        },
        "off_by_one_accuracy": {
          "mean": 0.7927559476075915,
          "std": 0.07242412848054237
        },
        "level_accuracy": {
          "mean": 0.5392943063352045,
          "std": 0.06212486555040686
        },
        "rmse": {
          "mean": 1.2484964602358413,
          "std": 0.14618090923662508
        },
        "mae": {
          "mean": 0.9630045442395081,
          "std": 0.14572790274584657
        },
        "quadratic_weighted_kappa": {
          "mean": 0.4066451217764671,
          "std": 0.12994098646601743
        }
      },
      "surprise": {
        "perfect_accuracy": {
          "mean": 0.2810478481689388,
          "std": 0.04048341969680112
        },
        "off_by_one_accuracy": {
          "mean": 0.705105586741513,
          "std": 0.023199765314384495
        },
        "level_accuracy": {
          "mean": 0.5898422881582465,
          "std": 0.03070281159475847
        },
        "rmse": {
          "mean": 1.4558626500490623,
          "std": 0.017268847795300933
        },
        "mae": {
          "mean": 1.1152365677626304,
          "std": 0.034965548266495845
        },
        "quadratic_weighted_kappa": {
          "mean": 0.22463547758203584,
          "std": 0.05034217309993799
        }
      }
    }
  },
  "fold_metrics": [
    {
      "story_classification": {
        "accuracy": 0.8160919540229885,
        "micro_precision": 0.8160919540229885,
        "micro_recall": 0.8160919540229885,
        "micro_f1": 0.8160919540229885,
        "macro_precision": 0.8106060606060606,
        "macro_recall": 0.8041208791208792,
        "macro_f1": 0.806881243063263,
        "weighted_precision": 0.8150470219435736,
        "weighted_recall": 0.8160919540229885,
        "weighted_f1": 0.8151224055009123,
        "mcc": 0.6146927304329804
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.42528735632183906,
          "off_by_one_accuracy": 0.8390804597701149,
          "level_accuracy": 0.5977011494252874,
          "rmse": 1.144702942944678,
          "mae": 0.7816091954022989,
          "quadratic_weighted_kappa": 0.40425276309466585
        },
        "curiosity": {
          "perfect_accuracy": 0.39080459770114945,
          "off_by_one_accuracy": 0.896551724137931,
          "level_accuracy": 0.632183908045977,
          "rmse": 1.0170952554312156,
          "mae": 0.735632183908046,
          "quadratic_weighted_kappa": 0.6040254880145646
        },
        "surprise": {
          "perfect_accuracy": 0.3563218390804598,
          "off_by_one_accuracy": 0.7011494252873564,
          "level_accuracy": 0.5747126436781609,
          "rmse": 1.4739110533215525,
          "mae": 1.0689655172413792,
          "quadratic_weighted_kappa": 0.18043164033295134
        }
      }
    },
    {
      "story_classification": {
        "accuracy": 0.7701149425287356,
        "micro_precision": 0.7701149425287356,
        "micro_recall": 0.7701149425287356,
        "micro_f1": 0.7701149425287356,
        "macro_precision": 0.7744173140954496,
        "macro_recall": 0.7630319148936171,
        "macro_f1": 0.7648648648648648,
        "weighted_precision": 0.7728322298340287,
        "weighted_recall": 0.7701149425287356,
        "weighted_f1": 0.7676918297607953,
        "mcc": 0.5373286205163066
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.4367816091954023,
          "off_by_one_accuracy": 0.7931034482758621,
          "level_accuracy": 0.5977011494252874,
          "rmse": 1.1396712572986316,
          "mae": 0.7931034482758621,
          "quadratic_weighted_kappa": 0.3484657697660548
        },
        "curiosity": {
          "perfect_accuracy": 0.3563218390804598,
          "off_by_one_accuracy": 0.7701149425287356,
          "level_accuracy": 0.5057471264367817,
          "rmse": 1.2730630994465333,
          "mae": 0.9310344827586207,
          "quadratic_weighted_kappa": 0.3040789697622964
        },
        "surprise": {
          "perfect_accuracy": 0.26436781609195403,
          "off_by_one_accuracy": 0.6666666666666666,
          "level_accuracy": 0.5517241379310345,
          "rmse": 1.4423798979862907,
          "mae": 1.1379310344827587,
          "quadratic_weighted_kappa": 0.22133214656579148
        }
      }
    },
    {
      "story_classification": {
        "accuracy": 0.632183908045977,
        "micro_precision": 0.632183908045977,
        "micro_recall": 0.632183908045977,
        "micro_f1": 0.632183908045977,
        "macro_precision": 0.6341269841269841,
        "macro_recall": 0.6354166666666666,
        "macro_f1": 0.6317460317460317,
        "weighted_precision": 0.6399562123700054,
        "weighted_recall": 0.632183908045977,
        "weighted_f1": 0.6330596606458675,
        "mcc": 0.26954056541106464
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.3333333333333333,
          "off_by_one_accuracy": 0.7701149425287356,
          "level_accuracy": 0.5862068965517241,
          "rmse": 1.3687816547538927,
          "mae": 1.0,
          "quadratic_weighted_kappa": 0.23942075623491543
        },
        "curiosity": {
          "perfect_accuracy": 0.2413793103448276,
          "off_by_one_accuracy": 0.6896551724137931,
          "level_accuracy": 0.4942528735632184,
          "rmse": 1.4263529572376852,
          "mae": 1.1379310344827587,
          "quadratic_weighted_kappa": 0.26175751474183817
        },
        "surprise": {
          "perfect_accuracy": 0.2413793103448276,
          "off_by_one_accuracy": 0.7011494252873564,
          "level_accuracy": 0.5977011494252874,
          "rmse": 1.4739110533215525,
          "mae": 1.160919540229885,
          "quadratic_weighted_kappa": 0.15948474160404857
        }
      }
    },
    {
      "story_classification": {
        "accuracy": 0.6666666666666666,
        "micro_precision": 0.6666666666666666,
        "micro_recall": 0.6666666666666666,
        "micro_f1": 0.6666666666666666,
        "macro_precision": 0.663563829787234,
        "macro_recall": 0.6642628205128205,
        "macro_f1": 0.6638241172551633,
        "weighted_precision": 0.6675531914893618,
        "weighted_recall": 0.6666666666666666,
        "weighted_f1": 0.6670219853431046,
        "mcc": 0.32782590510653636
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.3793103448275862,
          "off_by_one_accuracy": 0.7931034482758621,
          "level_accuracy": 0.6436781609195402,
          "rmse": 1.1646123127807209,
          "mae": 0.8505747126436781,
          "quadratic_weighted_kappa": 0.36409811694747274
        },
        "curiosity": {
          "perfect_accuracy": 0.20689655172413793,
          "off_by_one_accuracy": 0.7586206896551724,
          "level_accuracy": 0.47126436781609193,
          "rmse": 1.364576478442026,
          "mae": 1.103448275862069,
          "quadratic_weighted_kappa": 0.352
        },
        "surprise": {
          "perfect_accuracy": 0.28735632183908044,
          "off_by_one_accuracy": 0.735632183908046,
          "level_accuracy": 0.6436781609195402,
          "rmse": 1.43037652787448,
          "mae": 1.0804597701149425,
          "quadratic_weighted_kappa": 0.28858875413450946
        }
      }
    },
    {
      "story_classification": {
        "accuracy": 0.7558139534883721,
        "micro_precision": 0.7558139534883721,
        "micro_recall": 0.7558139534883721,
        "micro_f1": 0.755813953488372,
        "macro_precision": 0.7525913802509547,
        "macro_recall": 0.7538377192982456,
        "macro_f1": 0.75311004784689,
        "weighted_precision": 0.7566195967977265,
        "weighted_recall": 0.7558139534883721,
        "weighted_f1": 0.7561143874485369,
        "mcc": 0.5064275659057207
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.43023255813953487,
          "off_by_one_accuracy": 0.8372093023255814,
          "level_accuracy": 0.627906976744186,
          "rmse": 1.0837059585750939,
          "mae": 0.7558139534883721,
          "quadratic_weighted_kappa": 0.4594897324206597
        },
        "curiosity": {
          "perfect_accuracy": 0.27906976744186046,
          "off_by_one_accuracy": 0.8488372093023255,
          "level_accuracy": 0.5930232558139535,
          "rmse": 1.1613945106217463,
          "mae": 0.9069767441860465,
          "quadratic_weighted_kappa": 0.5113636363636365
        },
        "surprise": {
          "perfect_accuracy": 0.2558139534883721,
          "off_by_one_accuracy": 0.7209302325581395,
          "level_accuracy": 0.5813953488372093,
          "rmse": 1.4587347177414356,
          "mae": 1.127906976744186,
          "quadratic_weighted_kappa": 0.27334010527287844
        }
      }
    }
  ]
}