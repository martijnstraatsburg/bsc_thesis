{
  "average_metrics": {
    "story_classification": {
      "accuracy": {
        "mean": 0.7074044373162256,
        "std": 0.04495440610882009
      },
      "micro_precision": {
        "mean": 0.7074044373162256,
        "std": 0.04495440610882009
      },
      "micro_recall": {
        "mean": 0.7074044373162256,
        "std": 0.04495440610882009
      },
      "micro_f1": {
        "mean": 0.7074044373162256,
        "std": 0.04495440610882013
      },
      "macro_precision": {
        "mean": 0.7112266078015683,
        "std": 0.0419555095539818
      },
      "macro_recall": {
        "mean": 0.7137193690035235,
        "std": 0.04541569063316065
      },
      "macro_f1": {
        "mean": 0.706653536827199,
        "std": 0.04402141420305634
      },
      "weighted_precision": {
        "mean": 0.7200263605915762,
        "std": 0.04560759252615514
      },
      "weighted_recall": {
        "mean": 0.7074044373162256,
        "std": 0.04495440610882009
      },
      "weighted_f1": {
        "mean": 0.7079373483209273,
        "std": 0.04563757503063594
      },
      "mcc": {
        "mean": 0.4249246191486269,
        "std": 0.08730667762919082
      }
    },
    "rating_metrics": {
      "suspense": {
        "perfect_accuracy": {
          "mean": 0.3386260358192996,
          "std": 0.03888297181907537
        },
        "off_by_one_accuracy": {
          "mean": 0.7579791499599037,
          "std": 0.03240348843813372
        },
        "level_accuracy": {
          "mean": 0.594466720128308,
          "std": 0.004509830892053099
        },
        "rmse": {
          "mean": 1.3073144411534567,
          "std": 0.07396295273793684
        },
        "mae": {
          "mean": 0.9678962844159316,
          "std": 0.07679771746887128
        },
        "quadratic_weighted_kappa": {
          "mean": 0.2511586425173373,
          "std": 0.06669749417670655
        }
      },
      "curiosity": {
        "perfect_accuracy": {
          "mean": 0.20272654370489174,
          "std": 0.03584745568989995
        },
        "off_by_one_accuracy": {
          "mean": 0.7259021651964714,
          "std": 0.0374493359326354
        },
        "level_accuracy": {
          "mean": 0.5207965784549586,
          "std": 0.060605650200732414
        },
        "rmse": {
          "mean": 1.4786806944041817,
          "std": 0.04422317810301812
        },
        "mae": {
          "mean": 1.184228815824646,
          "std": 0.046234707967374965
        },
        "quadratic_weighted_kappa": {
          "mean": 0.24831666883960102,
          "std": 0.06854635905365457
        }
      },
      "surprise": {
        "perfect_accuracy": {
          "mean": 0.2558406843090083,
          "std": 0.04943745134654565
        },
        "off_by_one_accuracy": {
          "mean": 0.6844426623897354,
          "std": 0.03808874623071322
        },
        "level_accuracy": {
          "mean": 0.5738037957765304,
          "std": 0.0459059645143714
        },
        "rmse": {
          "mean": 1.5205833974876157,
          "std": 0.07174749512891478
        },
        "mae": {
          "mean": 1.1840951617214648,
          "std": 0.09401992293054581
        },
        "quadratic_weighted_kappa": {
          "mean": 0.17804740015366177,
          "std": 0.08437202260134169
        }
      }
    }
  },
  "fold_metrics": [
    {
      "story_classification": {
        "accuracy": 0.7816091954022989,
        "micro_precision": 0.7816091954022989,
        "micro_recall": 0.7816091954022989,
        "micro_f1": 0.781609195402299,
        "macro_precision": 0.7785714285714286,
        "macro_recall": 0.7892857142857144,
        "macro_f1": 0.7786852322934797,
        "weighted_precision": 0.7957854406130268,
        "weighted_recall": 0.7816091954022989,
        "weighted_f1": 0.7836559695784723,
        "mcc": 0.5677560556925044
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.367816091954023,
          "off_by_one_accuracy": 0.8160919540229885,
          "level_accuracy": 0.5977011494252874,
          "rmse": 1.1986582537134602,
          "mae": 0.8620689655172413,
          "quadratic_weighted_kappa": 0.37568172684999135
        },
        "curiosity": {
          "perfect_accuracy": 0.22988505747126436,
          "off_by_one_accuracy": 0.7586206896551724,
          "level_accuracy": 0.5402298850574713,
          "rmse": 1.4582307024641867,
          "mae": 1.1379310344827587,
          "quadratic_weighted_kappa": 0.26023808429470974
        },
        "surprise": {
          "perfect_accuracy": 0.3333333333333333,
          "off_by_one_accuracy": 0.7126436781609196,
          "level_accuracy": 0.6206896551724138,
          "rmse": 1.450326954814696,
          "mae": 1.0689655172413792,
          "quadratic_weighted_kappa": 0.2754289355117644
        }
      }
    },
    {
      "story_classification": {
        "accuracy": 0.7126436781609196,
        "micro_precision": 0.7126436781609196,
        "micro_recall": 0.7126436781609196,
        "micro_f1": 0.7126436781609196,
        "macro_precision": 0.7122093023255813,
        "macro_recall": 0.713563829787234,
        "macro_f1": 0.7120349529988085,
        "weighted_precision": 0.7152499331729484,
        "weighted_recall": 0.7126436781609196,
        "weighted_f1": 0.713100222032503,
        "mcc": 0.42577097750376625
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.39080459770114945,
          "off_by_one_accuracy": 0.7586206896551724,
          "level_accuracy": 0.5977011494252874,
          "rmse": 1.2730630994465333,
          "mae": 0.9080459770114943,
          "quadratic_weighted_kappa": 0.22512791358726558
        },
        "curiosity": {
          "perfect_accuracy": 0.14942528735632185,
          "off_by_one_accuracy": 0.7011494252873564,
          "level_accuracy": 0.40229885057471265,
          "rmse": 1.5237582093166127,
          "mae": 1.264367816091954,
          "quadratic_weighted_kappa": 0.12706139479435719
        },
        "surprise": {
          "perfect_accuracy": 0.20689655172413793,
          "off_by_one_accuracy": 0.6206896551724138,
          "level_accuracy": 0.4942528735632184,
          "rmse": 1.618854426800759,
          "mae": 1.3103448275862069,
          "quadratic_weighted_kappa": 0.08732860955185417
        }
      }
    },
    {
      "story_classification": {
        "accuracy": 0.6666666666666666,
        "micro_precision": 0.6666666666666666,
        "micro_recall": 0.6666666666666666,
        "micro_f1": 0.6666666666666666,
        "macro_precision": 0.6702014846235419,
        "macro_recall": 0.671474358974359,
        "macro_f1": 0.6664904163912756,
        "weighted_precision": 0.6765641569459173,
        "weighted_recall": 0.6666666666666666,
        "weighted_f1": 0.6672835426305354,
        "mcc": 0.34167347261563086
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.28735632183908044,
          "off_by_one_accuracy": 0.7586206896551724,
          "level_accuracy": 0.5977011494252874,
          "rmse": 1.4182715723279387,
          "mae": 1.0689655172413792,
          "quadratic_weighted_kappa": 0.1838649155722326
        },
        "curiosity": {
          "perfect_accuracy": 0.25287356321839083,
          "off_by_one_accuracy": 0.6666666666666666,
          "level_accuracy": 0.5747126436781609,
          "rmse": 1.5237582093166127,
          "mae": 1.1954022988505748,
          "quadratic_weighted_kappa": 0.25140569091838494
        },
        "surprise": {
          "perfect_accuracy": 0.20689655172413793,
          "off_by_one_accuracy": 0.6781609195402298,
          "level_accuracy": 0.5517241379310345,
          "rmse": 1.5462226869701368,
          "mae": 1.2413793103448276,
          "quadratic_weighted_kappa": 0.07399447344181753
        }
      }
    },
    {
      "story_classification": {
        "accuracy": 0.6551724137931034,
        "micro_precision": 0.6551724137931034,
        "micro_recall": 0.6551724137931034,
        "micro_f1": 0.6551724137931034,
        "macro_precision": 0.6643394199785178,
        "macro_recall": 0.6634615384615385,
        "macro_f1": 0.6551268498942917,
        "weighted_precision": 0.6718396977665838,
        "weighted_recall": 0.6551724137931034,
        "weighted_f1": 0.6547167748049865,
        "mcc": 0.32779978291368894
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.3448275862068966,
          "off_by_one_accuracy": 0.735632183908046,
          "level_accuracy": 0.5862068965517241,
          "rmse": 1.2954385047312087,
          "mae": 0.9655172413793104,
          "quadratic_weighted_kappa": 0.25840728631480625
        },
        "curiosity": {
          "perfect_accuracy": 0.19540229885057472,
          "off_by_one_accuracy": 0.735632183908046,
          "level_accuracy": 0.5402298850574713,
          "rmse": 1.4816890020684417,
          "mae": 1.1839080459770115,
          "quadratic_weighted_kappa": 0.2627445760681486
        },
        "surprise": {
          "perfect_accuracy": 0.2413793103448276,
          "off_by_one_accuracy": 0.6781609195402298,
          "level_accuracy": 0.5977011494252874,
          "rmse": 1.5610194134588469,
          "mae": 1.2183908045977012,
          "quadratic_weighted_kappa": 0.19275210084033623
        }
      }
    },
    {
      "story_classification": {
        "accuracy": 0.7209302325581395,
        "micro_precision": 0.7209302325581395,
        "micro_recall": 0.7209302325581395,
        "micro_f1": 0.7209302325581395,
        "macro_precision": 0.7308114035087719,
        "macro_recall": 0.7308114035087719,
        "macro_f1": 0.7209302325581395,
        "weighted_precision": 0.7406925744594043,
        "weighted_recall": 0.7209302325581395,
        "weighted_f1": 0.7209302325581395,
        "mcc": 0.4616228070175439
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.3023255813953488,
          "off_by_one_accuracy": 0.7209302325581395,
          "level_accuracy": 0.5930232558139535,
          "rmse": 1.3511407755481428,
          "mae": 1.0348837209302326,
          "quadratic_weighted_kappa": 0.21271137026239073
        },
        "curiosity": {
          "perfect_accuracy": 0.18604651162790697,
          "off_by_one_accuracy": 0.7674418604651163,
          "level_accuracy": 0.5465116279069767,
          "rmse": 1.4059673488550548,
          "mae": 1.1395348837209303,
          "quadratic_weighted_kappa": 0.34013359812240473
        },
        "surprise": {
          "perfect_accuracy": 0.29069767441860467,
          "off_by_one_accuracy": 0.7325581395348837,
          "level_accuracy": 0.6046511627906976,
          "rmse": 1.4264935053936392,
          "mae": 1.0813953488372092,
          "quadratic_weighted_kappa": 0.2607328814225366
        }
      }
    }
  ]
}