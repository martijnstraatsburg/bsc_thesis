{
  "average_metrics": {
    "story_classification": {
      "accuracy": {
        "mean": 0.7365576102418208,
        "std": 0.055058423719907185
      },
      "micro_precision": {
        "mean": 0.7365576102418208,
        "std": 0.055058423719907185
      },
      "micro_recall": {
        "mean": 0.7365576102418208,
        "std": 0.055058423719907185
      },
      "micro_f1": {
        "mean": 0.7365576102418208,
        "std": 0.055058423719907185
      },
      "macro_precision": {
        "mean": 0.7383458646616541,
        "std": 0.0391694728229403
      },
      "macro_recall": {
        "mean": 0.7451186494664755,
        "std": 0.0365211784666459
      },
      "macro_f1": {
        "mean": 0.7271615510922439,
        "std": 0.05013273322941748
      },
      "weighted_precision": {
        "mean": 0.769345219688469,
        "std": 0.033289304943479724
      },
      "weighted_recall": {
        "mean": 0.7365576102418208,
        "std": 0.055058423719907185
      },
      "weighted_f1": {
        "mean": 0.7387778155695288,
        "std": 0.05337412233034577
      },
      "mcc": {
        "mean": 0.4833590144937478,
        "std": 0.07555474675626822
      }
    },
    "rating_metrics": {
      "suspense": {
        "perfect_accuracy": {
          "mean": 0.28463726884779517,
          "std": 0.09016786839946314
        },
        "off_by_one_accuracy": {
          "mean": 0.8443812233285918,
          "std": 0.04820994577188112
        },
        "level_accuracy": {
          "mean": 0.5699857752489332,
          "std": 0.067584186022502
        },
        "rmse": {
          "mean": 1.192944626217869,
          "std": 0.16929622754587892
        },
        "mae": {
          "mean": 0.9248933143669985,
          "std": 0.1559889081703139
        },
        "quadratic_weighted_kappa": {
          "mean": 0.32320968691711055,
          "std": 0.16271431949282947
        }
      },
      "curiosity": {
        "perfect_accuracy": {
          "mean": 0.32275960170697016,
          "std": 0.062392034876234646
        },
        "off_by_one_accuracy": {
          "mean": 0.758321479374111,
          "std": 0.04342721217596092
        },
        "level_accuracy": {
          "mean": 0.5271692745376956,
          "std": 0.08622874674571192
        },
        "rmse": {
          "mean": 1.3507269421991948,
          "std": 0.09764160315664597
        },
        "mae": {
          "mean": 1.0052631578947369,
          "std": 0.0670340545067775
        },
        "quadratic_weighted_kappa": {
          "mean": 0.35228220175944813,
          "std": 0.0785392204434008
        }
      },
      "surprise": {
        "perfect_accuracy": {
          "mean": 0.21465149359886201,
          "std": 0.07863907613874771
        },
        "off_by_one_accuracy": {
          "mean": 0.6935988620199146,
          "std": 0.07140541668364454
        },
        "level_accuracy": {
          "mean": 0.590896159317212,
          "std": 0.0649783141044866
        },
        "rmse": {
          "mean": 1.5093078114394642,
          "std": 0.1325905633787321
        },
        "mae": {
          "mean": 1.2099573257467995,
          "std": 0.158235506839284
        },
        "quadratic_weighted_kappa": {
          "mean": 0.1708030262886392,
          "std": 0.11426065159767453
        }
      }
    }
  },
  "fold_metrics": [
    {
      "story_classification": {
        "accuracy": 0.6756756756756757,
        "micro_precision": 0.6756756756756757,
        "micro_recall": 0.6756756756756757,
        "micro_f1": 0.6756756756756757,
        "macro_precision": 0.7251552795031055,
        "macro_recall": 0.7323717948717949,
        "macro_f1": 0.6754385964912281,
        "weighted_precision": 0.785630350847742,
        "weighted_recall": 0.6756756756756757,
        "weighted_f1": 0.6780464675201517,
        "mcc": 0.457470158252961
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.24324324324324326,
          "off_by_one_accuracy": 0.8648648648648649,
          "level_accuracy": 0.6486486486486487,
          "rmse": 1.2520253861514021,
          "mae": 0.972972972972973,
          "quadratic_weighted_kappa": 0.33642547928262223
        },
        "curiosity": {
          "perfect_accuracy": 0.32432432432432434,
          "off_by_one_accuracy": 0.7297297297297297,
          "level_accuracy": 0.5945945945945946,
          "rmse": 1.5156837721956706,
          "mae": 1.1081081081081081,
          "quadratic_weighted_kappa": 0.26363849215640367
        },
        "surprise": {
          "perfect_accuracy": 0.1891891891891892,
          "off_by_one_accuracy": 0.7027027027027027,
          "level_accuracy": 0.5945945945945946,
          "rmse": 1.451932542547383,
          "mae": 1.1891891891891893,
          "quadratic_weighted_kappa": 0.18474576271186438
        }
      }
    },
    {
      "story_classification": {
        "accuracy": 0.7837837837837838,
        "micro_precision": 0.7837837837837838,
        "micro_recall": 0.7837837837837838,
        "micro_f1": 0.7837837837837838,
        "macro_precision": 0.7701863354037267,
        "macro_recall": 0.7701863354037267,
        "macro_f1": 0.7701863354037267,
        "weighted_precision": 0.7837837837837838,
        "weighted_recall": 0.7837837837837838,
        "weighted_f1": 0.7837837837837838,
        "mcc": 0.5403726708074534
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.21621621621621623,
          "off_by_one_accuracy": 0.8918918918918919,
          "level_accuracy": 0.5135135135135135,
          "rmse": 1.115008180796555,
          "mae": 0.918918918918919,
          "quadratic_weighted_kappa": 0.3018867924528301
        },
        "curiosity": {
          "perfect_accuracy": 0.24324324324324326,
          "off_by_one_accuracy": 0.7567567567567568,
          "level_accuracy": 0.40540540540540543,
          "rmse": 1.2734290799340267,
          "mae": 1.027027027027027,
          "quadratic_weighted_kappa": 0.2974683544303798
        },
        "surprise": {
          "perfect_accuracy": 0.13513513513513514,
          "off_by_one_accuracy": 0.7297297297297297,
          "level_accuracy": 0.5675675675675675,
          "rmse": 1.5334116705410037,
          "mae": 1.2702702702702702,
          "quadratic_weighted_kappa": 0.0715315835015864
        }
      }
    },
    {
      "story_classification": {
        "accuracy": 0.7368421052631579,
        "micro_precision": 0.7368421052631579,
        "micro_recall": 0.7368421052631579,
        "micro_f1": 0.7368421052631579,
        "macro_precision": 0.7246376811594203,
        "macro_recall": 0.7246376811594203,
        "macro_f1": 0.7246376811594203,
        "weighted_precision": 0.7368421052631579,
        "weighted_recall": 0.7368421052631579,
        "weighted_f1": 0.7368421052631579,
        "mcc": 0.4492753623188406
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.34210526315789475,
          "off_by_one_accuracy": 0.7894736842105263,
          "level_accuracy": 0.5526315789473685,
          "rmse": 1.1920791213585393,
          "mae": 0.8947368421052632,
          "quadratic_weighted_kappa": 0.28
        },
        "curiosity": {
          "perfect_accuracy": 0.2894736842105263,
          "off_by_one_accuracy": 0.7105263157894737,
          "level_accuracy": 0.47368421052631576,
          "rmse": 1.3078709094514924,
          "mae": 1.0263157894736843,
          "quadratic_weighted_kappa": 0.38952051408798827
        },
        "surprise": {
          "perfect_accuracy": 0.2894736842105263,
          "off_by_one_accuracy": 0.6842105263157895,
          "level_accuracy": 0.6842105263157895,
          "rmse": 1.5217718205053643,
          "mae": 1.1578947368421053,
          "quadratic_weighted_kappa": 0.22268712226871223
        }
      }
    },
    {
      "story_classification": {
        "accuracy": 0.6756756756756757,
        "micro_precision": 0.6756756756756757,
        "micro_recall": 0.6756756756756757,
        "micro_f1": 0.6756756756756757,
        "macro_precision": 0.6798245614035088,
        "macro_recall": 0.6971153846153846,
        "macro_f1": 0.6696428571428572,
        "weighted_precision": 0.7254623044096729,
        "weighted_recall": 0.6756756756756757,
        "weighted_f1": 0.6829150579150579,
        "mcc": 0.3765431586649556
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.1891891891891892,
          "off_by_one_accuracy": 0.7837837837837838,
          "level_accuracy": 0.4864864864864865,
          "rmse": 1.461210161179813,
          "mae": 1.162162162162162,
          "quadratic_weighted_kappa": 0.09532652429588362
        },
        "curiosity": {
          "perfect_accuracy": 0.43243243243243246,
          "off_by_one_accuracy": 0.7567567567567568,
          "level_accuracy": 0.6486486486486487,
          "rmse": 1.404625563263382,
          "mae": 0.9459459459459459,
          "quadratic_weighted_kappa": 0.3249187703074232
        },
        "surprise": {
          "perfect_accuracy": 0.13513513513513514,
          "off_by_one_accuracy": 0.5675675675675675,
          "level_accuracy": 0.4864864864864865,
          "rmse": 1.7242311251607114,
          "mae": 1.4594594594594594,
          "quadratic_weighted_kappa": 0.026315789473684292
        }
      }
    },
    {
      "story_classification": {
        "accuracy": 0.8108108108108109,
        "micro_precision": 0.8108108108108109,
        "micro_recall": 0.8108108108108109,
        "micro_f1": 0.8108108108108109,
        "macro_precision": 0.7919254658385093,
        "macro_recall": 0.8012820512820513,
        "macro_f1": 0.7959022852639874,
        "weighted_precision": 0.8150075541379889,
        "weighted_recall": 0.8108108108108109,
        "weighted_f1": 0.8123016633654931,
        "mcc": 0.5931337224245288
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.43243243243243246,
          "off_by_one_accuracy": 0.8918918918918919,
          "level_accuracy": 0.6486486486486487,
          "rmse": 0.944400281603035,
          "mae": 0.6756756756756757,
          "quadratic_weighted_kappa": 0.6024096385542168
        },
        "curiosity": {
          "perfect_accuracy": 0.32432432432432434,
          "off_by_one_accuracy": 0.8378378378378378,
          "level_accuracy": 0.5135135135135135,
          "rmse": 1.2520253861514021,
          "mae": 0.918918918918919,
          "quadratic_weighted_kappa": 0.4858648778150455
        },
        "surprise": {
          "perfect_accuracy": 0.32432432432432434,
          "off_by_one_accuracy": 0.7837837837837838,
          "level_accuracy": 0.6216216216216216,
          "rmse": 1.3151918984428583,
          "mae": 0.972972972972973,
          "quadratic_weighted_kappa": 0.34873487348734866
        }
      }
    }
  ]
}