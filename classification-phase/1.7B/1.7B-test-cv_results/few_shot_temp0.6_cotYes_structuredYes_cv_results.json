{
  "average_metrics": {
    "story_classification": {
      "accuracy": {
        "mean": 0.553769559032717,
        "std": 0.07150905592375141
      },
      "micro_precision": {
        "mean": 0.553769559032717,
        "std": 0.07150905592375141
      },
      "micro_recall": {
        "mean": 0.553769559032717,
        "std": 0.07150905592375141
      },
      "micro_f1": {
        "mean": 0.553769559032717,
        "std": 0.07150905592375141
      },
      "macro_precision": {
        "mean": 0.689757376722894,
        "std": 0.03892356209302221
      },
      "macro_recall": {
        "mean": 0.6395488931358496,
        "std": 0.0580097559632524
      },
      "macro_f1": {
        "mean": 0.540362018592531,
        "std": 0.08138120154957067
      },
      "weighted_precision": {
        "mean": 0.7546818979514078,
        "std": 0.04698416302271764
      },
      "weighted_recall": {
        "mean": 0.553769559032717,
        "std": 0.07150905592375141
      },
      "weighted_f1": {
        "mean": 0.5210617043347537,
        "std": 0.09354624575844205
      },
      "mcc": {
        "mean": 0.3225149212121702,
        "std": 0.09635141275083384
      }
    },
    "rating_metrics": {
      "suspense": {
        "perfect_accuracy": {
          "mean": 0.21009957325746803,
          "std": 0.10080080561956725
        },
        "off_by_one_accuracy": {
          "mean": 0.7099573257467994,
          "std": 0.05589874709772682
        },
        "level_accuracy": {
          "mean": 0.5970128022759601,
          "std": 0.045819599123724464
        },
        "rmse": {
          "mean": 1.513896819783154,
          "std": 0.11192002448703212
        },
        "mae": {
          "mean": 1.2088193456614509,
          "std": 0.1641055470213691
        },
        "quadratic_weighted_kappa": {
          "mean": 0.17889053129091853,
          "std": 0.1336186545419912
        }
      },
      "curiosity": {
        "perfect_accuracy": {
          "mean": 0.11820768136557609,
          "std": 0.06949253148794778
        },
        "off_by_one_accuracy": {
          "mean": 0.5219061166429587,
          "std": 0.05338433661495073
        },
        "level_accuracy": {
          "mean": 0.41948790896159316,
          "std": 0.037834629111318324
        },
        "rmse": {
          "mean": 1.889685513224934,
          "std": 0.08723122718880305
        },
        "mae": {
          "mean": 1.612375533428165,
          "std": 0.1141778865755762
        },
        "quadratic_weighted_kappa": {
          "mean": 0.06897283984372307,
          "std": 0.07935617900382308
        }
      },
      "surprise": {
        "perfect_accuracy": {
          "mean": 0.14537695590327168,
          "std": 0.06565242732555673
        },
        "off_by_one_accuracy": {
          "mean": 0.6506401137980087,
          "std": 0.04419847848862236
        },
        "level_accuracy": {
          "mean": 0.5645803698435278,
          "std": 0.005974395448079628
        },
        "rmse": {
          "mean": 1.675622986815031,
          "std": 0.08664923368738231
        },
        "mae": {
          "mean": 1.3864864864864865,
          "std": 0.11768400571393102
        },
        "quadratic_weighted_kappa": {
          "mean": 0.05640168391721463,
          "std": 0.08634947675974604
        }
      }
    }
  },
  "fold_metrics": [
    {
      "story_classification": {
        "accuracy": 0.4594594594594595,
        "micro_precision": 0.4594594594594595,
        "micro_recall": 0.4594594594594595,
        "micro_f1": 0.4594594594594595,
        "macro_precision": 0.696969696969697,
        "macro_recall": 0.5833333333333334,
        "macro_f1": 0.42546583850931674,
        "weighted_precision": 0.7870597870597871,
        "weighted_recall": 0.4594594594594595,
        "weighted_f1": 0.3839180795702534,
        "mcc": 0.25623537159526916
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.13513513513513514,
          "off_by_one_accuracy": 0.7297297297297297,
          "level_accuracy": 0.6216216216216216,
          "rmse": 1.5334116705410037,
          "mae": 1.2702702702702702,
          "quadratic_weighted_kappa": 0.16106333072713064
        },
        "curiosity": {
          "perfect_accuracy": 0.05405405405405406,
          "off_by_one_accuracy": 0.4864864864864865,
          "level_accuracy": 0.40540540540540543,
          "rmse": 1.993231791080248,
          "mae": 1.7567567567567568,
          "quadratic_weighted_kappa": 0.0439444542098788
        },
        "surprise": {
          "perfect_accuracy": 0.1891891891891892,
          "off_by_one_accuracy": 0.5945945945945946,
          "level_accuracy": 0.5675675675675675,
          "rmse": 1.643989873053573,
          "mae": 1.3513513513513513,
          "quadratic_weighted_kappa": 0.036959916710046725
        }
      }
    },
    {
      "story_classification": {
        "accuracy": 0.5675675675675675,
        "micro_precision": 0.5675675675675675,
        "micro_recall": 0.5675675675675675,
        "micro_f1": 0.5675675675675675,
        "macro_precision": 0.6765873015873016,
        "macro_recall": 0.6381987577639752,
        "macro_f1": 0.5595238095238095,
        "weighted_precision": 0.7282282282282282,
        "weighted_recall": 0.5675675675675675,
        "weighted_f1": 0.545045045045045,
        "mcc": 0.3124365261377582
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.1891891891891892,
          "off_by_one_accuracy": 0.6756756756756757,
          "level_accuracy": 0.5675675675675675,
          "rmse": 1.5245734893157248,
          "mae": 1.2432432432432432,
          "quadratic_weighted_kappa": 0.0808781051415367
        },
        "curiosity": {
          "perfect_accuracy": 0.05405405405405406,
          "off_by_one_accuracy": 0.5675675675675675,
          "level_accuracy": 0.3783783783783784,
          "rmse": 1.785830112073707,
          "mae": 1.5675675675675675,
          "quadratic_weighted_kappa": 0.07460788469690549
        },
        "surprise": {
          "perfect_accuracy": 0.05405405405405406,
          "off_by_one_accuracy": 0.6486486486486487,
          "level_accuracy": 0.5675675675675675,
          "rmse": 1.7553008520140019,
          "mae": 1.5135135135135136,
          "quadratic_weighted_kappa": -0.044059405940593876
        }
      }
    },
    {
      "story_classification": {
        "accuracy": 0.5526315789473685,
        "micro_precision": 0.5526315789473685,
        "micro_recall": 0.5526315789473685,
        "micro_f1": 0.5526315789473685,
        "macro_precision": 0.6708333333333334,
        "macro_recall": 0.618840579710145,
        "macro_f1": 0.5369175627240144,
        "weighted_precision": 0.7138157894736842,
        "weighted_recall": 0.5526315789473685,
        "weighted_f1": 0.5189586870401812,
        "mcc": 0.2849696992113355
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.13157894736842105,
          "off_by_one_accuracy": 0.6578947368421053,
          "level_accuracy": 0.5526315789473685,
          "rmse": 1.6383560438182505,
          "mae": 1.368421052631579,
          "quadratic_weighted_kappa": 0.06422018348623848
        },
        "curiosity": {
          "perfect_accuracy": 0.13157894736842105,
          "off_by_one_accuracy": 0.4473684210526316,
          "level_accuracy": 0.39473684210526316,
          "rmse": 1.9934101962532238,
          "mae": 1.7105263157894737,
          "quadratic_weighted_kappa": -0.06535462309691775
        },
        "surprise": {
          "perfect_accuracy": 0.10526315789473684,
          "off_by_one_accuracy": 0.631578947368421,
          "level_accuracy": 0.5526315789473685,
          "rmse": 1.7844356324383879,
          "mae": 1.5,
          "quadratic_weighted_kappa": -0.024053452115813112
        }
      }
    },
    {
      "story_classification": {
        "accuracy": 0.5135135135135135,
        "micro_precision": 0.5135135135135135,
        "micro_recall": 0.5135135135135135,
        "micro_f1": 0.5135135135135135,
        "macro_precision": 0.6443965517241379,
        "macro_recall": 0.6073717948717949,
        "macro_f1": 0.5044642857142858,
        "weighted_precision": 0.7129543336439889,
        "weighted_recall": 0.5135135135135135,
        "weighted_f1": 0.4845559845559846,
        "mcc": 0.24903105775720952
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.1891891891891892,
          "off_by_one_accuracy": 0.6756756756756757,
          "level_accuracy": 0.5675675675675675,
          "rmse": 1.5682663866382713,
          "mae": 1.2702702702702702,
          "quadratic_weighted_kappa": 0.15338194619059586
        },
        "curiosity": {
          "perfect_accuracy": 0.10810810810810811,
          "off_by_one_accuracy": 0.5135135135135135,
          "level_accuracy": 0.43243243243243246,
          "rmse": 1.8526824963906883,
          "mae": 1.5945945945945945,
          "quadratic_weighted_kappa": 0.1238112996457208
        },
        "surprise": {
          "perfect_accuracy": 0.13513513513513514,
          "off_by_one_accuracy": 0.6486486486486487,
          "level_accuracy": 0.5675675675675675,
          "rmse": 1.652189374657073,
          "mae": 1.3783783783783783,
          "quadratic_weighted_kappa": 0.1450469000228779
        }
      }
    },
    {
      "story_classification": {
        "accuracy": 0.6756756756756757,
        "micro_precision": 0.6756756756756757,
        "micro_recall": 0.6756756756756757,
        "micro_f1": 0.6756756756756757,
        "macro_precision": 0.76,
        "macro_recall": 0.75,
        "macro_f1": 0.6754385964912281,
        "weighted_precision": 0.8313513513513513,
        "weighted_recall": 0.6756756756756757,
        "weighted_f1": 0.6728307254623044,
        "mcc": 0.5099019513592785
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.40540540540540543,
          "off_by_one_accuracy": 0.8108108108108109,
          "level_accuracy": 0.6756756756756757,
          "rmse": 1.30487650860252,
          "mae": 0.8918918918918919,
          "quadratic_weighted_kappa": 0.4349090909090909
        },
        "curiosity": {
          "perfect_accuracy": 0.24324324324324326,
          "off_by_one_accuracy": 0.5945945945945946,
          "level_accuracy": 0.4864864864864865,
          "rmse": 1.8232729703268034,
          "mae": 1.4324324324324325,
          "quadratic_weighted_kappa": 0.167855183763028
        },
        "surprise": {
          "perfect_accuracy": 0.24324324324324326,
          "off_by_one_accuracy": 0.7297297297297297,
          "level_accuracy": 0.5675675675675675,
          "rmse": 1.5421992019121196,
          "mae": 1.1891891891891893,
          "quadratic_weighted_kappa": 0.1681144609095555
        }
      }
    }
  ]
}