{
  "average_metrics": {
    "story_classification": {
      "accuracy": {
        "mean": 0.5437316225608126,
        "std": 0.011163886505117983
      },
      "micro_precision": {
        "mean": 0.5437316225608126,
        "std": 0.011163886505117983
      },
      "micro_recall": {
        "mean": 0.5437316225608126,
        "std": 0.011163886505117983
      },
      "micro_f1": {
        "mean": 0.5437316225608126,
        "std": 0.011163886505117983
      },
      "macro_precision": {
        "mean": 0.7263240932861186,
        "std": 0.018611814774310346
      },
      "macro_recall": {
        "mean": 0.5911681669394435,
        "std": 0.017899207352347627
      },
      "macro_f1": {
        "mean": 0.4875359930263043,
        "std": 0.02346558490542862
      },
      "weighted_precision": {
        "mean": 0.7552930655897777,
        "std": 0.02759151859367324
      },
      "weighted_recall": {
        "mean": 0.5437316225608126,
        "std": 0.011163886505117983
      },
      "weighted_f1": {
        "mean": 0.4681745223494607,
        "std": 0.024058046925750108
      },
      "mcc": {
        "mean": 0.2858182598704445,
        "std": 0.030409281063113244
      }
    },
    "rating_metrics": {
      "suspense": {
        "perfect_accuracy": {
          "mean": 0.2143544506816359,
          "std": 0.030157815618382634
        },
        "off_by_one_accuracy": {
          "mean": 0.6336273723603314,
          "std": 0.02123147489125632
        },
        "level_accuracy": {
          "mean": 0.5668270515904839,
          "std": 0.02760143304582379
        },
        "rmse": {
          "mean": 1.642435677110378,
          "std": 0.07347719691323902
        },
        "mae": {
          "mean": 1.3156642608928095,
          "std": 0.06839373907422042
        },
        "quadratic_weighted_kappa": {
          "mean": 0.12364065049365151,
          "std": 0.055203852948635154
        }
      },
      "curiosity": {
        "perfect_accuracy": {
          "mean": 0.09206094627105052,
          "std": 0.03395365630739546
        },
        "off_by_one_accuracy": {
          "mean": 0.5621224271585138,
          "std": 0.02770200462810302
        },
        "level_accuracy": {
          "mean": 0.5252606255012029,
          "std": 0.02614760724827075
        },
        "rmse": {
          "mean": 1.8382159381064658,
          "std": 0.06604790137688041
        },
        "mae": {
          "mean": 1.5763432237369688,
          "std": 0.08477853283651955
        },
        "quadratic_weighted_kappa": {
          "mean": 0.08809883888410193,
          "std": 0.054223846774175834
        }
      },
      "surprise": {
        "perfect_accuracy": {
          "mean": 0.16599839615076184,
          "std": 0.03199535188653612
        },
        "off_by_one_accuracy": {
          "mean": 0.5989842288158247,
          "std": 0.028053560079636487
        },
        "level_accuracy": {
          "mean": 0.5805934242181235,
          "std": 0.016869668209721838
        },
        "rmse": {
          "mean": 1.7458414773122253,
          "std": 0.023536685216012918
        },
        "mae": {
          "mean": 1.4354717989842287,
          "std": 0.03855622286112894
        },
        "quadratic_weighted_kappa": {
          "mean": 0.0740643617720473,
          "std": 0.03958758608721761
        }
      }
    }
  },
  "fold_metrics": [
    {
      "story_classification": {
        "accuracy": 0.5517241379310345,
        "micro_precision": 0.5517241379310345,
        "micro_recall": 0.5517241379310345,
        "micro_f1": 0.5517241379310345,
        "macro_precision": 0.7364864864864865,
        "macro_recall": 0.625,
        "macro_f1": 0.5211009174311927,
        "weighted_precision": 0.7879776328052189,
        "weighted_recall": 0.5517241379310345,
        "weighted_f1": 0.49743751977222395,
        "mcc": 0.34386515270268847
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.25287356321839083,
          "off_by_one_accuracy": 0.6551724137931034,
          "level_accuracy": 0.5977011494252874,
          "rmse": 1.6259391627362754,
          "mae": 1.264367816091954,
          "quadratic_weighted_kappa": 0.1620603015075377
        },
        "curiosity": {
          "perfect_accuracy": 0.12643678160919541,
          "off_by_one_accuracy": 0.5977011494252874,
          "level_accuracy": 0.5632183908045977,
          "rmse": 1.7485626280942346,
          "mae": 1.471264367816092,
          "quadratic_weighted_kappa": 0.17930349670189372
        },
        "surprise": {
          "perfect_accuracy": 0.19540229885057472,
          "off_by_one_accuracy": 0.6091954022988506,
          "level_accuracy": 0.5977011494252874,
          "rmse": 1.7254018355887053,
          "mae": 1.3908045977011494,
          "quadratic_weighted_kappa": 0.11970152752275665
        }
      }
    },
    {
      "story_classification": {
        "accuracy": 0.5517241379310345,
        "micro_precision": 0.5517241379310345,
        "micro_recall": 0.5517241379310345,
        "micro_f1": 0.5517241379310345,
        "macro_precision": 0.7032467532467532,
        "macro_recall": 0.5832446808510638,
        "macro_f1": 0.49122807017543857,
        "weighted_precision": 0.7190774742498881,
        "weighted_recall": 0.5517241379310345,
        "weighted_f1": 0.4771123210324662,
        "mcc": 0.2601477357813509
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.20689655172413793,
          "off_by_one_accuracy": 0.632183908045977,
          "level_accuracy": 0.5632183908045977,
          "rmse": 1.5720255620746413,
          "mae": 1.2758620689655173,
          "quadratic_weighted_kappa": 0.1630498008859459
        },
        "curiosity": {
          "perfect_accuracy": 0.05747126436781609,
          "off_by_one_accuracy": 0.5402298850574713,
          "level_accuracy": 0.5057471264367817,
          "rmse": 1.8631329442141513,
          "mae": 1.632183908045977,
          "quadratic_weighted_kappa": 0.02334398929447634
        },
        "surprise": {
          "perfect_accuracy": 0.13793103448275862,
          "off_by_one_accuracy": 0.6206896551724138,
          "level_accuracy": 0.5862068965517241,
          "rmse": 1.7254018355887053,
          "mae": 1.4367816091954022,
          "quadratic_weighted_kappa": 0.10883923274668783
        }
      }
    },
    {
      "story_classification": {
        "accuracy": 0.5517241379310345,
        "micro_precision": 0.5517241379310345,
        "micro_recall": 0.5517241379310345,
        "micro_f1": 0.5517241379310345,
        "macro_precision": 0.7045454545454546,
        "macro_recall": 0.5913461538461539,
        "macro_f1": 0.49992630803242444,
        "weighted_precision": 0.725705329153605,
        "weighted_recall": 0.5517241379310345,
        "weighted_f1": 0.4832770055650141,
        "mcc": 0.2733820810473178
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.1724137931034483,
          "off_by_one_accuracy": 0.5977011494252874,
          "level_accuracy": 0.5172413793103449,
          "rmse": 1.7681732894739688,
          "mae": 1.4482758620689655,
          "quadratic_weighted_kappa": 0.03781410100024396
        },
        "curiosity": {
          "perfect_accuracy": 0.12643678160919541,
          "off_by_one_accuracy": 0.5632183908045977,
          "level_accuracy": 0.5287356321839081,
          "rmse": 1.793987403689809,
          "mae": 1.5172413793103448,
          "quadratic_weighted_kappa": 0.10697265195395544
        },
        "surprise": {
          "perfect_accuracy": 0.12643678160919541,
          "off_by_one_accuracy": 0.5747126436781609,
          "level_accuracy": 0.5632183908045977,
          "rmse": 1.7843508752420336,
          "mae": 1.5057471264367817,
          "quadratic_weighted_kappa": 0.02104236909452817
        }
      }
    },
    {
      "story_classification": {
        "accuracy": 0.5402298850574713,
        "micro_precision": 0.5402298850574713,
        "micro_recall": 0.5402298850574713,
        "micro_f1": 0.5402298850574713,
        "macro_precision": 0.7468354430379747,
        "macro_recall": 0.5833333333333334,
        "macro_f1": 0.47336561743341404,
        "weighted_precision": 0.7730248799650807,
        "weighted_recall": 0.5402298850574713,
        "weighted_f1": 0.45395341070384904,
        "mcc": 0.2868422580664471
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.19540229885057472,
          "off_by_one_accuracy": 0.6551724137931034,
          "level_accuracy": 0.5862068965517241,
          "rmse": 1.5720255620746413,
          "mae": 1.2758620689655173,
          "quadratic_weighted_kappa": 0.17668031163343467
        },
        "curiosity": {
          "perfect_accuracy": 0.10344827586206896,
          "off_by_one_accuracy": 0.5862068965517241,
          "level_accuracy": 0.5402298850574713,
          "rmse": 1.8414137497317515,
          "mae": 1.5517241379310345,
          "quadratic_weighted_kappa": 0.08551576696953511
        },
        "surprise": {
          "perfect_accuracy": 0.16091954022988506,
          "off_by_one_accuracy": 0.632183908045977,
          "level_accuracy": 0.5977011494252874,
          "rmse": 1.7320508075688772,
          "mae": 1.4137931034482758,
          "quadratic_weighted_kappa": 0.08642124321062161
        }
      }
    },
    {
      "story_classification": {
        "accuracy": 0.5232558139534884,
        "micro_precision": 0.5232558139534884,
        "micro_recall": 0.5232558139534884,
        "micro_f1": 0.5232558139534884,
        "macro_precision": 0.740506329113924,
        "macro_recall": 0.5729166666666666,
        "macro_f1": 0.45205905205905206,
        "weighted_precision": 0.7706800117750957,
        "weighted_recall": 0.5232558139534884,
        "weighted_f1": 0.42909235467375,
        "mcc": 0.2648540717544182
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.2441860465116279,
          "off_by_one_accuracy": 0.627906976744186,
          "level_accuracy": 0.5697674418604651,
          "rmse": 1.6740148091923646,
          "mae": 1.313953488372093,
          "quadratic_weighted_kappa": 0.07859873744109536
        },
        "curiosity": {
          "perfect_accuracy": 0.046511627906976744,
          "off_by_one_accuracy": 0.5232558139534884,
          "level_accuracy": 0.4883720930232558,
          "rmse": 1.9439829648023823,
          "mae": 1.7093023255813953,
          "quadratic_weighted_kappa": 0.04535828950064902
        },
        "surprise": {
          "perfect_accuracy": 0.20930232558139536,
          "off_by_one_accuracy": 0.5581395348837209,
          "level_accuracy": 0.5581395348837209,
          "rmse": 1.7620020325728054,
          "mae": 1.430232558139535,
          "quadratic_weighted_kappa": 0.03431743628564221
        }
      }
    }
  ]
}