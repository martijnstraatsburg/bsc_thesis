{
  "average_metrics": {
    "story_classification": {
      "accuracy": {
        "mean": 0.6335739107190591,
        "std": 0.024472004246486656
      },
      "micro_precision": {
        "mean": 0.6335739107190591,
        "std": 0.024472004246486656
      },
      "micro_recall": {
        "mean": 0.6335739107190591,
        "std": 0.024472004246486656
      },
      "micro_f1": {
        "mean": 0.6335739107190591,
        "std": 0.024472004246486656
      },
      "macro_precision": {
        "mean": 0.7042393485598513,
        "std": 0.023443901109622366
      },
      "macro_recall": {
        "mean": 0.6630419594403357,
        "std": 0.02885754229535495
      },
      "macro_f1": {
        "mean": 0.6210380028243785,
        "std": 0.03157578110290899
      },
      "weighted_precision": {
        "mean": 0.7233114103792977,
        "std": 0.03170367901917657
      },
      "weighted_recall": {
        "mean": 0.6335739107190591,
        "std": 0.024472004246486656
      },
      "weighted_f1": {
        "mean": 0.6138839849766448,
        "std": 0.03328569776065864
      },
      "mcc": {
        "mean": 0.36442594784513715,
        "std": 0.05043936629338226
      }
    },
    "rating_metrics": {
      "suspense": {
        "perfect_accuracy": {
          "mean": 0.30649558941459504,
          "std": 0.048819173275430026
        },
        "off_by_one_accuracy": {
          "mean": 0.7441593156909917,
          "std": 0.023271539878007088
        },
        "level_accuracy": {
          "mean": 0.6013365410318097,
          "std": 0.035057387189059125
        },
        "rmse": {
          "mean": 1.3728407838360632,
          "std": 0.05314968484649161
        },
        "mae": {
          "mean": 1.034643143544507,
          "std": 0.0509968015203275
        },
        "quadratic_weighted_kappa": {
          "mean": 0.3196728225329243,
          "std": 0.06263594762863557
        }
      },
      "curiosity": {
        "perfect_accuracy": {
          "mean": 0.13354717989842285,
          "std": 0.05255706251458666
        },
        "off_by_one_accuracy": {
          "mean": 0.6750868751670676,
          "std": 0.03686431182657349
        },
        "level_accuracy": {
          "mean": 0.4884522854851644,
          "std": 0.028177074556534242
        },
        "rmse": {
          "mean": 1.6033214864005534,
          "std": 0.09049531094181053
        },
        "mae": {
          "mean": 1.3388933440256616,
          "std": 0.10470069014357987
        },
        "quadratic_weighted_kappa": {
          "mean": 0.16548492497828457,
          "std": 0.0634963400185824
        }
      },
      "surprise": {
        "perfect_accuracy": {
          "mean": 0.26046511627906976,
          "std": 0.03610327612119267
        },
        "off_by_one_accuracy": {
          "mean": 0.7049719326383319,
          "std": 0.042258848947322174
        },
        "level_accuracy": {
          "mean": 0.5967388398823845,
          "std": 0.02118025812570566
        },
        "rmse": {
          "mean": 1.5008516891522516,
          "std": 0.06071123279413817
        },
        "mae": {
          "mean": 1.1590483827853517,
          "std": 0.06246072520406464
        },
        "quadratic_weighted_kappa": {
          "mean": 0.18859134548706039,
          "std": 0.05048262955369843
        }
      }
    }
  },
  "fold_metrics": [
    {
      "story_classification": {
        "accuracy": 0.6666666666666666,
        "micro_precision": 0.6666666666666666,
        "micro_recall": 0.6666666666666666,
        "micro_f1": 0.6666666666666666,
        "macro_precision": 0.7379629629629629,
        "macro_recall": 0.7118131868131868,
        "macro_f1": 0.6638241172551632,
        "weighted_precision": 0.7746913580246914,
        "weighted_recall": 0.6666666666666666,
        "weighted_f1": 0.6577836997557184,
        "mcc": 0.4490153383958883
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.3793103448275862,
          "off_by_one_accuracy": 0.7471264367816092,
          "level_accuracy": 0.6551724137931034,
          "rmse": 1.3978637231524922,
          "mae": 0.9885057471264368,
          "quadratic_weighted_kappa": 0.33034501494159185
        },
        "curiosity": {
          "perfect_accuracy": 0.21839080459770116,
          "off_by_one_accuracy": 0.7126436781609196,
          "level_accuracy": 0.5402298850574713,
          "rmse": 1.5829551877538357,
          "mae": 1.2413793103448276,
          "quadratic_weighted_kappa": 0.16939651397039512
        },
        "surprise": {
          "perfect_accuracy": 0.2988505747126437,
          "off_by_one_accuracy": 0.7586206896551724,
          "level_accuracy": 0.6206896551724138,
          "rmse": 1.4621665549733924,
          "mae": 1.0804597701149425,
          "quadratic_weighted_kappa": 0.2517340238601683
        }
      }
    },
    {
      "story_classification": {
        "accuracy": 0.6091954022988506,
        "micro_precision": 0.6091954022988506,
        "micro_recall": 0.6091954022988506,
        "micro_f1": 0.6091954022988506,
        "macro_precision": 0.6774891774891775,
        "macro_recall": 0.6308510638297873,
        "macro_f1": 0.589622641509434,
        "weighted_precision": 0.688112653629895,
        "weighted_recall": 0.6091954022988506,
        "weighted_f1": 0.582411624376491,
        "mcc": 0.3047927013084979
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.28735632183908044,
          "off_by_one_accuracy": 0.7471264367816092,
          "level_accuracy": 0.5977011494252874,
          "rmse": 1.3687816547538927,
          "mae": 1.0459770114942528,
          "quadratic_weighted_kappa": 0.28251960536301546
        },
        "curiosity": {
          "perfect_accuracy": 0.09195402298850575,
          "off_by_one_accuracy": 0.6436781609195402,
          "level_accuracy": 0.4942528735632184,
          "rmse": 1.6678156958735875,
          "mae": 1.4252873563218391,
          "quadratic_weighted_kappa": 0.08181421718273019
        },
        "surprise": {
          "perfect_accuracy": 0.2413793103448276,
          "off_by_one_accuracy": 0.7011494252873564,
          "level_accuracy": 0.6091954022988506,
          "rmse": 1.5312829869775528,
          "mae": 1.1954022988505748,
          "quadratic_weighted_kappa": 0.15558093063088774
        }
      }
    },
    {
      "story_classification": {
        "accuracy": 0.6551724137931034,
        "micro_precision": 0.6551724137931034,
        "micro_recall": 0.6551724137931034,
        "micro_f1": 0.6551724137931034,
        "macro_precision": 0.6894736842105263,
        "macro_recall": 0.6730769230769231,
        "macro_f1": 0.6514423076923077,
        "weighted_precision": 0.7009074410163341,
        "weighted_recall": 0.6551724137931034,
        "weighted_f1": 0.647712201591512,
        "mcc": 0.3621796364634902
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.3103448275862069,
          "off_by_one_accuracy": 0.7816091954022989,
          "level_accuracy": 0.6206896551724138,
          "rmse": 1.2775695315895637,
          "mae": 0.9655172413793104,
          "quadratic_weighted_kappa": 0.43231320650675487
        },
        "curiosity": {
          "perfect_accuracy": 0.1724137931034483,
          "off_by_one_accuracy": 0.7241379310344828,
          "level_accuracy": 0.47126436781609193,
          "rmse": 1.4343888121314305,
          "mae": 1.1839080459770115,
          "quadratic_weighted_kappa": 0.27779065992672647
        },
        "surprise": {
          "perfect_accuracy": 0.25287356321839083,
          "off_by_one_accuracy": 0.7471264367816092,
          "level_accuracy": 0.6091954022988506,
          "rmse": 1.4019690586383533,
          "mae": 1.0919540229885059,
          "quadratic_weighted_kappa": 0.2485984140613161
        }
      }
    },
    {
      "story_classification": {
        "accuracy": 0.632183908045977,
        "micro_precision": 0.632183908045977,
        "micro_recall": 0.632183908045977,
        "micro_f1": 0.632183908045977,
        "macro_precision": 0.7261194029850746,
        "macro_recall": 0.6618589743589743,
        "macro_f1": 0.6137624861265261,
        "weighted_precision": 0.744107050952136,
        "weighted_recall": 0.632183908045977,
        "weighted_f1": 0.6050365494278388,
        "mcc": 0.3826196787925461
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.22988505747126436,
          "off_by_one_accuracy": 0.735632183908046,
          "level_accuracy": 0.5517241379310345,
          "rmse": 1.381320374500968,
          "mae": 1.103448275862069,
          "quadratic_weighted_kappa": 0.3059400230680508
        },
        "curiosity": {
          "perfect_accuracy": 0.09195402298850575,
          "off_by_one_accuracy": 0.632183908045977,
          "level_accuracy": 0.45977011494252873,
          "rmse": 1.6609095970747993,
          "mae": 1.4252873563218391,
          "quadratic_weighted_kappa": 0.1497678964085022
        },
        "surprise": {
          "perfect_accuracy": 0.20689655172413793,
          "off_by_one_accuracy": 0.6551724137931034,
          "level_accuracy": 0.5632183908045977,
          "rmse": 1.5387709679981694,
          "mae": 1.2413793103448276,
          "quadratic_weighted_kappa": 0.1446162657502864
        }
      }
    },
    {
      "story_classification": {
        "accuracy": 0.6046511627906976,
        "micro_precision": 0.6046511627906976,
        "micro_recall": 0.6046511627906976,
        "micro_f1": 0.6046511627906976,
        "macro_precision": 0.6901515151515152,
        "macro_recall": 0.637609649122807,
        "macro_f1": 0.5865384615384615,
        "weighted_precision": 0.708738548273432,
        "weighted_recall": 0.6046511627906976,
        "weighted_f1": 0.5764758497316637,
        "mcc": 0.32352238426526303
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.32558139534883723,
          "off_by_one_accuracy": 0.7093023255813954,
          "level_accuracy": 0.5813953488372093,
          "rmse": 1.4386686351833995,
          "mae": 1.069767441860465,
          "quadratic_weighted_kappa": 0.24724626278520856
        },
        "curiosity": {
          "perfect_accuracy": 0.09302325581395349,
          "off_by_one_accuracy": 0.6627906976744186,
          "level_accuracy": 0.47674418604651164,
          "rmse": 1.6705381391691136,
          "mae": 1.4186046511627908,
          "quadratic_weighted_kappa": 0.1486553374030689
        },
        "surprise": {
          "perfect_accuracy": 0.3023255813953488,
          "off_by_one_accuracy": 0.6627906976744186,
          "level_accuracy": 0.5813953488372093,
          "rmse": 1.5700688771737905,
          "mae": 1.186046511627907,
          "quadratic_weighted_kappa": 0.14242709313264335
        }
      }
    }
  ]
}