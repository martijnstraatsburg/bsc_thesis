{
  "average_metrics": {
    "story_classification": {
      "accuracy": {
        "mean": 0.707457898957498,
        "std": 0.05894945396119563
      },
      "micro_precision": {
        "mean": 0.707457898957498,
        "std": 0.05894945396119563
      },
      "micro_recall": {
        "mean": 0.707457898957498,
        "std": 0.05894945396119563
      },
      "micro_f1": {
        "mean": 0.707457898957498,
        "std": 0.05894945396119564
      },
      "macro_precision": {
        "mean": 0.7125142698606384,
        "std": 0.05778440376921321
      },
      "macro_recall": {
        "mean": 0.7150734754234194,
        "std": 0.060645212315458594
      },
      "macro_f1": {
        "mean": 0.7070100686336388,
        "std": 0.05852348915614586
      },
      "weighted_precision": {
        "mean": 0.7219625859119811,
        "std": 0.061356832096886096
      },
      "weighted_recall": {
        "mean": 0.707457898957498,
        "std": 0.05894945396119563
      },
      "weighted_f1": {
        "mean": 0.7079815031415443,
        "std": 0.059495318530528675
      },
      "mcc": {
        "mean": 0.4275650988811138,
        "std": 0.11835653916172682
      }
    },
    "rating_metrics": {
      "suspense": {
        "perfect_accuracy": {
          "mean": 0.3502272119754076,
          "std": 0.040153714474990644
        },
        "off_by_one_accuracy": {
          "mean": 0.8295375568029938,
          "std": 0.02733317580889109
        },
        "level_accuracy": {
          "mean": 0.6083934776797648,
          "std": 0.036095121116060114
        },
        "rmse": {
          "mean": 1.2133184882716754,
          "std": 0.09394627041647931
        },
        "mae": {
          "mean": 0.8823576583801123,
          "std": 0.08232625821229303
        },
        "quadratic_weighted_kappa": {
          "mean": 0.38936913003189594,
          "std": 0.08707970889425404
        }
      },
      "curiosity": {
        "perfect_accuracy": {
          "mean": 0.3202085004009623,
          "std": 0.04151946161168755
        },
        "off_by_one_accuracy": {
          "mean": 0.8204223469660519,
          "std": 0.06080198831422867
        },
        "level_accuracy": {
          "mean": 0.5392408446939321,
          "std": 0.04468491433820651
        },
        "rmse": {
          "mean": 1.1641362787498781,
          "std": 0.11344657182467552
        },
        "mae": {
          "mean": 0.8892809409248864,
          "std": 0.10002691655037732
        },
        "quadratic_weighted_kappa": {
          "mean": 0.4426423676121827,
          "std": 0.10749410811041023
        }
      },
      "surprise": {
        "perfect_accuracy": {
          "mean": 0.32496658647420473,
          "std": 0.07286351008129005
        },
        "off_by_one_accuracy": {
          "mean": 0.725821972734563,
          "std": 0.00801193643503256
        },
        "level_accuracy": {
          "mean": 0.5623362737236034,
          "std": 0.04309366451399672
        },
        "rmse": {
          "mean": 1.3885706319160698,
          "std": 0.051108158955586944
        },
        "mae": {
          "mean": 1.0321571772253408,
          "std": 0.08270887397880665
        },
        "quadratic_weighted_kappa": {
          "mean": 0.23770213125654488,
          "std": 0.0815761606390886
        }
      }
    }
  },
  "fold_metrics": [
    {
      "story_classification": {
        "accuracy": 0.7816091954022989,
        "micro_precision": 0.7816091954022989,
        "micro_recall": 0.7816091954022989,
        "micro_f1": 0.781609195402299,
        "macro_precision": 0.78276955602537,
        "macro_recall": 0.7939560439560439,
        "macro_f1": 0.779746835443038,
        "weighted_precision": 0.8024956866175792,
        "weighted_recall": 0.7816091954022989,
        "weighted_f1": 0.7837043503564673,
        "mcc": 0.5766171001294522
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.3793103448275862,
          "off_by_one_accuracy": 0.8505747126436781,
          "level_accuracy": 0.5977011494252874,
          "rmse": 1.2223963651627971,
          "mae": 0.8505747126436781,
          "quadratic_weighted_kappa": 0.426179604261796
        },
        "curiosity": {
          "perfect_accuracy": 0.39080459770114945,
          "off_by_one_accuracy": 0.896551724137931,
          "level_accuracy": 0.6091954022988506,
          "rmse": 1.0170952554312156,
          "mae": 0.735632183908046,
          "quadratic_weighted_kappa": 0.5640311804008908
        },
        "surprise": {
          "perfect_accuracy": 0.42528735632183906,
          "off_by_one_accuracy": 0.7241379310344828,
          "level_accuracy": 0.5977011494252874,
          "rmse": 1.3518824678560455,
          "mae": 0.9310344827586207,
          "quadratic_weighted_kappa": 0.3174618838505946
        }
      }
    },
    {
      "story_classification": {
        "accuracy": 0.735632183908046,
        "micro_precision": 0.735632183908046,
        "micro_recall": 0.735632183908046,
        "micro_f1": 0.735632183908046,
        "macro_precision": 0.7373015873015873,
        "macro_recall": 0.7385638297872341,
        "macro_f1": 0.7354923992068736,
        "weighted_precision": 0.7411968618865171,
        "weighted_recall": 0.735632183908046,
        "weighted_f1": 0.7359816456609763,
        "mcc": 0.47586374302422463
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.39080459770114945,
          "off_by_one_accuracy": 0.8160919540229885,
          "level_accuracy": 0.5977011494252874,
          "rmse": 1.1793237883215744,
          "mae": 0.8390804597701149,
          "quadratic_weighted_kappa": 0.39931526390870187
        },
        "curiosity": {
          "perfect_accuracy": 0.28735632183908044,
          "off_by_one_accuracy": 0.735632183908046,
          "level_accuracy": 0.4942528735632184,
          "rmse": 1.317433939105884,
          "mae": 1.0229885057471264,
          "quadratic_weighted_kappa": 0.28792888503441927
        },
        "surprise": {
          "perfect_accuracy": 0.3563218390804598,
          "off_by_one_accuracy": 0.7126436781609196,
          "level_accuracy": 0.5402298850574713,
          "rmse": 1.3390681268239724,
          "mae": 0.9885057471264368,
          "quadratic_weighted_kappa": 0.26843467011642963
        }
      }
    },
    {
      "story_classification": {
        "accuracy": 0.6436781609195402,
        "micro_precision": 0.6436781609195402,
        "micro_recall": 0.6436781609195402,
        "micro_f1": 0.6436781609195402,
        "macro_precision": 0.6471367974549311,
        "macro_recall": 0.6482371794871795,
        "macro_f1": 0.6434897554527428,
        "weighted_precision": 0.6533623432186346,
        "weighted_recall": 0.6436781609195402,
        "weighted_f1": 0.6443375800533309,
        "mcc": 0.2953719272611762
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.27586206896551724,
          "off_by_one_accuracy": 0.7816091954022989,
          "level_accuracy": 0.5517241379310345,
          "rmse": 1.3854747308720783,
          "mae": 1.0459770114942528,
          "quadratic_weighted_kappa": 0.22738633342196224
        },
        "curiosity": {
          "perfect_accuracy": 0.3448275862068966,
          "off_by_one_accuracy": 0.7931034482758621,
          "level_accuracy": 0.4942528735632184,
          "rmse": 1.1793237883215744,
          "mae": 0.8850574712643678,
          "quadratic_weighted_kappa": 0.40245217687460966
        },
        "surprise": {
          "perfect_accuracy": 0.22988505747126436,
          "off_by_one_accuracy": 0.7241379310344828,
          "level_accuracy": 0.5632183908045977,
          "rmse": 1.4816890020684417,
          "mae": 1.160919540229885,
          "quadratic_weighted_kappa": 0.09379942193379498
        }
      }
    },
    {
      "story_classification": {
        "accuracy": 0.632183908045977,
        "micro_precision": 0.632183908045977,
        "micro_recall": 0.632183908045977,
        "micro_f1": 0.632183908045977,
        "macro_precision": 0.6409774436090225,
        "macro_recall": 0.640224358974359,
        "macro_f1": 0.6321353065539113,
        "weighted_precision": 0.6481721545242417,
        "weighted_recall": 0.632183908045977,
        "weighted_f1": 0.631697893125319,
        "mcc": 0.28120079416615473
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.3563218390804598,
          "off_by_one_accuracy": 0.8505747126436781,
          "level_accuracy": 0.6436781609195402,
          "rmse": 1.174440439029407,
          "mae": 0.8505747126436781,
          "quadratic_weighted_kappa": 0.40465328467153283
        },
        "curiosity": {
          "perfect_accuracy": 0.28735632183908044,
          "off_by_one_accuracy": 0.7931034482758621,
          "level_accuracy": 0.5287356321839081,
          "rmse": 1.2502873232999676,
          "mae": 0.9655172413793104,
          "quadratic_weighted_kappa": 0.3934174100276837
        },
        "surprise": {
          "perfect_accuracy": 0.25287356321839083,
          "off_by_one_accuracy": 0.735632183908046,
          "level_accuracy": 0.4942528735632184,
          "rmse": 1.4019690586383533,
          "mae": 1.0919540229885059,
          "quadratic_weighted_kappa": 0.20592473979183357
        }
      }
    },
    {
      "story_classification": {
        "accuracy": 0.7441860465116279,
        "micro_precision": 0.7441860465116279,
        "micro_recall": 0.7441860465116279,
        "micro_f1": 0.7441860465116278,
        "macro_precision": 0.7543859649122806,
        "macro_recall": 0.7543859649122806,
        "macro_f1": 0.744186046511628,
        "weighted_precision": 0.7645858833129334,
        "weighted_recall": 0.7441860465116279,
        "weighted_f1": 0.7441860465116281,
        "mcc": 0.5087719298245614
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.3488372093023256,
          "off_by_one_accuracy": 0.8488372093023255,
          "level_accuracy": 0.6511627906976745,
          "rmse": 1.1049571179725208,
          "mae": 0.8255813953488372,
          "quadratic_weighted_kappa": 0.4893111638954869
        },
        "curiosity": {
          "perfect_accuracy": 0.29069767441860467,
          "off_by_one_accuracy": 0.8837209302325582,
          "level_accuracy": 0.5697674418604651,
          "rmse": 1.0565410875907486,
          "mae": 0.8372093023255814,
          "quadratic_weighted_kappa": 0.5653821857233101
        },
        "surprise": {
          "perfect_accuracy": 0.36046511627906974,
          "off_by_one_accuracy": 0.7325581395348837,
          "level_accuracy": 0.6162790697674418,
          "rmse": 1.3682445041935356,
          "mae": 0.9883720930232558,
          "quadratic_weighted_kappa": 0.30288994059007157
        }
      }
    }
  ]
}