{
  "average_metrics": {
    "story_classification": {
      "accuracy": {
        "mean": 0.6075391180654338,
        "std": 0.036278414250488104
      },
      "micro_precision": {
        "mean": 0.6075391180654338,
        "std": 0.036278414250488104
      },
      "micro_recall": {
        "mean": 0.6075391180654338,
        "std": 0.036278414250488104
      },
      "micro_f1": {
        "mean": 0.6075391180654338,
        "std": 0.036278414250488104
      },
      "macro_precision": {
        "mean": 0.7217089947089947,
        "std": 0.033173858881299007
      },
      "macro_recall": {
        "mean": 0.6844401974836758,
        "std": 0.03090325098826993
      },
      "macro_f1": {
        "mean": 0.6024718720905142,
        "std": 0.03808692235474512
      },
      "weighted_precision": {
        "mean": 0.7863999036630617,
        "std": 0.03504418550634728
      },
      "weighted_recall": {
        "mean": 0.6075391180654338,
        "std": 0.036278414250488104
      },
      "weighted_f1": {
        "mean": 0.5917151675745991,
        "std": 0.04164933083705465
      },
      "mcc": {
        "mean": 0.40392556614330843,
        "std": 0.060693461556059994
      }
    },
    "rating_metrics": {
      "suspense": {
        "perfect_accuracy": {
          "mean": 0.3874822190611665,
          "std": 0.0926799025361904
        },
        "off_by_one_accuracy": {
          "mean": 0.8278805120910384,
          "std": 0.06320657332346545
        },
        "level_accuracy": {
          "mean": 0.559601706970128,
          "std": 0.08341867523660794
        },
        "rmse": {
          "mean": 1.0665394805445985,
          "std": 0.13298710681513756
        },
        "mae": {
          "mean": 0.7899004267425319,
          "std": 0.14681955379410574
        },
        "quadratic_weighted_kappa": {
          "mean": 0.23759316598772973,
          "std": 0.1625616822732152
        }
      },
      "curiosity": {
        "perfect_accuracy": {
          "mean": 0.3226173541963016,
          "std": 0.04535353922073631
        },
        "off_by_one_accuracy": {
          "mean": 0.7364153627311522,
          "std": 0.06070419809315625
        },
        "level_accuracy": {
          "mean": 0.44637268847795164,
          "std": 0.023707159779231487
        },
        "rmse": {
          "mean": 1.2912128481832927,
          "std": 0.11948725580360231
        },
        "mae": {
          "mean": 0.9836415362731152,
          "std": 0.11281402466983736
        },
        "quadratic_weighted_kappa": {
          "mean": 0.20419039649116372,
          "std": 0.12811185756595164
        }
      },
      "surprise": {
        "perfect_accuracy": {
          "mean": 0.34935988620199143,
          "std": 0.01415630760127991
        },
        "off_by_one_accuracy": {
          "mean": 0.7796586059743954,
          "std": 0.04597126167086669
        },
        "level_accuracy": {
          "mean": 0.6073968705547653,
          "std": 0.028985002849388406
        },
        "rmse": {
          "mean": 1.2981320093006228,
          "std": 0.05255709297801316
        },
        "mae": {
          "mean": 0.946230440967283,
          "std": 0.038226223060982126
        },
        "quadratic_weighted_kappa": {
          "mean": 0.1703429541396872,
          "std": 0.06639165130266991
        }
      }
    }
  },
  "fold_metrics": [
    {
      "story_classification": {
        "accuracy": 0.6216216216216216,
        "micro_precision": 0.6216216216216216,
        "micro_recall": 0.6216216216216216,
        "micro_f1": 0.6216216216216216,
        "macro_precision": 0.7407407407407407,
        "macro_recall": 0.7083333333333334,
        "macro_f1": 0.6191176470588236,
        "weighted_precision": 0.8178178178178178,
        "weighted_recall": 0.6216216216216216,
        "weighted_f1": 0.6099364069952306,
        "mcc": 0.44790320823880836
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.5405405405405406,
          "off_by_one_accuracy": 0.918918918918919,
          "level_accuracy": 0.6756756756756757,
          "rmse": 0.8382736442849094,
          "mae": 0.5405405405405406,
          "quadratic_weighted_kappa": 0.5076765609007166
        },
        "curiosity": {
          "perfect_accuracy": 0.32432432432432434,
          "off_by_one_accuracy": 0.7837837837837838,
          "level_accuracy": 0.4594594594594595,
          "rmse": 1.150792911137501,
          "mae": 0.8918918918918919,
          "quadratic_weighted_kappa": 0.3671902268760907
        },
        "surprise": {
          "perfect_accuracy": 0.32432432432432434,
          "off_by_one_accuracy": 0.8108108108108109,
          "level_accuracy": 0.5945945945945946,
          "rmse": 1.2302493704584911,
          "mae": 0.918918918918919,
          "quadratic_weighted_kappa": 0.29379686434901153
        }
      }
    },
    {
      "story_classification": {
        "accuracy": 0.6486486486486487,
        "micro_precision": 0.6486486486486487,
        "micro_recall": 0.6486486486486487,
        "micro_f1": 0.6486486486486487,
        "macro_precision": 0.7183333333333333,
        "macro_recall": 0.7034161490683231,
        "macro_f1": 0.6476190476190475,
        "weighted_precision": 0.7665765765765766,
        "weighted_recall": 0.6486486486486487,
        "weighted_f1": 0.642985842985843,
        "mcc": 0.4214855911175004
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.40540540540540543,
          "off_by_one_accuracy": 0.8648648648648649,
          "level_accuracy": 0.5675675675675675,
          "rmse": 1.0,
          "mae": 0.7297297297297297,
          "quadratic_weighted_kappa": 0.19706744868035186
        },
        "curiosity": {
          "perfect_accuracy": 0.40540540540540543,
          "off_by_one_accuracy": 0.7837837837837838,
          "level_accuracy": 0.4864864864864865,
          "rmse": 1.1740436015661335,
          "mae": 0.8378378378378378,
          "quadratic_weighted_kappa": 0.24970178926441344
        },
        "surprise": {
          "perfect_accuracy": 0.35135135135135137,
          "off_by_one_accuracy": 0.8378378378378378,
          "level_accuracy": 0.5675675675675675,
          "rmse": 1.2411851354816252,
          "mae": 0.8918918918918919,
          "quadratic_weighted_kappa": 0.1105018979333614
        }
      }
    },
    {
      "story_classification": {
        "accuracy": 0.6052631578947368,
        "micro_precision": 0.6052631578947368,
        "micro_recall": 0.6052631578947368,
        "micro_f1": 0.6052631578947368,
        "macro_precision": 0.75,
        "macro_recall": 0.6739130434782609,
        "macro_f1": 0.5913978494623655,
        "weighted_precision": 0.8026315789473685,
        "weighted_recall": 0.6052631578947368,
        "weighted_f1": 0.5755517826825127,
        "mcc": 0.41702882811414954
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.3157894736842105,
          "off_by_one_accuracy": 0.8421052631578947,
          "level_accuracy": 0.47368421052631576,
          "rmse": 1.1355499479153377,
          "mae": 0.868421052631579,
          "quadratic_weighted_kappa": 0.07547169811320753
        },
        "curiosity": {
          "perfect_accuracy": 0.3157894736842105,
          "off_by_one_accuracy": 0.7631578947368421,
          "level_accuracy": 0.42105263157894735,
          "rmse": 1.3860204297119676,
          "mae": 1.0263157894736843,
          "quadratic_weighted_kappa": 0.0545330606680301
        },
        "surprise": {
          "perfect_accuracy": 0.3684210526315789,
          "off_by_one_accuracy": 0.7631578947368421,
          "level_accuracy": 0.631578947368421,
          "rmse": 1.3178930553209385,
          "mae": 0.9473684210526315,
          "quadratic_weighted_kappa": 0.17662508207485217
        }
      }
    },
    {
      "story_classification": {
        "accuracy": 0.5405405405405406,
        "micro_precision": 0.5405405405405406,
        "micro_recall": 0.5405405405405406,
        "micro_f1": 0.5405405405405406,
        "macro_precision": 0.6587301587301587,
        "macro_recall": 0.6282051282051282,
        "macro_f1": 0.5351071692535108,
        "weighted_precision": 0.7271557271557272,
        "weighted_recall": 0.5405405405405406,
        "weighted_f1": 0.5201653982141787,
        "mcc": 0.2853069950072753
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.2702702702702703,
          "off_by_one_accuracy": 0.7567567567567568,
          "level_accuracy": 0.4594594594594595,
          "rmse": 1.2080808993852437,
          "mae": 0.972972972972973,
          "quadratic_weighted_kappa": 0.08348623853211012
        },
        "curiosity": {
          "perfect_accuracy": 0.2702702702702703,
          "off_by_one_accuracy": 0.6216216216216216,
          "level_accuracy": 0.43243243243243246,
          "rmse": 1.461210161179813,
          "mae": 1.162162162162162,
          "quadratic_weighted_kappa": 0.0537390741340239
        },
        "surprise": {
          "perfect_accuracy": 0.35135135135135137,
          "off_by_one_accuracy": 0.7027027027027027,
          "level_accuracy": 0.5945945945945946,
          "rmse": 1.34566370643293,
          "mae": 1.0,
          "quadratic_weighted_kappa": 0.11622103386809268
        }
      }
    },
    {
      "story_classification": {
        "accuracy": 0.6216216216216216,
        "micro_precision": 0.6216216216216216,
        "micro_recall": 0.6216216216216216,
        "micro_f1": 0.6216216216216216,
        "macro_precision": 0.7407407407407407,
        "macro_recall": 0.7083333333333334,
        "macro_f1": 0.6191176470588236,
        "weighted_precision": 0.8178178178178178,
        "weighted_recall": 0.6216216216216216,
        "weighted_f1": 0.6099364069952306,
        "mcc": 0.44790320823880836
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.40540540540540543,
          "off_by_one_accuracy": 0.7567567567567568,
          "level_accuracy": 0.6216216216216216,
          "rmse": 1.150792911137501,
          "mae": 0.8378378378378378,
          "quadratic_weighted_kappa": 0.32426388371226245
        },
        "curiosity": {
          "perfect_accuracy": 0.2972972972972973,
          "off_by_one_accuracy": 0.7297297297297297,
          "level_accuracy": 0.43243243243243246,
          "rmse": 1.283997137321049,
          "mae": 1.0,
          "quadratic_weighted_kappa": 0.2957878315132605
        },
        "surprise": {
          "perfect_accuracy": 0.35135135135135137,
          "off_by_one_accuracy": 0.7837837837837838,
          "level_accuracy": 0.6486486486486487,
          "rmse": 1.35566877880913,
          "mae": 0.972972972972973,
          "quadratic_weighted_kappa": 0.15456989247311825
        }
      }
    }
  ]
}