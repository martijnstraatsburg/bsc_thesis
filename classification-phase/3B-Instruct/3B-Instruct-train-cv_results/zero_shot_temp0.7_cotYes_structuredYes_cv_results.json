{
  "average_metrics": {
    "story_classification": {
      "accuracy": {
        "mean": 0.5277198609997328,
        "std": 0.024921613003781002
      },
      "micro_precision": {
        "mean": 0.5277198609997328,
        "std": 0.024921613003781002
      },
      "micro_recall": {
        "mean": 0.5277198609997328,
        "std": 0.024921613003781002
      },
      "micro_f1": {
        "mean": 0.5277198609997328,
        "std": 0.024921613003781002
      },
      "macro_precision": {
        "mean": 0.7021461317652745,
        "std": 0.05303442621525564
      },
      "macro_recall": {
        "mean": 0.5763161483906164,
        "std": 0.02805959311463601
      },
      "macro_f1": {
        "mean": 0.46511204172675297,
        "std": 0.03560060266345223
      },
      "weighted_precision": {
        "mean": 0.7293799655325635,
        "std": 0.060729077748891726
      },
      "weighted_recall": {
        "mean": 0.5277198609997328,
        "std": 0.024921613003781002
      },
      "weighted_f1": {
        "mean": 0.4441641564806262,
        "std": 0.03650828747460521
      },
      "mcc": {
        "mean": 0.2473912955914137,
        "std": 0.07661402514773534
      }
    },
    "rating_metrics": {
      "suspense": {
        "perfect_accuracy": {
          "mean": 0.24199411921946007,
          "std": 0.017146857809752915
        },
        "off_by_one_accuracy": {
          "mean": 0.6429296979417268,
          "std": 0.02627703571553751
        },
        "level_accuracy": {
          "mean": 0.5944934509489441,
          "std": 0.012566329081575171
        },
        "rmse": {
          "mean": 1.6383444793866944,
          "std": 0.04713150919839597
        },
        "mae": {
          "mean": 1.2855920876770917,
          "std": 0.04380221599194078
        },
        "quadratic_weighted_kappa": {
          "mean": 0.1139559412112627,
          "std": 0.03607876103820184
        }
      },
      "curiosity": {
        "perfect_accuracy": {
          "mean": 0.10596097300187117,
          "std": 0.024428284790762197
        },
        "off_by_one_accuracy": {
          "mean": 0.5899492114407913,
          "std": 0.02936402243168132
        },
        "level_accuracy": {
          "mean": 0.5092221331194867,
          "std": 0.011267603721661997
        },
        "rmse": {
          "mean": 1.7658894248113122,
          "std": 0.07615248646428077
        },
        "mae": {
          "mean": 1.5022453889334402,
          "std": 0.07139453411750069
        },
        "quadratic_weighted_kappa": {
          "mean": 0.10812432567986441,
          "std": 0.06414815572855677
        }
      },
      "surprise": {
        "perfect_accuracy": {
          "mean": 0.166025126971398,
          "std": 0.03608229106814985
        },
        "off_by_one_accuracy": {
          "mean": 0.6036888532477948,
          "std": 0.005162855773539843
        },
        "level_accuracy": {
          "mean": 0.5760491847099706,
          "std": 0.002673082063619381
        },
        "rmse": {
          "mean": 1.72468371855141,
          "std": 0.0428488683781297
        },
        "mae": {
          "mean": 1.4191392675755146,
          "std": 0.05237522152894223
        },
        "quadratic_weighted_kappa": {
          "mean": 0.060804469379483694,
          "std": 0.031839974321796
        }
      }
    }
  },
  "fold_metrics": [
    {
      "story_classification": {
        "accuracy": 0.5287356321839081,
        "micro_precision": 0.5287356321839081,
        "micro_recall": 0.5287356321839081,
        "micro_f1": 0.5287356321839081,
        "macro_precision": 0.7302631578947368,
        "macro_recall": 0.6057692307692307,
        "macro_f1": 0.48991848991848996,
        "weighted_precision": 0.7829703569267997,
        "weighted_recall": 0.5287356321839081,
        "weighted_f1": 0.4624230141471521,
        "mcc": 0.31212021456496686
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.25287356321839083,
          "off_by_one_accuracy": 0.6551724137931034,
          "level_accuracy": 0.5747126436781609,
          "rmse": 1.6678156958735875,
          "mae": 1.2873563218390804,
          "quadratic_weighted_kappa": 0.08189429618001054
        },
        "curiosity": {
          "perfect_accuracy": 0.12643678160919541,
          "off_by_one_accuracy": 0.5747126436781609,
          "level_accuracy": 0.5057471264367817,
          "rmse": 1.8003831009940154,
          "mae": 1.5172413793103448,
          "quadratic_weighted_kappa": 0.08639308855291583
        },
        "surprise": {
          "perfect_accuracy": 0.19540229885057472,
          "off_by_one_accuracy": 0.5977011494252874,
          "level_accuracy": 0.5747126436781609,
          "rmse": 1.7353657403768128,
          "mae": 1.4022988505747127,
          "quadratic_weighted_kappa": 0.04411641365428176
        }
      }
    },
    {
      "story_classification": {
        "accuracy": 0.5517241379310345,
        "micro_precision": 0.5517241379310345,
        "micro_recall": 0.5517241379310345,
        "micro_f1": 0.5517241379310345,
        "macro_precision": 0.7531645569620253,
        "macro_recall": 0.5851063829787234,
        "macro_f1": 0.4815889992360581,
        "weighted_precision": 0.7730248799650807,
        "weighted_recall": 0.5517241379310345,
        "weighted_f1": 0.466246937646532,
        "mcc": 0.2935705689707261
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.2413793103448276,
          "off_by_one_accuracy": 0.632183908045977,
          "level_accuracy": 0.5862068965517241,
          "rmse": 1.5610194134588469,
          "mae": 1.2413793103448276,
          "quadratic_weighted_kappa": 0.150359314538419
        },
        "curiosity": {
          "perfect_accuracy": 0.06896551724137931,
          "off_by_one_accuracy": 0.5747126436781609,
          "level_accuracy": 0.4942528735632184,
          "rmse": 1.7843508752420336,
          "mae": 1.5517241379310345,
          "quadratic_weighted_kappa": 0.051183117445568804
        },
        "surprise": {
          "perfect_accuracy": 0.13793103448275862,
          "off_by_one_accuracy": 0.6091954022988506,
          "level_accuracy": 0.5747126436781609,
          "rmse": 1.7187271419587657,
          "mae": 1.4367816091954022,
          "quadratic_weighted_kappa": 0.0857832113505338
        }
      }
    },
    {
      "story_classification": {
        "accuracy": 0.5057471264367817,
        "micro_precision": 0.5057471264367817,
        "micro_recall": 0.5057471264367817,
        "micro_f1": 0.5057471264367817,
        "macro_precision": 0.6260683760683761,
        "macro_recall": 0.547275641025641,
        "macro_f1": 0.4390463337831759,
        "weighted_precision": 0.6417624521072797,
        "weighted_recall": 0.5057471264367817,
        "weighted_f1": 0.41903609598709424,
        "mcc": 0.15440159703440975
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.22988505747126436,
          "off_by_one_accuracy": 0.6551724137931034,
          "level_accuracy": 0.6091954022988506,
          "rmse": 1.6849571091781876,
          "mae": 1.3218390804597702,
          "quadratic_weighted_kappa": 0.11396528264544603
        },
        "curiosity": {
          "perfect_accuracy": 0.13793103448275862,
          "off_by_one_accuracy": 0.6206896551724138,
          "level_accuracy": 0.5287356321839081,
          "rmse": 1.6400168207716022,
          "mae": 1.3793103448275863,
          "quadratic_weighted_kappa": 0.20058116704625784
        },
        "surprise": {
          "perfect_accuracy": 0.12643678160919541,
          "off_by_one_accuracy": 0.6091954022988506,
          "level_accuracy": 0.5747126436781609,
          "rmse": 1.7386743529905555,
          "mae": 1.4597701149425288,
          "quadratic_weighted_kappa": 0.023889765794974815
        }
      }
    },
    {
      "story_classification": {
        "accuracy": 0.4942528735632184,
        "micro_precision": 0.4942528735632184,
        "micro_recall": 0.4942528735632184,
        "micro_f1": 0.4942528735632184,
        "macro_precision": 0.6512345679012346,
        "macro_recall": 0.5392628205128205,
        "macro_f1": 0.40925925925925927,
        "weighted_precision": 0.6700723712217964,
        "weighted_recall": 0.4942528735632184,
        "weighted_f1": 0.3860791826309068,
        "mcc": 0.1541154852030144
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.21839080459770116,
          "off_by_one_accuracy": 0.5977011494252874,
          "level_accuracy": 0.5977011494252874,
          "rmse": 1.6712580435934667,
          "mae": 1.3448275862068966,
          "quadratic_weighted_kappa": 0.06625149065853986
        },
        "curiosity": {
          "perfect_accuracy": 0.10344827586206896,
          "off_by_one_accuracy": 0.5517241379310345,
          "level_accuracy": 0.5057471264367817,
          "rmse": 1.869292078184471,
          "mae": 1.5862068965517242,
          "quadratic_weighted_kappa": 0.037134119702927015
        },
        "surprise": {
          "perfect_accuracy": 0.14942528735632185,
          "off_by_one_accuracy": 0.5977011494252874,
          "level_accuracy": 0.5747126436781609,
          "rmse": 1.7811271131234425,
          "mae": 1.471264367816092,
          "quadratic_weighted_kappa": 0.04051786142411884
        }
      }
    },
    {
      "story_classification": {
        "accuracy": 0.5581395348837209,
        "micro_precision": 0.5581395348837209,
        "micro_recall": 0.5581395348837209,
        "micro_f1": 0.5581395348837209,
        "macro_precision": 0.75,
        "macro_recall": 0.6041666666666666,
        "macro_f1": 0.5057471264367817,
        "weighted_precision": 0.7790697674418605,
        "weighted_recall": 0.5581395348837209,
        "weighted_f1": 0.48703555199144616,
        "mcc": 0.3227486121839514
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.26744186046511625,
          "off_by_one_accuracy": 0.6744186046511628,
          "level_accuracy": 0.6046511627906976,
          "rmse": 1.6066721348293838,
          "mae": 1.2325581395348837,
          "quadratic_weighted_kappa": 0.15730932203389814
        },
        "curiosity": {
          "perfect_accuracy": 0.09302325581395349,
          "off_by_one_accuracy": 0.627906976744186,
          "level_accuracy": 0.5116279069767442,
          "rmse": 1.735404248864438,
          "mae": 1.4767441860465116,
          "quadratic_weighted_kappa": 0.16533013565165255
        },
        "surprise": {
          "perfect_accuracy": 0.22093023255813954,
          "off_by_one_accuracy": 0.6046511627906976,
          "level_accuracy": 0.5813953488372093,
          "rmse": 1.649524244307473,
          "mae": 1.3255813953488371,
          "quadratic_weighted_kappa": 0.10971509467350926
        }
      }
    }
  ]
}