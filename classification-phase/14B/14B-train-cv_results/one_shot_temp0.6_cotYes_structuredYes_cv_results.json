{
  "average_metrics": {
    "story_classification": {
      "accuracy": {
        "mean": 0.6936113338679497,
        "std": 0.04475713330666616
      },
      "micro_precision": {
        "mean": 0.6936113338679497,
        "std": 0.04475713330666616
      },
      "micro_recall": {
        "mean": 0.6936113338679497,
        "std": 0.04475713330666616
      },
      "micro_f1": {
        "mean": 0.6936113338679497,
        "std": 0.04475713330666616
      },
      "macro_precision": {
        "mean": 0.730257917979149,
        "std": 0.046115783866860424
      },
      "macro_recall": {
        "mean": 0.713159679066734,
        "std": 0.04606040744027325
      },
      "macro_f1": {
        "mean": 0.6901465272046395,
        "std": 0.04583864694072035
      },
      "weighted_precision": {
        "mean": 0.7455105960868516,
        "std": 0.04913239773452038
      },
      "weighted_recall": {
        "mean": 0.6936113338679497,
        "std": 0.04475713330666616
      },
      "weighted_f1": {
        "mean": 0.687530664147701,
        "std": 0.047265026390517766
      },
      "mcc": {
        "mean": 0.4428824532002693,
        "std": 0.0910884745967981
      }
    },
    "rating_metrics": {
      "suspense": {
        "perfect_accuracy": {
          "mean": 0.26271050521251,
          "std": 0.02493594456767401
        },
        "off_by_one_accuracy": {
          "mean": 0.6843892007484629,
          "std": 0.017711786081054007
        },
        "level_accuracy": {
          "mean": 0.5599572306869821,
          "std": 0.01712475762659686
        },
        "rmse": {
          "mean": 1.4952036489691642,
          "std": 0.0757981428144762
        },
        "mae": {
          "mean": 1.1634322373696873,
          "std": 0.072633025970203
        },
        "quadratic_weighted_kappa": {
          "mean": 0.16073951056223307,
          "std": 0.05494471805228073
        }
      },
      "curiosity": {
        "perfect_accuracy": {
          "mean": 0.20732424485431702,
          "std": 0.038905458273879534
        },
        "off_by_one_accuracy": {
          "mean": 0.7304998663458968,
          "std": 0.018999879203414005
        },
        "level_accuracy": {
          "mean": 0.4977011494252874,
          "std": 0.05696327215339708
        },
        "rmse": {
          "mean": 1.4136159741256495,
          "std": 0.07460460233148293
        },
        "mae": {
          "mean": 1.1427158513766371,
          "std": 0.05045079657197899
        },
        "quadratic_weighted_kappa": {
          "mean": 0.26392227197544516,
          "std": 0.07697025740471967
        }
      },
      "surprise": {
        "perfect_accuracy": {
          "mean": 0.23509756749532212,
          "std": 0.040031668595142816
        },
        "off_by_one_accuracy": {
          "mean": 0.6913392141138732,
          "std": 0.04008995763096265
        },
        "level_accuracy": {
          "mean": 0.5713980219192729,
          "std": 0.02279410288770045
        },
        "rmse": {
          "mean": 1.5282885171652594,
          "std": 0.0675580739681319
        },
        "mae": {
          "mean": 1.2025126971398021,
          "std": 0.08406077678323723
        },
        "quadratic_weighted_kappa": {
          "mean": 0.17313392872348482,
          "std": 0.07201533910643027
        }
      }
    }
  },
  "fold_metrics": [
    {
      "story_classification": {
        "accuracy": 0.735632183908046,
        "micro_precision": 0.735632183908046,
        "micro_recall": 0.735632183908046,
        "micro_f1": 0.735632183908046,
        "macro_precision": 0.7559459459459459,
        "macro_recall": 0.7601648351648351,
        "macro_f1": 0.7354923992068738,
        "weighted_precision": 0.7825100963031997,
        "weighted_recall": 0.735632183908046,
        "weighted_f1": 0.7366805691668376,
        "mcc": 0.5160935374062917
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.27586206896551724,
          "off_by_one_accuracy": 0.6896551724137931,
          "level_accuracy": 0.5632183908045977,
          "rmse": 1.4816890020684417,
          "mae": 1.1379310344827587,
          "quadratic_weighted_kappa": 0.17504840391202892
        },
        "curiosity": {
          "perfect_accuracy": 0.26436781609195403,
          "off_by_one_accuracy": 0.7241379310344828,
          "level_accuracy": 0.5402298850574713,
          "rmse": 1.381320374500968,
          "mae": 1.0804597701149425,
          "quadratic_weighted_kappa": 0.3033957167663516
        },
        "surprise": {
          "perfect_accuracy": 0.28735632183908044,
          "off_by_one_accuracy": 0.7471264367816092,
          "level_accuracy": 0.6091954022988506,
          "rmse": 1.4894263340621117,
          "mae": 1.1149425287356323,
          "quadratic_weighted_kappa": 0.24741159069517293
        }
      }
    },
    {
      "story_classification": {
        "accuracy": 0.7126436781609196,
        "micro_precision": 0.7126436781609196,
        "micro_recall": 0.7126436781609196,
        "micro_f1": 0.7126436781609196,
        "macro_precision": 0.7599878934624698,
        "macro_recall": 0.7284574468085107,
        "macro_f1": 0.7070707070707072,
        "weighted_precision": 0.7706785227241102,
        "weighted_recall": 0.7126436781609196,
        "weighted_f1": 0.7038198072680832,
        "mcc": 0.48742659074596617
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.28735632183908044,
          "off_by_one_accuracy": 0.6896551724137931,
          "level_accuracy": 0.5402298850574713,
          "rmse": 1.450326954814696,
          "mae": 1.1149425287356323,
          "quadratic_weighted_kappa": 0.1304276585285924
        },
        "curiosity": {
          "perfect_accuracy": 0.14942528735632185,
          "off_by_one_accuracy": 0.7241379310344828,
          "level_accuracy": 0.39080459770114945,
          "rmse": 1.4019690586383533,
          "mae": 1.1839080459770115,
          "quadratic_weighted_kappa": 0.1480845215598694
        },
        "surprise": {
          "perfect_accuracy": 0.21839080459770116,
          "off_by_one_accuracy": 0.6666666666666666,
          "level_accuracy": 0.5517241379310345,
          "rmse": 1.5720255620746413,
          "mae": 1.2528735632183907,
          "quadratic_weighted_kappa": 0.12850020966314113
        }
      }
    },
    {
      "story_classification": {
        "accuracy": 0.6091954022988506,
        "micro_precision": 0.6091954022988506,
        "micro_recall": 0.6091954022988506,
        "micro_f1": 0.6091954022988506,
        "macro_precision": 0.6385964912280702,
        "macro_recall": 0.6266025641025641,
        "macro_f1": 0.6049679487179487,
        "weighted_precision": 0.6483968542044767,
        "weighted_recall": 0.6091954022988506,
        "weighted_f1": 0.6007404951370469,
        "mcc": 0.2649276970427382
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.21839080459770116,
          "off_by_one_accuracy": 0.6551724137931034,
          "level_accuracy": 0.5402298850574713,
          "rmse": 1.6365087593838994,
          "mae": 1.2988505747126438,
          "quadratic_weighted_kappa": 0.08528496006497899
        },
        "curiosity": {
          "perfect_accuracy": 0.20689655172413793,
          "off_by_one_accuracy": 0.7126436781609196,
          "level_accuracy": 0.5057471264367817,
          "rmse": 1.4739110533215525,
          "mae": 1.1839080459770115,
          "quadratic_weighted_kappa": 0.2445904350622502
        },
        "surprise": {
          "perfect_accuracy": 0.1724137931034483,
          "off_by_one_accuracy": 0.6551724137931034,
          "level_accuracy": 0.5517241379310345,
          "rmse": 1.5974116995724785,
          "mae": 1.3103448275862069,
          "quadratic_weighted_kappa": 0.08168505135032322
        }
      }
    },
    {
      "story_classification": {
        "accuracy": 0.6896551724137931,
        "micro_precision": 0.6896551724137931,
        "micro_recall": 0.6896551724137931,
        "micro_f1": 0.6896551724137931,
        "macro_precision": 0.7444444444444445,
        "macro_recall": 0.7115384615384616,
        "macro_f1": 0.6836363636363637,
        "weighted_precision": 0.7593869731800765,
        "weighted_recall": 0.6896551724137931,
        "weighted_f1": 0.6791222570532917,
        "mcc": 0.4547940268270977
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.25287356321839083,
          "off_by_one_accuracy": 0.6781609195402298,
          "level_accuracy": 0.5747126436781609,
          "rmse": 1.4932799662056893,
          "mae": 1.1724137931034482,
          "quadratic_weighted_kappa": 0.16121657886889973
        },
        "curiosity": {
          "perfect_accuracy": 0.22988505747126436,
          "off_by_one_accuracy": 0.7241379310344828,
          "level_accuracy": 0.5517241379310345,
          "rmse": 1.5124008469447114,
          "mae": 1.1839080459770115,
          "quadratic_weighted_kappa": 0.24215364412344065
        },
        "surprise": {
          "perfect_accuracy": 0.22988505747126436,
          "off_by_one_accuracy": 0.6551724137931034,
          "level_accuracy": 0.5862068965517241,
          "rmse": 1.568365427743971,
          "mae": 1.2413793103448276,
          "quadratic_weighted_kappa": 0.14020504294818514
        }
      }
    },
    {
      "story_classification": {
        "accuracy": 0.7209302325581395,
        "micro_precision": 0.7209302325581395,
        "micro_recall": 0.7209302325581395,
        "micro_f1": 0.7209302325581395,
        "macro_precision": 0.7523148148148149,
        "macro_recall": 0.7390350877192983,
        "macro_f1": 0.7195652173913044,
        "weighted_precision": 0.7665805340223945,
        "weighted_recall": 0.7209302325581395,
        "weighted_f1": 0.7172901921132457,
        "mcc": 0.4911704139792532
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.27906976744186046,
          "off_by_one_accuracy": 0.7093023255813954,
          "level_accuracy": 0.5813953488372093,
          "rmse": 1.4142135623730951,
          "mae": 1.0930232558139534,
          "quadratic_weighted_kappa": 0.25171995143666537
        },
        "curiosity": {
          "perfect_accuracy": 0.18604651162790697,
          "off_by_one_accuracy": 0.7674418604651163,
          "level_accuracy": 0.5,
          "rmse": 1.2984785372226633,
          "mae": 1.0813953488372092,
          "quadratic_weighted_kappa": 0.38138704236531407
        },
        "surprise": {
          "perfect_accuracy": 0.26744186046511625,
          "off_by_one_accuracy": 0.7325581395348837,
          "level_accuracy": 0.5581395348837209,
          "rmse": 1.4142135623730951,
          "mae": 1.0930232558139534,
          "quadratic_weighted_kappa": 0.2678677489606017
        }
      }
    }
  ]
}