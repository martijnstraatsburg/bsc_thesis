{
  "average_metrics": {
    "story_classification": {
      "accuracy": {
        "mean": 0.689093825180433,
        "std": 0.052198962055181146
      },
      "micro_precision": {
        "mean": 0.689093825180433,
        "std": 0.052198962055181146
      },
      "micro_recall": {
        "mean": 0.689093825180433,
        "std": 0.052198962055181146
      },
      "micro_f1": {
        "mean": 0.689093825180433,
        "std": 0.05219896205518112
      },
      "macro_precision": {
        "mean": 0.7280623577317215,
        "std": 0.05853771202325496
      },
      "macro_recall": {
        "mean": 0.7100100086550254,
        "std": 0.05718728622209327
      },
      "macro_f1": {
        "mean": 0.6857181860283612,
        "std": 0.053092662978670206
      },
      "weighted_precision": {
        "mean": 0.7441898313029489,
        "std": 0.06434961581841368
      },
      "weighted_recall": {
        "mean": 0.689093825180433,
        "std": 0.052198962055181146
      },
      "weighted_f1": {
        "mean": 0.6827877683963588,
        "std": 0.05365449598107219
      },
      "mcc": {
        "mean": 0.4375482833890777,
        "std": 0.11521650975677328
      }
    },
    "rating_metrics": {
      "suspense": {
        "perfect_accuracy": {
          "mean": 0.24883720930232558,
          "std": 0.02668780286246827
        },
        "off_by_one_accuracy": {
          "mean": 0.7027265437048917,
          "std": 0.022776227838145733
        },
        "level_accuracy": {
          "mean": 0.39165998396150764,
          "std": 0.05829805070368946
        },
        "rmse": {
          "mean": 1.4013558666209593,
          "std": 0.04273054819300731
        },
        "mae": {
          "mean": 1.1129644480085539,
          "std": 0.027886075862136407
        },
        "quadratic_weighted_kappa": {
          "mean": 0.010014762633920982,
          "std": 0.03940920760033243
        }
      },
      "curiosity": {
        "perfect_accuracy": {
          "mean": 0.3272654370489174,
          "std": 0.07781844623422017
        },
        "off_by_one_accuracy": {
          "mean": 0.8111200213846566,
          "std": 0.029277558990376772
        },
        "level_accuracy": {
          "mean": 0.5209035017375034,
          "std": 0.058600494838548824
        },
        "rmse": {
          "mean": 1.2286676596745003,
          "std": 0.07840259080757377
        },
        "mae": {
          "mean": 0.9168938786420743,
          "std": 0.10080555033344887
        },
        "quadratic_weighted_kappa": {
          "mean": 0.28660985197624245,
          "std": 0.0729934437717065
        }
      },
      "surprise": {
        "perfect_accuracy": {
          "mean": 0.2765570703020582,
          "std": 0.02257233813114942
        },
        "off_by_one_accuracy": {
          "mean": 0.7488639401229619,
          "std": 0.013062047710468094
        },
        "level_accuracy": {
          "mean": 0.45851376637262764,
          "std": 0.028266954941147605
        },
        "rmse": {
          "mean": 1.3339864682322662,
          "std": 0.03625057485136183
        },
        "mae": {
          "mean": 1.0344560278000534,
          "std": 0.02625783083336732
        },
        "quadratic_weighted_kappa": {
          "mean": 0.1254026836333757,
          "std": 0.043042000716789194
        }
      }
    }
  },
  "fold_metrics": [
    {
      "story_classification": {
        "accuracy": 0.735632183908046,
        "micro_precision": 0.735632183908046,
        "micro_recall": 0.735632183908046,
        "micro_f1": 0.735632183908046,
        "macro_precision": 0.7752525252525253,
        "macro_recall": 0.7695054945054944,
        "macro_f1": 0.7354923992068738,
        "weighted_precision": 0.8073261349123418,
        "weighted_recall": 0.735632183908046,
        "weighted_f1": 0.73430422924691,
        "mcc": 0.5447277042048364
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.26436781609195403,
          "off_by_one_accuracy": 0.6896551724137931,
          "level_accuracy": 0.4482758620689655,
          "rmse": 1.4582307024641867,
          "mae": 1.1379310344827587,
          "quadratic_weighted_kappa": 0.013847190735861603
        },
        "curiosity": {
          "perfect_accuracy": 0.367816091954023,
          "off_by_one_accuracy": 0.7701149425287356,
          "level_accuracy": 0.5287356321839081,
          "rmse": 1.2909944487358056,
          "mae": 0.9310344827586207,
          "quadratic_weighted_kappa": 0.24111171268724063
        },
        "surprise": {
          "perfect_accuracy": 0.2988505747126437,
          "off_by_one_accuracy": 0.7241379310344828,
          "level_accuracy": 0.5057471264367817,
          "rmse": 1.3978637231524922,
          "mae": 1.0574712643678161,
          "quadratic_weighted_kappa": 0.09074142382884542
        }
      }
    },
    {
      "story_classification": {
        "accuracy": 0.6781609195402298,
        "micro_precision": 0.6781609195402298,
        "micro_recall": 0.6781609195402298,
        "micro_f1": 0.6781609195402298,
        "macro_precision": 0.7259259259259259,
        "macro_recall": 0.6946808510638298,
        "macro_f1": 0.6708108108108108,
        "weighted_precision": 0.7360578969774372,
        "weighted_recall": 0.6781609195402298,
        "weighted_f1": 0.6668530599565083,
        "mcc": 0.41944464014524263
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.28735632183908044,
          "off_by_one_accuracy": 0.7011494252873564,
          "level_accuracy": 0.45977011494252873,
          "rmse": 1.3771534860493695,
          "mae": 1.0689655172413792,
          "quadratic_weighted_kappa": 0.0350877192982455
        },
        "curiosity": {
          "perfect_accuracy": 0.1724137931034483,
          "off_by_one_accuracy": 0.8045977011494253,
          "level_accuracy": 0.41379310344827586,
          "rmse": 1.3476245597431757,
          "mae": 1.103448275862069,
          "quadratic_weighted_kappa": 0.17232658959537583
        },
        "surprise": {
          "perfect_accuracy": 0.27586206896551724,
          "off_by_one_accuracy": 0.7586206896551724,
          "level_accuracy": 0.47126436781609193,
          "rmse": 1.317433939105884,
          "mae": 1.0229885057471264,
          "quadratic_weighted_kappa": 0.16095037363479603
        }
      }
    },
    {
      "story_classification": {
        "accuracy": 0.6091954022988506,
        "micro_precision": 0.6091954022988506,
        "micro_recall": 0.6091954022988506,
        "micro_f1": 0.6091954022988506,
        "macro_precision": 0.6265260821309656,
        "macro_recall": 0.6217948717948718,
        "macro_f1": 0.6079003181336161,
        "weighted_precision": 0.6347353515251253,
        "weighted_recall": 0.6091954022988506,
        "weighted_f1": 0.6055691666361941,
        "mcc": 0.24827587842437188
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.20689655172413793,
          "off_by_one_accuracy": 0.7471264367816092,
          "level_accuracy": 0.2988505747126437,
          "rmse": 1.3347693416475743,
          "mae": 1.0919540229885059,
          "quadratic_weighted_kappa": 0.052687038988408874
        },
        "curiosity": {
          "perfect_accuracy": 0.3563218390804598,
          "off_by_one_accuracy": 0.7931034482758621,
          "level_accuracy": 0.5402298850574713,
          "rmse": 1.1986582537134602,
          "mae": 0.8850574712643678,
          "quadratic_weighted_kappa": 0.29515846782033817
        },
        "surprise": {
          "perfect_accuracy": 0.2413793103448276,
          "off_by_one_accuracy": 0.7471264367816092,
          "level_accuracy": 0.42528735632183906,
          "rmse": 1.3217891045025334,
          "mae": 1.0574712643678161,
          "quadratic_weighted_kappa": 0.06425134446645908
        }
      }
    },
    {
      "story_classification": {
        "accuracy": 0.6666666666666666,
        "micro_precision": 0.6666666666666666,
        "micro_recall": 0.6666666666666666,
        "micro_f1": 0.6666666666666666,
        "macro_precision": 0.7175925925925926,
        "macro_recall": 0.688301282051282,
        "macro_f1": 0.6602020202020202,
        "weighted_precision": 0.7314814814814814,
        "weighted_recall": 0.6666666666666666,
        "weighted_f1": 0.6553535353535354,
        "mcc": 0.40483559206199987
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.2413793103448276,
          "off_by_one_accuracy": 0.6896551724137931,
          "level_accuracy": 0.3793103448275862,
          "rmse": 1.4060624076664854,
          "mae": 1.1264367816091954,
          "quadratic_weighted_kappa": -0.06278409090909087
        },
        "curiosity": {
          "perfect_accuracy": 0.3793103448275862,
          "off_by_one_accuracy": 0.8505747126436781,
          "level_accuracy": 0.5287356321839081,
          "rmse": 1.1396712572986316,
          "mae": 0.8160919540229885,
          "quadratic_weighted_kappa": 0.3628232549095859
        },
        "surprise": {
          "perfect_accuracy": 0.26436781609195403,
          "off_by_one_accuracy": 0.7586206896551724,
          "level_accuracy": 0.4367816091954023,
          "rmse": 1.3433531557819876,
          "mae": 1.0459770114942528,
          "quadratic_weighted_kappa": 0.13060912736299413
        }
      }
    },
    {
      "story_classification": {
        "accuracy": 0.7558139534883721,
        "micro_precision": 0.7558139534883721,
        "micro_recall": 0.7558139534883721,
        "micro_f1": 0.755813953488372,
        "macro_precision": 0.7950146627565982,
        "macro_recall": 0.7757675438596491,
        "macro_f1": 0.7541853817884849,
        "weighted_precision": 0.811348291618359,
        "weighted_recall": 0.7558139534883721,
        "weighted_f1": 0.7518588507886464,
        "mcc": 0.5704576021089378
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.2441860465116279,
          "off_by_one_accuracy": 0.686046511627907,
          "level_accuracy": 0.37209302325581395,
          "rmse": 1.4305633952771812,
          "mae": 1.1395348837209303,
          "quadratic_weighted_kappa": 0.011235955056179803
        },
        "curiosity": {
          "perfect_accuracy": 0.36046511627906974,
          "off_by_one_accuracy": 0.8372093023255814,
          "level_accuracy": 0.5930232558139535,
          "rmse": 1.1663897788814295,
          "mae": 0.8488372093023255,
          "quadratic_weighted_kappa": 0.3616292348686716
        },
        "surprise": {
          "perfect_accuracy": 0.3023255813953488,
          "off_by_one_accuracy": 0.7558139534883721,
          "level_accuracy": 0.45348837209302323,
          "rmse": 1.289492418618434,
          "mae": 0.9883720930232558,
          "quadratic_weighted_kappa": 0.18046114887378384
        }
      }
    }
  ]
}