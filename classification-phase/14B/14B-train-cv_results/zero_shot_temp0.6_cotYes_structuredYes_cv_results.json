{
  "average_metrics": {
    "story_classification": {
      "accuracy": {
        "mean": 0.4999732691793638,
        "std": 0.019249606159889986
      },
      "micro_precision": {
        "mean": 0.4999732691793638,
        "std": 0.019249606159889986
      },
      "micro_recall": {
        "mean": 0.4999732691793638,
        "std": 0.019249606159889986
      },
      "micro_f1": {
        "mean": 0.4999732691793638,
        "std": 0.019249606159889986
      },
      "macro_precision": {
        "mean": 0.711708706830658,
        "std": 0.043985100650895746
      },
      "macro_recall": {
        "mean": 0.5525095471903982,
        "std": 0.007747933029024414
      },
      "macro_f1": {
        "mean": 0.41964964459216303,
        "std": 0.019269055927349805
      },
      "weighted_precision": {
        "mean": 0.7416811891247013,
        "std": 0.050041849064877186
      },
      "weighted_recall": {
        "mean": 0.4999732691793638,
        "std": 0.019249606159889986
      },
      "weighted_f1": {
        "mean": 0.39410881700236733,
        "std": 0.025420534278257623
      },
      "mcc": {
        "mean": 0.20957340587174883,
        "std": 0.03254128164660418
      }
    },
    "rating_metrics": {
      "suspense": {
        "perfect_accuracy": {
          "mean": 0.20513231756214917,
          "std": 0.017707509265432502
        },
        "off_by_one_accuracy": {
          "mean": 0.5967923015236568,
          "std": 0.005937853355536958
        },
        "level_accuracy": {
          "mean": 0.5760759155306068,
          "std": 0.009571873538555523
        },
        "rmse": {
          "mean": 1.7043776138412607,
          "std": 0.0523637674229234
        },
        "mae": {
          "mean": 1.3777866880513232,
          "std": 0.03646710996881646
        },
        "quadratic_weighted_kappa": {
          "mean": 0.04096916266945201,
          "std": 0.02318164115429479
        }
      },
      "curiosity": {
        "perfect_accuracy": {
          "mean": 0.09443998930767175,
          "std": 0.03110250158911371
        },
        "off_by_one_accuracy": {
          "mean": 0.555252606255012,
          "std": 0.02221419652500809
        },
        "level_accuracy": {
          "mean": 0.5046244319700615,
          "std": 0.018685895236833568
        },
        "rmse": {
          "mean": 1.8590378482856686,
          "std": 0.0566901858454937
        },
        "mae": {
          "mean": 1.5923549852980488,
          "std": 0.06048777768854118
        },
        "quadratic_weighted_kappa": {
          "mean": 0.05364873215086907,
          "std": 0.018561913544378434
        }
      },
      "surprise": {
        "perfect_accuracy": {
          "mean": 0.16597166533012564,
          "std": 0.03179534201332804
        },
        "off_by_one_accuracy": {
          "mean": 0.5967388398823845,
          "std": 0.011468238591000856
        },
        "level_accuracy": {
          "mean": 0.5668003207698475,
          "std": 0.012282921157556487
        },
        "rmse": {
          "mean": 1.7473344738455776,
          "std": 0.04436123852267901
        },
        "mae": {
          "mean": 1.437717187917669,
          "std": 0.03623539968792679
        },
        "quadratic_weighted_kappa": {
          "mean": 0.04268097825774499,
          "std": 0.03143825866127399
        }
      }
    }
  },
  "fold_metrics": [
    {
      "story_classification": {
        "accuracy": 0.47126436781609193,
        "micro_precision": 0.47126436781609193,
        "micro_recall": 0.47126436781609193,
        "micro_f1": 0.47126436781609193,
        "macro_precision": 0.7160493827160493,
        "macro_recall": 0.5576923076923077,
        "macro_f1": 0.4051724137931034,
        "weighted_precision": 0.7715339860933731,
        "weighted_recall": 0.47126436781609193,
        "weighted_f1": 0.36642885453824814,
        "mcc": 0.22328804235236124
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.21839080459770116,
          "off_by_one_accuracy": 0.5862068965517241,
          "level_accuracy": 0.5747126436781609,
          "rmse": 1.7551238617587483,
          "mae": 1.4022988505747127,
          "quadratic_weighted_kappa": 0.01686625063248437
        },
        "curiosity": {
          "perfect_accuracy": 0.13793103448275862,
          "off_by_one_accuracy": 0.5747126436781609,
          "level_accuracy": 0.5287356321839081,
          "rmse": 1.8131068147903147,
          "mae": 1.5172413793103448,
          "quadratic_weighted_kappa": 0.08353591160220986
        },
        "surprise": {
          "perfect_accuracy": 0.20689655172413793,
          "off_by_one_accuracy": 0.5862068965517241,
          "level_accuracy": 0.5747126436781609,
          "rmse": 1.8067561584486032,
          "mae": 1.4482758620689655,
          "quadratic_weighted_kappa": -0.006928029994294871
        }
      }
    },
    {
      "story_classification": {
        "accuracy": 0.5287356321839081,
        "micro_precision": 0.5287356321839081,
        "micro_recall": 0.5287356321839081,
        "micro_f1": 0.5287356321839081,
        "macro_precision": 0.7469135802469136,
        "macro_recall": 0.5638297872340425,
        "macro_f1": 0.4437860595665055,
        "weighted_precision": 0.7672768553994608,
        "weighted_recall": 0.5287356321839081,
        "weighted_f1": 0.4262964416746874,
        "mcc": 0.2510811923849032
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.19540229885057472,
          "off_by_one_accuracy": 0.5977011494252874,
          "level_accuracy": 0.5632183908045977,
          "rmse": 1.6609095970747993,
          "mae": 1.3563218390804597,
          "quadratic_weighted_kappa": 0.03681151397730409
        },
        "curiosity": {
          "perfect_accuracy": 0.04597701149425287,
          "off_by_one_accuracy": 0.5517241379310345,
          "level_accuracy": 0.5057471264367817,
          "rmse": 1.8569533817705186,
          "mae": 1.632183908045977,
          "quadratic_weighted_kappa": 0.047375720855537096
        },
        "surprise": {
          "perfect_accuracy": 0.12643678160919541,
          "off_by_one_accuracy": 0.5977011494252874,
          "level_accuracy": 0.5517241379310345,
          "rmse": 1.7811271131234425,
          "mae": 1.4942528735632183,
          "quadratic_weighted_kappa": 0.019598236158745896
        }
      }
    },
    {
      "story_classification": {
        "accuracy": 0.5057471264367817,
        "micro_precision": 0.5057471264367817,
        "micro_recall": 0.5057471264367817,
        "micro_f1": 0.5057471264367817,
        "macro_precision": 0.6260683760683761,
        "macro_recall": 0.547275641025641,
        "macro_f1": 0.4390463337831759,
        "weighted_precision": 0.6417624521072797,
        "weighted_recall": 0.5057471264367817,
        "weighted_f1": 0.41903609598709424,
        "mcc": 0.15440159703440975
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.19540229885057472,
          "off_by_one_accuracy": 0.5977011494252874,
          "level_accuracy": 0.5747126436781609,
          "rmse": 1.7778975055468988,
          "mae": 1.4367816091954022,
          "quadratic_weighted_kappa": 0.021031957117721767
        },
        "curiosity": {
          "perfect_accuracy": 0.09195402298850575,
          "off_by_one_accuracy": 0.5862068965517241,
          "level_accuracy": 0.47126436781609193,
          "rmse": 1.7843508752420336,
          "mae": 1.528735632183908,
          "quadratic_weighted_kappa": 0.06269690015946494
        },
        "surprise": {
          "perfect_accuracy": 0.16091954022988506,
          "off_by_one_accuracy": 0.6091954022988506,
          "level_accuracy": 0.5862068965517241,
          "rmse": 1.6781215551988444,
          "mae": 1.3908045977011494,
          "quadratic_weighted_kappa": 0.07531126632250207
        }
      }
    },
    {
      "story_classification": {
        "accuracy": 0.5057471264367817,
        "micro_precision": 0.5057471264367817,
        "micro_recall": 0.5057471264367817,
        "micro_f1": 0.5057471264367817,
        "macro_precision": 0.7378048780487805,
        "macro_recall": 0.5520833333333334,
        "macro_f1": 0.41665367222828625,
        "weighted_precision": 0.764928511354079,
        "weighted_recall": 0.5057471264367817,
        "weighted_f1": 0.39307011082015514,
        "mcc": 0.2225818566883412
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.1839080459770115,
          "off_by_one_accuracy": 0.5977011494252874,
          "level_accuracy": 0.5747126436781609,
          "rmse": 1.6470105276411926,
          "mae": 1.3563218390804597,
          "quadratic_weighted_kappa": 0.08134228187919457
        },
        "curiosity": {
          "perfect_accuracy": 0.11494252873563218,
          "off_by_one_accuracy": 0.5287356321839081,
          "level_accuracy": 0.5057471264367817,
          "rmse": 1.899788251963584,
          "mae": 1.6091954022988506,
          "quadratic_weighted_kappa": 0.027967549103330325
        },
        "surprise": {
          "perfect_accuracy": 0.13793103448275862,
          "off_by_one_accuracy": 0.6091954022988506,
          "level_accuracy": 0.5632183908045977,
          "rmse": 1.7419766814227708,
          "mae": 1.4482758620689655,
          "quadratic_weighted_kappa": 0.06906614785992227
        }
      }
    },
    {
      "story_classification": {
        "accuracy": 0.4883720930232558,
        "micro_precision": 0.4883720930232558,
        "micro_recall": 0.4883720930232558,
        "micro_f1": 0.4883720930232558,
        "macro_precision": 0.7317073170731707,
        "macro_recall": 0.5416666666666666,
        "macro_f1": 0.39358974358974363,
        "weighted_precision": 0.7629041406693137,
        "weighted_recall": 0.4883720930232558,
        "weighted_f1": 0.3657125819916518,
        "mcc": 0.19651434089872877
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.23255813953488372,
          "off_by_one_accuracy": 0.6046511627906976,
          "level_accuracy": 0.5930232558139535,
          "rmse": 1.680946577184664,
          "mae": 1.3372093023255813,
          "quadratic_weighted_kappa": 0.04879380974055525
        },
        "curiosity": {
          "perfect_accuracy": 0.08139534883720931,
          "off_by_one_accuracy": 0.5348837209302325,
          "level_accuracy": 0.5116279069767442,
          "rmse": 1.9409899176618914,
          "mae": 1.6744186046511629,
          "quadratic_weighted_kappa": 0.04666757903380314
        },
        "surprise": {
          "perfect_accuracy": 0.19767441860465115,
          "off_by_one_accuracy": 0.5813953488372093,
          "level_accuracy": 0.5581395348837209,
          "rmse": 1.7286908610342266,
          "mae": 1.4069767441860466,
          "quadratic_weighted_kappa": 0.05635727094184961
        }
      }
    }
  ]
}