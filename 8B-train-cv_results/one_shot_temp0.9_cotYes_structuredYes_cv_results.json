{
  "average_metrics": {
    "story_classification": {
      "accuracy": {
        "mean": 0.7256883186313818,
        "std": 0.04227000713143399
      },
      "micro_precision": {
        "mean": 0.7256883186313818,
        "std": 0.04227000713143399
      },
      "micro_recall": {
        "mean": 0.7256883186313818,
        "std": 0.04227000713143399
      },
      "micro_f1": {
        "mean": 0.7256883186313818,
        "std": 0.04227000713143399
      },
      "macro_precision": {
        "mean": 0.7331098053660022,
        "std": 0.05662641200667062
      },
      "macro_recall": {
        "mean": 0.7080859751222705,
        "std": 0.04887385122170001
      },
      "macro_f1": {
        "mean": 0.7090994966744134,
        "std": 0.04816188693194847
      },
      "mcc": {
        "mean": 0.44036676700951816,
        "std": 0.10492355188609022
      }
    },
    "rating_metrics": {
      "suspense": {
        "perfect_accuracy": {
          "mean": 0.37086340550654906,
          "std": 0.050926135084483744
        },
        "off_by_one_accuracy": {
          "mean": 0.8363271852445869,
          "std": 0.04045527564012607
        },
        "rmse": {
          "mean": 1.1305521502142,
          "std": 0.08932267201480648
        },
        "mae": {
          "mean": 0.8250735097567494,
          "std": 0.09443036103049013
        },
        "quadratic_weighted_kappa": {
          "mean": 0.3796658703568782,
          "std": 0.08589213440315423
        }
      },
      "curiosity": {
        "perfect_accuracy": {
          "mean": 0.2809943865276664,
          "std": 0.06725793194489903
        },
        "off_by_one_accuracy": {
          "mean": 0.7927024859663192,
          "std": 0.04014040158155288
        },
        "rmse": {
          "mean": 1.2256352851367065,
          "std": 0.10738276777133147
        },
        "mae": {
          "mean": 0.9608660785886126,
          "std": 0.11298743189901844
        },
        "quadratic_weighted_kappa": {
          "mean": 0.36299062732947784,
          "std": 0.08712572525057655
        }
      },
      "surprise": {
        "perfect_accuracy": {
          "mean": 0.36856455493183643,
          "std": 0.07580634516180719
        },
        "off_by_one_accuracy": {
          "mean": 0.792675755145683,
          "std": 0.045189667845495884
        },
        "rmse": {
          "mean": 1.2576000030746743,
          "std": 0.12321149357492339
        },
        "mae": {
          "mean": 0.9055867415129644,
          "std": 0.12427588303891296
        },
        "quadratic_weighted_kappa": {
          "mean": 0.2870609370526348,
          "std": 0.1365696188334096
        }
      }
    }
  },
  "fold_metrics": [
    {
      "story_classification": {
        "accuracy": 0.6781609195402298,
        "micro_precision": 0.6781609195402298,
        "micro_recall": 0.6781609195402298,
        "micro_f1": 0.6781609195402298,
        "macro_precision": 0.6700549450549451,
        "macro_recall": 0.6672972972972973,
        "macro_f1": 0.6683006535947712,
        "mcc": 0.33734097112407024
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.3448275862068966,
          "off_by_one_accuracy": 0.8505747126436781,
          "rmse": 1.1547005383792515,
          "mae": 0.8505747126436781,
          "quadratic_weighted_kappa": 0.3661600301469665
        },
        "curiosity": {
          "perfect_accuracy": 0.22988505747126436,
          "off_by_one_accuracy": 0.7241379310344828,
          "rmse": 1.3937462952891333,
          "mae": 1.1149425287356323,
          "quadratic_weighted_kappa": 0.2357710899734915
        },
        "surprise": {
          "perfect_accuracy": 0.3563218390804598,
          "off_by_one_accuracy": 0.7126436781609196,
          "rmse": 1.4019690586383533,
          "mae": 1.0229885057471264,
          "quadratic_weighted_kappa": 0.23365785813630036
        }
      }
    },
    {
      "story_classification": {
        "accuracy": 0.735632183908046,
        "micro_precision": 0.735632183908046,
        "micro_recall": 0.735632183908046,
        "micro_f1": 0.735632183908046,
        "macro_precision": 0.7452711223203027,
        "macro_recall": 0.7102702702702703,
        "macro_f1": 0.7138567138567138,
        "mcc": 0.4541947827960271
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.3218390804597701,
          "off_by_one_accuracy": 0.7816091954022989,
          "rmse": 1.2270888828592579,
          "mae": 0.9310344827586207,
          "quadratic_weighted_kappa": 0.24777242426242496
        },
        "curiosity": {
          "perfect_accuracy": 0.21839080459770116,
          "off_by_one_accuracy": 0.7816091954022989,
          "rmse": 1.2909944487358056,
          "mae": 1.0459770114942528,
          "quadratic_weighted_kappa": 0.2964698009034632
        },
        "surprise": {
          "perfect_accuracy": 0.3333333333333333,
          "off_by_one_accuracy": 0.7931034482758621,
          "rmse": 1.3603583030377249,
          "mae": 0.9770114942528736,
          "quadratic_weighted_kappa": 0.07709033405811416
        }
      }
    },
    {
      "story_classification": {
        "accuracy": 0.7701149425287356,
        "micro_precision": 0.7701149425287356,
        "micro_recall": 0.7701149425287356,
        "micro_f1": 0.7701149425287356,
        "macro_precision": 0.7929782082324455,
        "macro_recall": 0.7574468085106383,
        "macro_f1": 0.7586015538290787,
        "mcc": 0.549276996323743
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.41379310344827586,
          "off_by_one_accuracy": 0.8505747126436781,
          "rmse": 1.072112534837795,
          "mae": 0.7586206896551724,
          "quadratic_weighted_kappa": 0.43292921392256556
        },
        "curiosity": {
          "perfect_accuracy": 0.3448275862068966,
          "off_by_one_accuracy": 0.8390804597701149,
          "rmse": 1.093344547181068,
          "mae": 0.8275862068965517,
          "quadratic_weighted_kappa": 0.48131162577390496
        },
        "surprise": {
          "perfect_accuracy": 0.3103448275862069,
          "off_by_one_accuracy": 0.7931034482758621,
          "rmse": 1.217685764345488,
          "mae": 0.9310344827586207,
          "quadratic_weighted_kappa": 0.3228142158933204
        }
      }
    },
    {
      "story_classification": {
        "accuracy": 0.7701149425287356,
        "micro_precision": 0.7701149425287356,
        "micro_recall": 0.7701149425287356,
        "micro_f1": 0.7701149425287356,
        "macro_precision": 0.7929824561403509,
        "macro_recall": 0.765079365079365,
        "macro_f1": 0.7630718954248366,
        "mcc": 0.5573638074115582
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.4482758620689655,
          "off_by_one_accuracy": 0.896551724137931,
          "rmse": 0.988438917815802,
          "mae": 0.6781609195402298,
          "quadratic_weighted_kappa": 0.503924330851278
        },
        "curiosity": {
          "perfect_accuracy": 0.3793103448275862,
          "off_by_one_accuracy": 0.7931034482758621,
          "rmse": 1.1396712572986316,
          "mae": 0.8390804597701149,
          "quadratic_weighted_kappa": 0.41652323580034434
        },
        "surprise": {
          "perfect_accuracy": 0.5172413793103449,
          "off_by_one_accuracy": 0.8505747126436781,
          "rmse": 1.0504514628777804,
          "mae": 0.6666666666666666,
          "quadratic_weighted_kappa": 0.4986794717887154
        }
      }
    },
    {
      "story_classification": {
        "accuracy": 0.6744186046511628,
        "micro_precision": 0.6744186046511628,
        "micro_recall": 0.6744186046511628,
        "micro_f1": 0.6744186046511628,
        "macro_precision": 0.6642622950819672,
        "macro_recall": 0.6403361344537815,
        "macro_f1": 0.6416666666666666,
        "mcc": 0.30365727739219217
      },
      "rating_metrics": {
        "suspense": {
          "perfect_accuracy": 0.32558139534883723,
          "off_by_one_accuracy": 0.8023255813953488,
          "rmse": 1.2104198771788934,
          "mae": 0.9069767441860465,
          "quadratic_weighted_kappa": 0.347543352601156
        },
        "curiosity": {
          "perfect_accuracy": 0.23255813953488372,
          "off_by_one_accuracy": 0.8255813953488372,
          "rmse": 1.2104198771788934,
          "mae": 0.9767441860465116,
          "quadratic_weighted_kappa": 0.3848773841961852
        },
        "surprise": {
          "perfect_accuracy": 0.32558139534883723,
          "off_by_one_accuracy": 0.813953488372093,
          "rmse": 1.2575354264740255,
          "mae": 0.9302325581395349,
          "quadratic_weighted_kappa": 0.3030628053867238
        }
      }
    }
  ]
}