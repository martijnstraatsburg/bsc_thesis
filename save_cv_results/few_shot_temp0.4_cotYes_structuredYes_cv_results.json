{
  "average_metrics": {
    "story_classification": {
      "accuracy": {
        "mean": 0.8229018492176386,
        "std": 0.04205814133024014
      },
      "precision": {
        "mean": 0.8024258858354969,
        "std": 0.054502319284398186
      },
      "recall": {
        "mean": 0.8133383429436061,
        "std": 0.052403967747972645
      },
      "f1": {
        "mean": 0.8058951748723123,
        "std": 0.05405587913402345
      }
    },
    "rating_metrics": {
      "suspense": {
        "accuracy": {
          "mean": 0.381792318634424,
          "std": 0.06050186467152871
        },
        "tolerance1_accuracy": {
          "mean": 0.9354196301564721,
          "std": 0.013482783056697303
        },
        "tolerance2_accuracy": {
          "mean": 0.9947368421052631,
          "std": 0.010526315789473672
        },
        "rmse": {
          "mean": 0.9141438000259925,
          "std": 0.05104599145752613
        },
        "kappa": {
          "mean": 0.17393560964312632,
          "std": 0.09616506906611187
        },
        "weighted_kappa": {
          "mean": 0.47000625889013714,
          "std": 0.10395645784289648
        },
        "pearson_correlation": {
          "mean": 0.48464267541191414,
          "std": 0.10414019294099626
        },
        "pearson_p_value": {
          "mean": 0.009055612058318934,
          "std": 0.010009746416086016
        }
      },
      "curiosity": {
        "accuracy": {
          "mean": 0.4248933143669985,
          "std": 0.03322507235088863
        },
        "tolerance1_accuracy": {
          "mean": 0.8442389758179232,
          "std": 0.030541835780346926
        },
        "tolerance2_accuracy": {
          "mean": 0.9839260312944523,
          "std": 0.013126909809455826
        },
        "rmse": {
          "mean": 1.0578784195278585,
          "std": 0.06043882199555592
        },
        "kappa": {
          "mean": 0.2080888658694115,
          "std": 0.06212426940391597
        },
        "weighted_kappa": {
          "mean": 0.44830907641401624,
          "std": 0.07742293689577535
        },
        "pearson_correlation": {
          "mean": 0.4750073088707416,
          "std": 0.09379358088721611
        },
        "pearson_p_value": {
          "mean": 0.00787484521323991,
          "std": 0.006820179454886727
        }
      },
      "surprise": {
        "accuracy": {
          "mean": 0.33357041251778097,
          "std": 0.04846445961883887
        },
        "tolerance1_accuracy": {
          "mean": 0.8712660028449501,
          "std": 0.0567137810250862
        },
        "tolerance2_accuracy": {
          "mean": 0.9839260312944523,
          "std": 0.013126909809455826
        },
        "rmse": {
          "mean": 1.0568055246648211,
          "std": 0.12713577589734096
        },
        "kappa": {
          "mean": 0.08980877146192959,
          "std": 0.06424588306050344
        },
        "weighted_kappa": {
          "mean": 0.3656677681836456,
          "std": 0.09997498308972833
        },
        "pearson_correlation": {
          "mean": 0.37730637308404585,
          "std": 0.09420085845723264
        },
        "pearson_p_value": {
          "mean": 0.04688579711851732,
          "std": 0.053505916055778466
        }
      }
    }
  },
  "fold_metrics": [
    {
      "story_classification": {
        "accuracy": 0.7631578947368421,
        "precision": 0.7347826086956522,
        "recall": 0.7596153846153846,
        "f1": 0.7414965986394557
      },
      "rating_metrics": {
        "suspense": {
          "accuracy": 0.3684210526315789,
          "tolerance1_accuracy": 0.9473684210526315,
          "tolerance2_accuracy": 0.9736842105263158,
          "rmse": 0.9597148699373931,
          "kappa": 0.11197663096397281,
          "weighted_kappa": 0.3396226415094339,
          "pearson_correlation": 0.35816486499603695,
          "pearson_p_value": 0.027247792590536526
        },
        "curiosity": {
          "accuracy": 0.39473684210526316,
          "tolerance1_accuracy": 0.8157894736842105,
          "tolerance2_accuracy": 0.9736842105263158,
          "rmse": 1.1355499479153377,
          "kappa": 0.12424849699398799,
          "weighted_kappa": 0.3838517538054269,
          "pearson_correlation": 0.39370425585997504,
          "pearson_p_value": 0.01446273936007151
        },
        "surprise": {
          "accuracy": 0.2894736842105263,
          "tolerance1_accuracy": 0.8157894736842105,
          "tolerance2_accuracy": 0.9736842105263158,
          "rmse": 1.180989772227204,
          "kappa": 0.04999999999999993,
          "weighted_kappa": 0.23999999999999988,
          "pearson_correlation": 0.24304256663276283,
          "pearson_p_value": 0.14147621994024845
        }
      }
    },
    {
      "story_classification": {
        "accuracy": 0.7837837837837838,
        "precision": 0.7412587412587412,
        "recall": 0.7412587412587412,
        "f1": 0.7412587412587412
      },
      "rating_metrics": {
        "suspense": {
          "accuracy": 0.43243243243243246,
          "tolerance1_accuracy": 0.9459459459459459,
          "tolerance2_accuracy": 1.0,
          "rmse": 0.8542421961772491,
          "kappa": 0.2592945662535747,
          "weighted_kappa": 0.5732592909013242,
          "pearson_correlation": 0.5885014040385506,
          "pearson_p_value": 0.00012759215805473745
        },
        "curiosity": {
          "accuracy": 0.40540540540540543,
          "tolerance1_accuracy": 0.8378378378378378,
          "tolerance2_accuracy": 1.0,
          "rmse": 1.0397504898200727,
          "kappa": 0.1819095477386935,
          "weighted_kappa": 0.4117647058823529,
          "pearson_correlation": 0.4367628469911154,
          "pearson_p_value": 0.0068764205861088015
        },
        "surprise": {
          "accuracy": 0.3783783783783784,
          "tolerance1_accuracy": 0.8918918918918919,
          "tolerance2_accuracy": 0.972972972972973,
          "rmse": 1.0397504898200727,
          "kappa": 0.14985014985014966,
          "weighted_kappa": 0.42006269592476486,
          "pearson_correlation": 0.4306033537311654,
          "pearson_p_value": 0.007804423158008979
        }
      }
    },
    {
      "story_classification": {
        "accuracy": 0.8648648648648649,
        "precision": 0.8654970760233918,
        "recall": 0.8654970760233918,
        "f1": 0.8648648648648649
      },
      "rating_metrics": {
        "suspense": {
          "accuracy": 0.43243243243243246,
          "tolerance1_accuracy": 0.918918918918919,
          "tolerance2_accuracy": 1.0,
          "rmse": 0.9004503377814963,
          "kappa": 0.2961956521739131,
          "weighted_kappa": 0.6107994389901824,
          "pearson_correlation": 0.6263075896755874,
          "pearson_p_value": 3.3759802182661456e-05
        },
        "curiosity": {
          "accuracy": 0.43243243243243246,
          "tolerance1_accuracy": 0.8648648648648649,
          "tolerance2_accuracy": 0.972972972972973,
          "rmse": 1.0526671402243484,
          "kappa": 0.25144508670520227,
          "weighted_kappa": 0.5751890226827219,
          "pearson_correlation": 0.6400643785808191,
          "pearson_p_value": 1.9906609666879835e-05
        },
        "surprise": {
          "accuracy": 0.2972972972972973,
          "tolerance1_accuracy": 0.8378378378378378,
          "tolerance2_accuracy": 0.972972972972973,
          "rmse": 1.150792911137501,
          "kappa": 0.09159584513692176,
          "weighted_kappa": 0.40224200461589177,
          "pearson_correlation": 0.40637959315100125,
          "pearson_p_value": 0.012566488622072661
        }
      }
    },
    {
      "story_classification": {
        "accuracy": 0.8648648648648649,
        "precision": 0.8493788819875776,
        "recall": 0.8605769230769231,
        "f1": 0.8542159180457052
      },
      "rating_metrics": {
        "suspense": {
          "accuracy": 0.2702702702702703,
          "tolerance1_accuracy": 0.918918918918919,
          "tolerance2_accuracy": 1.0,
          "rmse": 0.9863939238321437,
          "kappa": 0.03197674418604657,
          "weighted_kappa": 0.4064171122994652,
          "pearson_correlation": 0.41312551386070373,
          "pearson_p_value": 0.011042415401929602
        },
        "curiosity": {
          "accuracy": 0.4864864864864865,
          "tolerance1_accuracy": 0.8108108108108109,
          "tolerance2_accuracy": 0.972972972972973,
          "rmse": 1.1028219331407116,
          "kappa": 0.302579365079365,
          "weighted_kappa": 0.3719351188230856,
          "pearson_correlation": 0.3904708281269844,
          "pearson_p_value": 0.016878755093316038
        },
        "surprise": {
          "accuracy": 0.2972972972972973,
          "tolerance1_accuracy": 0.8378378378378378,
          "tolerance2_accuracy": 1.0,
          "rmse": 1.0904995136125413,
          "kappa": -0.00838574423480054,
          "weighted_kappa": 0.2620126926563917,
          "pearson_correlation": 0.30003024846499127,
          "pearson_p_value": 0.07120078156607186
        }
      }
    },
    {
      "story_classification": {
        "accuracy": 0.8378378378378378,
        "precision": 0.8212121212121212,
        "recall": 0.8397435897435898,
        "f1": 0.8276397515527949
      },
      "rating_metrics": {
        "suspense": {
          "accuracy": 0.40540540540540543,
          "tolerance1_accuracy": 0.9459459459459459,
          "tolerance2_accuracy": 1.0,
          "rmse": 0.86991767240168,
          "kappa": 0.17023445463812448,
          "weighted_kappa": 0.41993281075028,
          "pearson_correlation": 0.4371140044886922,
          "pearson_p_value": 0.006826500338891145
        },
        "curiosity": {
          "accuracy": 0.40540540540540543,
          "tolerance1_accuracy": 0.8918918918918919,
          "tolerance2_accuracy": 1.0,
          "rmse": 0.9586025865388216,
          "kappa": 0.18026183282980868,
          "weighted_kappa": 0.49880478087649405,
          "pearson_correlation": 0.5140342347948141,
          "pearson_p_value": 0.001136404417036324
        },
        "surprise": {
          "accuracy": 0.40540540540540543,
          "tolerance1_accuracy": 0.972972972972973,
          "tolerance2_accuracy": 1.0,
          "rmse": 0.8219949365267865,
          "kappa": 0.1659836065573771,
          "weighted_kappa": 0.5040214477211796,
          "pearson_correlation": 0.5064761034403085,
          "pearson_p_value": 0.001381072306184632
        }
      }
    }
  ]
}