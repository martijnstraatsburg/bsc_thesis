{
  "average_metrics": {
    "story_classification": {
      "accuracy": {
        "mean": 0.7903234429296979,
        "std": 0.039400758107915435
      },
      "precision": {
        "mean": 0.787791804380127,
        "std": 0.03978400340369649
      },
      "recall": {
        "mean": 0.7882798422416696,
        "std": 0.037245598953492096
      },
      "f1": {
        "mean": 0.7872460848095129,
        "std": 0.03918551045760899
      }
    },
    "rating_metrics": {
      "suspense": {
        "accuracy": {
          "mean": 0.3756749532210639,
          "std": 0.05395855671294637
        },
        "tolerance1_accuracy": {
          "mean": 0.9032611601176157,
          "std": 0.017031204165466975
        },
        "tolerance2_accuracy": {
          "mean": 0.9930767174552259,
          "std": 0.0056530471947221226
        },
        "rmse": {
          "mean": 0.9729663532191235,
          "std": 0.0499445157493745
        },
        "kappa": {
          "mean": 0.14978216241861836,
          "std": 0.07073162031279778
        },
        "weighted_kappa": {
          "mean": 0.40563850645800964,
          "std": 0.04584943158432639
        },
        "pearson_correlation": {
          "mean": 0.45145469295615825,
          "std": 0.04797959457655523
        },
        "pearson_p_value": {
          "mean": 5.5043238963912183e-05,
          "std": 6.220640320553337e-05
        }
      },
      "curiosity": {
        "accuracy": {
          "mean": 0.4423683507083667,
          "std": 0.041806046350268175
        },
        "tolerance1_accuracy": {
          "mean": 0.880112269446672,
          "std": 0.04112447926281414
        },
        "tolerance2_accuracy": {
          "mean": 1.0,
          "std": 0.0
        },
        "rmse": {
          "mean": 0.9556226167292646,
          "std": 0.06387687646660821
        },
        "kappa": {
          "mean": 0.18860731425730787,
          "std": 0.057851142345398475
        },
        "weighted_kappa": {
          "mean": 0.4443183630939611,
          "std": 0.06034256756811337
        },
        "pearson_correlation": {
          "mean": 0.44937199214602386,
          "std": 0.06168637905872659
        },
        "pearson_p_value": {
          "mean": 0.00010025378518839934,
          "std": 0.00014046512941421444
        }
      },
      "surprise": {
        "accuracy": {
          "mean": 0.3778401496925955,
          "std": 0.05897039563549455
        },
        "tolerance1_accuracy": {
          "mean": 0.875568029938519,
          "std": 0.022326236854125756
        },
        "tolerance2_accuracy": {
          "mean": 0.9908045977011494,
          "std": 0.013404487114586881
        },
        "rmse": {
          "mean": 1.0170246981910407,
          "std": 0.08422312779470358
        },
        "kappa": {
          "mean": 0.13660590496324193,
          "std": 0.06999595284824252
        },
        "weighted_kappa": {
          "mean": 0.3257255585402971,
          "std": 0.09948535948550596
        },
        "pearson_correlation": {
          "mean": 0.3505979352070261,
          "std": 0.1069618896729467
        },
        "pearson_p_value": {
          "mean": 0.02888024717134865,
          "std": 0.05675162559585406
        }
      }
    }
  },
  "fold_metrics": [
    {
      "story_classification": {
        "accuracy": 0.735632183908046,
        "precision": 0.7333333333333334,
        "recall": 0.7383783783783784,
        "f1": 0.733377748167888
      },
      "rating_metrics": {
        "suspense": {
          "accuracy": 0.3448275862068966,
          "tolerance1_accuracy": 0.9195402298850575,
          "tolerance2_accuracy": 1.0,
          "rmse": 0.9468641529479986,
          "kappa": 0.07117437722419928,
          "weighted_kappa": 0.3544520547945206,
          "pearson_correlation": 0.3993039354371151,
          "pearson_p_value": 0.00012749734795727293
        },
        "curiosity": {
          "accuracy": 0.4827586206896552,
          "tolerance1_accuracy": 0.8160919540229885,
          "tolerance2_accuracy": 1.0,
          "rmse": 1.0339078862458586,
          "kappa": 0.23862310385064178,
          "weighted_kappa": 0.3729365263892118,
          "pearson_correlation": 0.3730167464023798,
          "pearson_p_value": 0.000373286671433834
        },
        "surprise": {
          "accuracy": 0.3448275862068966,
          "tolerance1_accuracy": 0.8850574712643678,
          "tolerance2_accuracy": 0.9885057471264368,
          "rmse": 1.0283342182227606,
          "kappa": 0.11509635974304078,
          "weighted_kappa": 0.3158974358974359,
          "pearson_correlation": 0.34442127839188175,
          "pearson_p_value": 0.0010883178847660463
        }
      }
    },
    {
      "story_classification": {
        "accuracy": 0.8275862068965517,
        "precision": 0.8243464052287581,
        "recall": 0.8218918918918918,
        "f1": 0.8230028482300284
      },
      "rating_metrics": {
        "suspense": {
          "accuracy": 0.4367816091954023,
          "tolerance1_accuracy": 0.9080459770114943,
          "tolerance2_accuracy": 0.9885057471264368,
          "rmse": 0.9468641529479986,
          "kappa": 0.22715736040609147,
          "weighted_kappa": 0.45979939500079614,
          "pearson_correlation": 0.49090612090454033,
          "pearson_p_value": 1.383480767968e-06
        },
        "curiosity": {
          "accuracy": 0.42528735632183906,
          "tolerance1_accuracy": 0.9195402298850575,
          "tolerance2_accuracy": 1.0,
          "rmse": 0.9033780792243016,
          "kappa": 0.1795548849490758,
          "weighted_kappa": 0.47648105771675564,
          "pearson_correlation": 0.4901010157205482,
          "pearson_p_value": 1.448185252048118e-06
        },
        "surprise": {
          "accuracy": 0.3333333333333333,
          "tolerance1_accuracy": 0.8390804597701149,
          "tolerance2_accuracy": 0.9655172413793104,
          "rmse": 1.1497126077675979,
          "kappa": 0.08520667150108774,
          "weighted_kappa": 0.14814814814814814,
          "pearson_correlation": 0.15857386757009376,
          "pearson_p_value": 0.14238010979437776
        }
      }
    },
    {
      "story_classification": {
        "accuracy": 0.7586206896551724,
        "precision": 0.7581923890063424,
        "recall": 0.7598404255319149,
        "f1": 0.758109360518999
      },
      "rating_metrics": {
        "suspense": {
          "accuracy": 0.28735632183908044,
          "tolerance1_accuracy": 0.8735632183908046,
          "tolerance2_accuracy": 0.9885057471264368,
          "rmse": 1.072112534837795,
          "kappa": 0.06289089645587209,
          "weighted_kappa": 0.3595406360424027,
          "pearson_correlation": 0.39803428594054474,
          "pearson_p_value": 0.00013456833224634708
        },
        "curiosity": {
          "accuracy": 0.4942528735632184,
          "tolerance1_accuracy": 0.9195402298850575,
          "tolerance2_accuracy": 1.0,
          "rmse": 0.864364759104401,
          "kappa": 0.2631376323387873,
          "weighted_kappa": 0.5447226471298607,
          "pearson_correlation": 0.546264878277632,
          "pearson_p_value": 4.427603950737742e-08
        },
        "surprise": {
          "accuracy": 0.3563218390804598,
          "tolerance1_accuracy": 0.8735632183908046,
          "tolerance2_accuracy": 1.0,
          "rmse": 1.0114289425101135,
          "kappa": 0.1341745157277413,
          "weighted_kappa": 0.3986953482954104,
          "pearson_correlation": 0.4278327391216083,
          "pearson_p_value": 3.574267323328079e-05
        }
      }
    },
    {
      "story_classification": {
        "accuracy": 0.8390804597701149,
        "precision": 0.8398936170212765,
        "recall": 0.8380952380952381,
        "f1": 0.8385471898197243
      },
      "rating_metrics": {
        "suspense": {
          "accuracy": 0.39080459770114945,
          "tolerance1_accuracy": 0.896551724137931,
          "tolerance2_accuracy": 1.0,
          "rmse": 0.9589266029707683,
          "kappa": 0.16723857684666787,
          "weighted_kappa": 0.3963573287077189,
          "pearson_correlation": 0.4513902413857598,
          "pearson_p_value": 1.1434122094257152e-05
        },
        "curiosity": {
          "accuracy": 0.3793103448275862,
          "tolerance1_accuracy": 0.896551724137931,
          "tolerance2_accuracy": 1.0,
          "rmse": 0.9649012813540153,
          "kappa": 0.09982755317110548,
          "weighted_kappa": 0.4058679706601468,
          "pearson_correlation": 0.4070759749354026,
          "pearson_p_value": 9.118678241380673e-05
        },
        "surprise": {
          "accuracy": 0.4942528735632184,
          "tolerance1_accuracy": 0.9080459770114943,
          "tolerance2_accuracy": 1.0,
          "rmse": 0.8840866447369844,
          "kappa": 0.270579268292683,
          "weighted_kappa": 0.43764258555133084,
          "pearson_correlation": 0.470289187417558,
          "pearson_p_value": 4.3027580748935764e-06
        }
      }
    },
    {
      "story_classification": {
        "accuracy": 0.7906976744186046,
        "precision": 0.7831932773109244,
        "recall": 0.7831932773109244,
        "f1": 0.7831932773109244
      },
      "rating_metrics": {
        "suspense": {
          "accuracy": 0.4186046511627907,
          "tolerance1_accuracy": 0.9186046511627907,
          "tolerance2_accuracy": 0.9883720930232558,
          "rmse": 0.9400643223910575,
          "kappa": 0.22044960116026102,
          "weighted_kappa": 0.45804311774461026,
          "pearson_correlation": 0.5176388811128312,
          "pearson_p_value": 3.329117537157359e-07
        },
        "curiosity": {
          "accuracy": 0.43023255813953487,
          "tolerance1_accuracy": 0.8488372093023255,
          "tolerance2_accuracy": 1.0,
          "rmse": 1.0115610777177464,
          "kappa": 0.16189339697692906,
          "weighted_kappa": 0.4215836135738307,
          "pearson_correlation": 0.43040134539415686,
          "pearson_p_value": 3.530301080280047e-05
        },
        "surprise": {
          "accuracy": 0.36046511627906974,
          "tolerance1_accuracy": 0.872093023255814,
          "tolerance2_accuracy": 1.0,
          "rmse": 1.0115610777177464,
          "kappa": 0.07797270955165692,
          "weighted_kappa": 0.3282442748091603,
          "pearson_correlation": 0.35187260353398875,
          "pearson_p_value": 0.0008927627462912728
        }
      }
    }
  ]
}