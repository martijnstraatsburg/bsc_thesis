{
  "average_metrics": {
    "story_classification": {
      "accuracy": {
        "mean": 0.7649291633253141,
        "std": 0.028188331614657915
      },
      "precision": {
        "mean": 0.7639281413493717,
        "std": 0.030941246902520474
      },
      "recall": {
        "mean": 0.7592276958703742,
        "std": 0.02906581030319644
      },
      "f1": {
        "mean": 0.759246505103037,
        "std": 0.03007314065367119
      }
    },
    "rating_metrics": {
      "suspense": {
        "accuracy": {
          "mean": 0.3640203154236835,
          "std": 0.059986582119062756
        },
        "tolerance1_accuracy": {
          "mean": 0.8179631114675219,
          "std": 0.03366937598664335
        },
        "tolerance2_accuracy": {
          "mean": 0.9608660785886126,
          "std": 0.009101991621582572
        },
        "rmse": {
          "mean": 1.1786434302073334,
          "std": 0.06956070318232274
        },
        "kappa": {
          "mean": 0.1474089183306392,
          "std": 0.0734331344769741
        },
        "weighted_kappa": {
          "mean": 0.3494037278772967,
          "std": 0.06941412411937471
        },
        "pearson_correlation": {
          "mean": 0.4104908895375038,
          "std": 0.08244087448672736
        },
        "pearson_p_value": {
          "mean": 0.0011959918105774608,
          "std": 0.001456860368400463
        }
      },
      "curiosity": {
        "accuracy": {
          "mean": 0.2717989842288158,
          "std": 0.02028261676652221
        },
        "tolerance1_accuracy": {
          "mean": 0.8411120021384658,
          "std": 0.03632151015689137
        },
        "tolerance2_accuracy": {
          "mean": 0.9469927826784282,
          "std": 0.02369557210112546
        },
        "rmse": {
          "mean": 1.215983181807903,
          "std": 0.08589500303837502
        },
        "kappa": {
          "mean": 0.10969872461212629,
          "std": 0.018285144901740154
        },
        "weighted_kappa": {
          "mean": 0.3765825568728382,
          "std": 0.06874550800203175
        },
        "pearson_correlation": {
          "mean": 0.42301488107037677,
          "std": 0.06888202755177554
        },
        "pearson_p_value": {
          "mean": 0.0005487169964794507,
          "std": 0.0009491633209833202
        }
      },
      "surprise": {
        "accuracy": {
          "mean": 0.3731889869018979,
          "std": 0.0827263578292596
        },
        "tolerance1_accuracy": {
          "mean": 0.7672012830793904,
          "std": 0.07936111893599802
        },
        "tolerance2_accuracy": {
          "mean": 0.944747393744988,
          "std": 0.02221419652500812
        },
        "rmse": {
          "mean": 1.262257308314259,
          "std": 0.15578394257824144
        },
        "kappa": {
          "mean": 0.14089147796025994,
          "std": 0.09719880502277647
        },
        "weighted_kappa": {
          "mean": 0.2724266002556354,
          "std": 0.10894966848239367
        },
        "pearson_correlation": {
          "mean": 0.3747636663156825,
          "std": 0.13146477927855768
        },
        "pearson_p_value": {
          "mean": 0.01604386755502505,
          "std": 0.024123620551055505
        }
      }
    }
  },
  "fold_metrics": [
    {
      "story_classification": {
        "accuracy": 0.7241379310344828,
        "precision": 0.7233086680761099,
        "recall": 0.7283783783783784,
        "f1": 0.7223404255319149
      },
      "rating_metrics": {
        "suspense": {
          "accuracy": 0.3218390804597701,
          "tolerance1_accuracy": 0.8505747126436781,
          "tolerance2_accuracy": 0.9540229885057471,
          "rmse": 1.1646123127807209,
          "kappa": 0.09550660792951537,
          "weighted_kappa": 0.3603738317757009,
          "pearson_correlation": 0.42794335023226654,
          "pearson_p_value": 3.5558832665901345e-05
        },
        "curiosity": {
          "accuracy": 0.28735632183908044,
          "tolerance1_accuracy": 0.7931034482758621,
          "tolerance2_accuracy": 0.9080459770114943,
          "rmse": 1.3687816547538927,
          "kappa": 0.12859450726979005,
          "weighted_kappa": 0.2710871241326138,
          "pearson_correlation": 0.3209151031542822,
          "pearson_p_value": 0.0024407557907946647
        },
        "surprise": {
          "accuracy": 0.27586206896551724,
          "tolerance1_accuracy": 0.632183908045977,
          "tolerance2_accuracy": 0.9080459770114943,
          "rmse": 1.5124008469447114,
          "kappa": 0.058247422680412386,
          "weighted_kappa": 0.1207658321060382,
          "pearson_correlation": 0.20074035196775922,
          "pearson_p_value": 0.062276040629784084
        }
      }
    },
    {
      "story_classification": {
        "accuracy": 0.7701149425287356,
        "precision": 0.7648648648648648,
        "recall": 0.7648648648648648,
        "f1": 0.7648648648648649
      },
      "rating_metrics": {
        "suspense": {
          "accuracy": 0.2988505747126437,
          "tolerance1_accuracy": 0.7816091954022989,
          "tolerance2_accuracy": 0.9540229885057471,
          "rmse": 1.259447059844805,
          "kappa": 0.057873246937688694,
          "weighted_kappa": 0.25102932002495326,
          "pearson_correlation": 0.31253306437841755,
          "pearson_p_value": 0.0032071198646033915
        },
        "curiosity": {
          "accuracy": 0.27586206896551724,
          "tolerance1_accuracy": 0.8505747126436781,
          "tolerance2_accuracy": 0.9425287356321839,
          "rmse": 1.2082094665009577,
          "kappa": 0.12107119948685052,
          "weighted_kappa": 0.37374596157116136,
          "pearson_correlation": 0.40746301850519906,
          "pearson_p_value": 8.965791420737862e-05
        },
        "surprise": {
          "accuracy": 0.3103448275862069,
          "tolerance1_accuracy": 0.7931034482758621,
          "tolerance2_accuracy": 0.9310344827586207,
          "rmse": 1.317433939105884,
          "kappa": 0.03547671840354771,
          "weighted_kappa": 0.17796132907828044,
          "pearson_correlation": 0.2534668030993626,
          "pearson_p_value": 0.01784548646124
        }
      }
    },
    {
      "story_classification": {
        "accuracy": 0.7816091954022989,
        "precision": 0.7887205387205387,
        "recall": 0.7736702127659575,
        "f1": 0.7758036077580361
      },
      "rating_metrics": {
        "suspense": {
          "accuracy": 0.3793103448275862,
          "tolerance1_accuracy": 0.7816091954022989,
          "tolerance2_accuracy": 0.9540229885057471,
          "rmse": 1.2270888828592579,
          "kappa": 0.16849557522123892,
          "weighted_kappa": 0.29452181987000925,
          "pearson_correlation": 0.31745576073380244,
          "pearson_p_value": 0.002734466052483122
        },
        "curiosity": {
          "accuracy": 0.27586206896551724,
          "tolerance1_accuracy": 0.8735632183908046,
          "tolerance2_accuracy": 0.9770114942528736,
          "rmse": 1.1038074128205977,
          "kappa": 0.11881028938906757,
          "weighted_kappa": 0.47212364052661704,
          "pearson_correlation": 0.5002192255102602,
          "pearson_p_value": 8.084037943407869e-07
        },
        "surprise": {
          "accuracy": 0.4827586206896552,
          "tolerance1_accuracy": 0.8160919540229885,
          "tolerance2_accuracy": 0.9540229885057471,
          "rmse": 1.1396712572986316,
          "kappa": 0.27325041767217373,
          "weighted_kappa": 0.36149899331038526,
          "pearson_correlation": 0.44663352826768554,
          "pearson_p_value": 1.449182447428325e-05
        }
      }
    },
    {
      "story_classification": {
        "accuracy": 0.8045977011494253,
        "precision": 0.8060897435897436,
        "recall": 0.8031746031746032,
        "f1": 0.8036638789326961
      },
      "rating_metrics": {
        "suspense": {
          "accuracy": 0.47126436781609193,
          "tolerance1_accuracy": 0.8620689655172413,
          "tolerance2_accuracy": 0.9655172413793104,
          "rmse": 1.0559083903140614,
          "kappa": 0.27302452316076287,
          "weighted_kappa": 0.4446199407699901,
          "pearson_correlation": 0.5108346110969979,
          "pearson_p_value": 4.296359680875028e-07
        },
        "curiosity": {
          "accuracy": 0.28735632183908044,
          "tolerance1_accuracy": 0.8045977011494253,
          "tolerance2_accuracy": 0.9655172413793104,
          "rmse": 1.2129568697262454,
          "kappa": 0.10279441117764465,
          "weighted_kappa": 0.34246575342465757,
          "pearson_correlation": 0.3872075517681037,
          "pearson_p_value": 0.00021137112306092941
        },
        "surprise": {
          "accuracy": 0.45977011494252873,
          "tolerance1_accuracy": 0.8620689655172413,
          "tolerance2_accuracy": 0.9655172413793104,
          "rmse": 1.0613372610104648,
          "kappa": 0.24024526198439233,
          "weighted_kappa": 0.4107809260539046,
          "pearson_correlation": 0.561614050112943,
          "pearson_p_value": 1.5207609624399447e-08
        }
      }
    },
    {
      "story_classification": {
        "accuracy": 0.7441860465116279,
        "precision": 0.7366568914956012,
        "recall": 0.7260504201680672,
        "f1": 0.729559748427673
      },
      "rating_metrics": {
        "suspense": {
          "accuracy": 0.3488372093023256,
          "tolerance1_accuracy": 0.813953488372093,
          "tolerance2_accuracy": 0.9767441860465116,
          "rmse": 1.1861605052378226,
          "kappa": 0.14214463840399016,
          "weighted_kappa": 0.39647372694582994,
          "pearson_correlation": 0.48368766124603435,
          "pearson_p_value": 2.3846671668015044e-06
        },
        "curiosity": {
          "accuracy": 0.23255813953488372,
          "tolerance1_accuracy": 0.8837209302325582,
          "tolerance2_accuracy": 0.9418604651162791,
          "rmse": 1.1861605052378226,
          "kappa": 0.07722321573727864,
          "weighted_kappa": 0.4234903047091413,
          "pearson_correlation": 0.49926950641403856,
          "pearson_p_value": 9.917505399399563e-07
        },
        "surprise": {
          "accuracy": 0.3372093023255814,
          "tolerance1_accuracy": 0.7325581395348837,
          "tolerance2_accuracy": 0.9651162790697675,
          "rmse": 1.2804432372116032,
          "kappa": 0.09723756906077352,
          "weighted_kappa": 0.29112592072956867,
          "pearson_correlation": 0.4113635981306623,
          "pearson_p_value": 8.330365201726774e-05
        }
      }
    }
  ]
}