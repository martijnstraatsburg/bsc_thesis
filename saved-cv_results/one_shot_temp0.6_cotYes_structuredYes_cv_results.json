{
  "average_metrics": {
    "story_classification": {
      "accuracy": {
        "mean": 0.7557070302058273,
        "std": 0.022884829903934256
      },
      "precision": {
        "mean": 0.7720899446891375,
        "std": 0.026399654495858096
      },
      "recall": {
        "mean": 0.7356011794021807,
        "std": 0.033854901016525066
      },
      "f1": {
        "mean": 0.7374077139464731,
        "std": 0.032618793054737596
      }
    },
    "rating_metrics": {
      "suspense": {
        "accuracy": {
          "mean": 0.3870890136327185,
          "std": 0.035930280062663286
        },
        "tolerance1_accuracy": {
          "mean": 0.8525260625501202,
          "std": 0.03443626953094103
        },
        "tolerance2_accuracy": {
          "mean": 0.9700882117080993,
          "std": 0.011679992236095353
        },
        "rmse": {
          "mean": 1.0958376853154461,
          "std": 0.06349415421051455
        },
        "kappa": {
          "mean": 0.20316781485113156,
          "std": 0.04106534324727365
        },
        "weighted_kappa": {
          "mean": 0.39898547887997066,
          "std": 0.059465808624690614
        },
        "pearson_correlation": {
          "mean": 0.4078516422578565,
          "std": 0.06317248382767293
        },
        "pearson_p_value": {
          "mean": 0.0007088208507124971,
          "std": 0.001010659787581014
        }
      },
      "curiosity": {
        "accuracy": {
          "mean": 0.336460839347768,
          "std": 0.017955790822533232
        },
        "tolerance1_accuracy": {
          "mean": 0.7996792301523656,
          "std": 0.04413212343342586
        },
        "tolerance2_accuracy": {
          "mean": 0.9723603314621758,
          "std": 0.01170138429313246
        },
        "rmse": {
          "mean": 1.1815915724509147,
          "std": 0.08087749252561025
        },
        "kappa": {
          "mean": 0.14616474928336814,
          "std": 0.015454360225118419
        },
        "weighted_kappa": {
          "mean": 0.41523717701412055,
          "std": 0.0689671893084965
        },
        "pearson_correlation": {
          "mean": 0.43613897741631913,
          "std": 0.0682318819495056
        },
        "pearson_p_value": {
          "mean": 0.0003642977725035897,
          "std": 0.0006124787777489983
        }
      },
      "surprise": {
        "accuracy": {
          "mean": 0.35926222935044105,
          "std": 0.05969130509326085
        },
        "tolerance1_accuracy": {
          "mean": 0.8110398289227477,
          "std": 0.03615430204589771
        },
        "tolerance2_accuracy": {
          "mean": 0.956214915797915,
          "std": 0.01342451512932903
        },
        "rmse": {
          "mean": 1.197931375586892,
          "std": 0.08715548347192568
        },
        "kappa": {
          "mean": 0.13786799401108946,
          "std": 0.0808840591315479
        },
        "weighted_kappa": {
          "mean": 0.33942693491314174,
          "std": 0.08041427712960196
        },
        "pearson_correlation": {
          "mean": 0.36156997449131634,
          "std": 0.07358446884882555
        },
        "pearson_p_value": {
          "mean": 0.0020981396166047223,
          "std": 0.0018225470019129475
        }
      }
    }
  },
  "fold_metrics": [
    {
      "story_classification": {
        "accuracy": 0.7701149425287356,
        "precision": 0.7710253456221199,
        "recall": 0.7543243243243243,
        "f1": 0.7586015538290788
      },
      "rating_metrics": {
        "suspense": {
          "accuracy": 0.4367816091954023,
          "tolerance1_accuracy": 0.896551724137931,
          "tolerance2_accuracy": 0.9655172413793104,
          "rmse": 1.0227301753122633,
          "kappa": 0.2489429175475687,
          "weighted_kappa": 0.4706118355065195,
          "pearson_correlation": 0.4878777946089988,
          "pearson_p_value": 1.6420220284631366e-06
        },
        "curiosity": {
          "accuracy": 0.3103448275862069,
          "tolerance1_accuracy": 0.7471264367816092,
          "tolerance2_accuracy": 0.9540229885057471,
          "rmse": 1.2954385047312087,
          "kappa": 0.12180349932705248,
          "weighted_kappa": 0.30907310704960833,
          "pearson_correlation": 0.3338578991207106,
          "pearson_p_value": 0.0015767490783361946
        },
        "surprise": {
          "accuracy": 0.3448275862068966,
          "tolerance1_accuracy": 0.7586206896551724,
          "tolerance2_accuracy": 0.9425287356321839,
          "rmse": 1.2909944487358056,
          "kappa": 0.15548365122615815,
          "weighted_kappa": 0.26016069438742606,
          "pearson_correlation": 0.30485323297509515,
          "pearson_p_value": 0.004091362577107298
        }
      }
    },
    {
      "story_classification": {
        "accuracy": 0.735632183908046,
        "precision": 0.7394067796610169,
        "recall": 0.7137837837837837,
        "f1": 0.7175723359209598
      },
      "rating_metrics": {
        "suspense": {
          "accuracy": 0.367816091954023,
          "tolerance1_accuracy": 0.8275862068965517,
          "tolerance2_accuracy": 0.9540229885057471,
          "rmse": 1.174440439029407,
          "kappa": 0.174573055028463,
          "weighted_kappa": 0.3116180931029935,
          "pearson_correlation": 0.31884118831888725,
          "pearson_p_value": 0.002613230712775524
        },
        "curiosity": {
          "accuracy": 0.3448275862068966,
          "tolerance1_accuracy": 0.7586206896551724,
          "tolerance2_accuracy": 0.9655172413793104,
          "rmse": 1.2456821978060995,
          "kappa": 0.14720550300945834,
          "weighted_kappa": 0.37088221115217734,
          "pearson_correlation": 0.385141596584019,
          "pearson_p_value": 0.00022998765710042837
        },
        "surprise": {
          "accuracy": 0.3793103448275862,
          "tolerance1_accuracy": 0.8390804597701149,
          "tolerance2_accuracy": 0.9425287356321839,
          "rmse": 1.2129568697262454,
          "kappa": 0.13766519823788548,
          "weighted_kappa": 0.2755659640905542,
          "pearson_correlation": 0.30201534472477204,
          "pearson_p_value": 0.004469447896746055
        }
      }
    },
    {
      "story_classification": {
        "accuracy": 0.7471264367816092,
        "precision": 0.7755359394703657,
        "recall": 0.7324468085106384,
        "f1": 0.7314814814814815
      },
      "rating_metrics": {
        "suspense": {
          "accuracy": 0.3333333333333333,
          "tolerance1_accuracy": 0.8045977011494253,
          "tolerance2_accuracy": 0.9770114942528736,
          "rmse": 1.1695366997037857,
          "kappa": 0.14242012236573776,
          "weighted_kappa": 0.3475137076952166,
          "pearson_correlation": 0.35004954572080593,
          "pearson_p_value": 0.0008885173948776084
        },
        "curiosity": {
          "accuracy": 0.3218390804597701,
          "tolerance1_accuracy": 0.8390804597701149,
          "tolerance2_accuracy": 0.9885057471264368,
          "rmse": 1.1038074128205977,
          "kappa": 0.1373109243697479,
          "weighted_kappa": 0.4830717488789239,
          "pearson_correlation": 0.5005211994293103,
          "pearson_p_value": 7.942299783629584e-07
        },
        "surprise": {
          "accuracy": 0.3333333333333333,
          "tolerance1_accuracy": 0.7931034482758621,
          "tolerance2_accuracy": 0.9655172413793104,
          "rmse": 1.2082094665009577,
          "kappa": 0.09537468626747947,
          "weighted_kappa": 0.34702440754092556,
          "pearson_correlation": 0.35232848417204954,
          "pearson_p_value": 0.0008175888976617063
        }
      }
    },
    {
      "story_classification": {
        "accuracy": 0.7931034482758621,
        "precision": 0.8184210526315789,
        "recall": 0.7880952380952382,
        "f1": 0.7867647058823529
      },
      "rating_metrics": {
        "suspense": {
          "accuracy": 0.41379310344827586,
          "tolerance1_accuracy": 0.8850574712643678,
          "tolerance2_accuracy": 0.9655172413793104,
          "rmse": 1.0504514628777804,
          "kappa": 0.24592114208021754,
          "weighted_kappa": 0.4347590687601516,
          "pearson_correlation": 0.4359782606975117,
          "pearson_p_value": 2.4331217136246823e-05
        },
        "curiosity": {
          "accuracy": 0.3448275862068966,
          "tolerance1_accuracy": 0.7931034482758621,
          "tolerance2_accuracy": 0.9770114942528736,
          "rmse": 1.1793237883215744,
          "kappa": 0.16360263113509865,
          "weighted_kappa": 0.42105263157894735,
          "pearson_correlation": 0.4480336342157455,
          "pearson_p_value": 1.3520375013252274e-05
        },
        "surprise": {
          "accuracy": 0.45977011494252873,
          "tolerance1_accuracy": 0.8620689655172413,
          "tolerance2_accuracy": 0.9770114942528736,
          "rmse": 1.0339078862458586,
          "kappa": 0.273454157782516,
          "weighted_kappa": 0.48696975461289715,
          "pearson_correlation": 0.5029002025369748,
          "pearson_p_value": 6.90505673034809e-07
        }
      }
    },
    {
      "story_classification": {
        "accuracy": 0.7325581395348837,
        "precision": 0.7560606060606061,
        "recall": 0.6893557422969188,
        "f1": 0.6926184926184926
      },
      "rating_metrics": {
        "suspense": {
          "accuracy": 0.38372093023255816,
          "tolerance1_accuracy": 0.8488372093023255,
          "tolerance2_accuracy": 0.9883720930232558,
          "rmse": 1.0620296496539945,
          "kappa": 0.20398183723367092,
          "weighted_kappa": 0.430424689334972,
          "pearson_correlation": 0.4465114219430786,
          "pearson_p_value": 1.638290674464282e-05
        },
        "curiosity": {
          "accuracy": 0.36046511627906974,
          "tolerance1_accuracy": 0.8604651162790697,
          "tolerance2_accuracy": 0.9767441860465116,
          "rmse": 1.0837059585750939,
          "kappa": 0.1609011885754833,
          "weighted_kappa": 0.49210618641094606,
          "pearson_correlation": 0.5131405577318104,
          "pearson_p_value": 4.37522089710247e-07
        },
        "surprise": {
          "accuracy": 0.27906976744186046,
          "tolerance1_accuracy": 0.8023255813953488,
          "tolerance2_accuracy": 0.9534883720930233,
          "rmse": 1.2435882067255932,
          "kappa": 0.027362276541408215,
          "weighted_kappa": 0.32741385393390565,
          "pearson_correlation": 0.3457526080476899,
          "pearson_p_value": 0.001111608205835519
        }
      }
    }
  ]
}